// Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "paddle/phi/api/include/api.h"
#include <memory>

#include "glog/logging.h"

#include "paddle/phi/api/lib/api_custom_impl.h"
#include "paddle/phi/api/lib/api_gen_utils.h"
#include "paddle/phi/api/lib/data_transform.h"
#include "paddle/phi/api/lib/kernel_dispatch.h"
#include "paddle/phi/core/kernel_registry.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/unary.h"

#include "paddle/fluid/platform/profiler/event_tracing.h"
#include "paddle/fluid/platform/profiler/supplement_tracing.h"

DECLARE_bool(conv2d_disable_cudnn);

namespace paddle {
namespace experimental {

PADDLE_API Tensor atan2(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atan2 API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atan2", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "atan2 kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("atan2", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "atan2 infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::Atan2InferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "atan2 compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor bernoulli(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bernoulli API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bernoulli", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bernoulli kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("bernoulli", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bernoulli infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bernoulli compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor cholesky(const Tensor& x, bool upper) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cholesky API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cholesky", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cholesky kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("cholesky", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cholesky infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CholeskyInferMeta(MakeMetaTensor(*input_x), upper, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cholesky compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, upper, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor cholesky_solve(const Tensor& x, const Tensor& y, bool upper) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cholesky_solve API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cholesky_solve", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cholesky_solve kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("cholesky_solve", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cholesky_solve infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CholeskySolveInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), upper, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cholesky_solve compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, upper, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor cross(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cross API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cross", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cross kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("cross", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cross infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CrossInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cross compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor diag(const Tensor& x, int offset, float padding_value) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "diag API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diag", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "diag kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("diag", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "diag infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DiagInferMeta(
      MakeMetaTensor(*input_x), offset, padding_value, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "diag compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, offset, padding_value, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor diagonal(const Tensor& x, int offset, int axis1, int axis2) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "diagonal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diagonal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "diagonal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("diagonal", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "diagonal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DiagonalInferMeta(
      MakeMetaTensor(*input_x), offset, axis1, axis2, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "diagonal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, offset, axis1, axis2, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor digamma(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "digamma API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "digamma", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "digamma kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("digamma", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "digamma infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "digamma compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor dist(const Tensor& x, const Tensor& y, float p) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dist API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dist", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "dist kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("dist", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "dist infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DistInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), p, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "dist compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, p, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor dot(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dot API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dot", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "dot kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("dot", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "dot infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DotInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "dot compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor erf(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "erf API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erf", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "erf kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("erf", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "erf infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "erf compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor erfinv(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "erfinv API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erfinv", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "erfinv kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("erfinv", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "erfinv infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "erfinv compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& erfinv_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "erfinv API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erfinv", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "erfinv kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("erfinv", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "erfinv infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "erfinv compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fft_c2c(const Tensor& x,
                          const std::vector<int64_t>& axes,
                          const std::string& normalization,
                          bool forward) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fft_c2c API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_c2c", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fft_c2c kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fft_c2c", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fft_c2c infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FFTC2CInferMeta(
      MakeMetaTensor(*input_x), axes, normalization, forward, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    const std::string&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fft_c2c compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axes, normalization, forward, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fft_c2r(const Tensor& x,
                          const std::vector<int64_t>& axes,
                          const std::string& normalization,
                          bool forward,
                          int64_t last_dim_size) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fft_c2r API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_c2r", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fft_c2r kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fft_c2r", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fft_c2r infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FFTC2RInferMeta(MakeMetaTensor(*input_x),
                       axes,
                       normalization,
                       forward,
                       last_dim_size,
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    const std::string&,
                                    bool,
                                    int64_t,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fft_c2r compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               axes,
               normalization,
               forward,
               last_dim_size,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fft_r2c(const Tensor& x,
                          const std::vector<int64_t>& axes,
                          const std::string& normalization,
                          bool forward,
                          bool onesided) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fft_r2c API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_r2c", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fft_r2c kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fft_r2c", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fft_r2c infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FFTR2CInferMeta(MakeMetaTensor(*input_x),
                       axes,
                       normalization,
                       forward,
                       onesided,
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    const std::string&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fft_r2c compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, axes, normalization, forward, onesided, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor graph_send_uv(const Tensor& x,
                                const Tensor& y,
                                const Tensor& src_index,
                                const Tensor& dst_index,
                                const std::string& message_op) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, src_index, dst_index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "graph_send_uv API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "graph_send_uv", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "graph_send_uv kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  auto input_src_index = PrepareData(src_index, kernel.InputAt(2), {});
  auto input_dst_index = PrepareData(dst_index, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"y", {(*input_y).dims()}},
        {"src_index", {(*input_src_index).dims()}},
        {"dst_index", {(*input_dst_index).dims()}}};
    platform::RecordOpInfoSupplement("graph_send_uv", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "graph_send_uv infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::GraphSendUVInferMeta(MakeMetaTensor(*input_x),
                            MakeMetaTensor(*input_y),
                            MakeMetaTensor(*input_src_index),
                            MakeMetaTensor(*input_dst_index),
                            message_op,
                            &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "graph_send_uv compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_y,
               *input_src_index,
               *input_dst_index,
               message_op,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor lgamma(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lgamma API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lgamma", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "lgamma kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("lgamma", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "lgamma infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "lgamma compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor mv(const Tensor& x, const Tensor& vec) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, vec);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mv API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mv", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "mv kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_vec = PrepareData(vec, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"vec", {(*input_vec).dims()}}};
    platform::RecordOpInfoSupplement("mv", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "mv infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MvInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_vec), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "mv compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_vec, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor poisson(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "poisson API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "poisson", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "poisson kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("poisson", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "poisson infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "poisson compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor solve(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "solve API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "solve", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "solve kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("solve", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "solve infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SolveInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "solve compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor trace(const Tensor& x, int offset, int axis1, int axis2) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trace API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trace", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "trace kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("trace", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "trace infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TraceInferMeta(
      MakeMetaTensor(*input_x), offset, axis1, axis2, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "trace compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, offset, axis1, axis2, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor trunc(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trunc API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trunc", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "trunc kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("trunc", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "trunc infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "trunc compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor abs(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "abs API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "abs", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "abs kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("abs", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "abs infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "abs compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> accuracy(const Tensor& x,
                                                       const Tensor& indices,
                                                       const Tensor& label) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "accuracy API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "accuracy", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "accuracy kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_indices = PrepareData(indices, kernel.InputAt(1), {});
  auto input_label = PrepareData(label, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"indices", {(*input_indices).dims()}},
        {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("accuracy", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "accuracy infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::AccuracyInferMeta(MakeMetaTensor(*input_x),
                         MakeMetaTensor(*input_indices),
                         MakeMetaTensor(*input_label),
                         kernel_out_0 ? &meta_out_0 : nullptr,
                         kernel_out_1 ? &meta_out_1 : nullptr,
                         kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "accuracy compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_indices,
               *input_label,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor acos(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "acos API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acos", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "acos kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("acos", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "acos infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "acos compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor acosh(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "acosh API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acosh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "acosh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("acosh", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "acosh infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "acosh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&> adadelta_(
    Tensor& param,
    const Tensor& grad,
    Tensor& avg_squared_grad,
    Tensor& avg_squared_update,
    float rho,
    float epsilon) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(
        param, grad, avg_squared_grad, avg_squared_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "adadelta_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "adadelta", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "adadelta kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_param = PrepareData(param, kernel.InputAt(0), {});
  auto input_grad = PrepareData(grad, kernel.InputAt(1), {});
  auto input_avg_squared_grad =
      PrepareData(avg_squared_grad, kernel.InputAt(2), {});
  auto input_avg_squared_update =
      PrepareData(avg_squared_update, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"param", {(*input_param).dims()}},
        {"grad", {(*input_grad).dims()}},
        {"avg_squared_grad", {(*input_avg_squared_grad).dims()}},
        {"avg_squared_update", {(*input_avg_squared_update).dims()}}};
    platform::RecordOpInfoSupplement("adadelta_", input_shapes);
  }

  std::tuple<Tensor&, Tensor&, Tensor&> api_output{
      param, avg_squared_grad, avg_squared_update};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "adadelta_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::AdadeltaInferMeta(MakeMetaTensor(*input_param),
                         MakeMetaTensor(*input_grad),
                         MakeMetaTensor(*input_avg_squared_grad),
                         MakeMetaTensor(*input_avg_squared_update),
                         rho,
                         epsilon,
                         kernel_out_0 ? &meta_out_0 : nullptr,
                         kernel_out_1 ? &meta_out_1 : nullptr,
                         kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "adadelta_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_param,
               *input_grad,
               *input_avg_squared_grad,
               *input_avg_squared_update,
               rho,
               epsilon,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&> adagrad_(Tensor& param,
                                                 const Tensor& grad,
                                                 Tensor& moment,
                                                 const Tensor& learning_rate,
                                                 float epsilon) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(param, grad, moment, learning_rate);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (param.is_dense_tensor() && grad.is_dense_tensor() &&
      moment.is_dense_tensor() && learning_rate.is_dense_tensor()) {
    VLOG(6) << "adagrad_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "adagrad", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "adagrad kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_grad = PrepareData(grad, kernel.InputAt(1), {});
    auto input_moment = PrepareData(moment, kernel.InputAt(2), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(3), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"moment", {(*input_moment).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}}};
      platform::RecordOpInfoSupplement("adagrad_", input_shapes);
    }

    std::tuple<Tensor&, Tensor&> api_output{param, moment};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "adagrad_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);

    phi::AdagradInferMeta(MakeMetaTensor(*input_param),
                          MakeMetaTensor(*input_grad),
                          MakeMetaTensor(*input_moment),
                          MakeMetaTensor(*input_learning_rate),
                          epsilon,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      float,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "adagrad_ compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_grad,
                 *input_moment,
                 *input_learning_rate,
                 epsilon,
                 kernel_out_0,
                 kernel_out_1);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    }
    return api_output;
  }

  if (param.is_dense_tensor() && grad.is_selected_rows() &&
      moment.is_dense_tensor() && learning_rate.is_dense_tensor()) {
    VLOG(6) << "adagrad_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "adagrad_dense_param_sparse_grad",
            {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "adagrad_dense_param_sparse_grad kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_grad = TensorToSelectedRows(grad);

    auto input_moment = PrepareData(moment, kernel.InputAt(2), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(3), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"moment", {(*input_moment).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}}};
      platform::RecordOpInfoSupplement("adagrad_", input_shapes);
    }

    std::tuple<Tensor&, Tensor&> api_output{param, moment};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "adagrad_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);

    phi::AdagradInferMeta(MakeMetaTensor(*input_param),
                          MakeMetaTensor(*input_grad),
                          MakeMetaTensor(*input_moment),
                          MakeMetaTensor(*input_learning_rate),
                          epsilon,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::SelectedRows&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      float,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "adagrad_ compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_grad,
                 *input_moment,
                 *input_learning_rate,
                 epsilon,
                 kernel_out_0,
                 kernel_out_1);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (adagrad_) for input tensors is unimplemented, please "
      "check the type of input tensors."));
}

PADDLE_API std::tuple<Tensor&,
                      Tensor&,
                      Tensor&,
                      Tensor&,
                      Tensor&,
                      paddle::optional<Tensor>&>
adam_(Tensor& param,
      const Tensor& grad,
      const Tensor& learning_rate,
      Tensor& moment1,
      Tensor& moment2,
      Tensor& beta1_pow,
      Tensor& beta2_pow,
      paddle::optional<Tensor>& master_param,
      const paddle::optional<Tensor>& skip_update,
      const Scalar& beta1,
      const Scalar& beta2,
      const Scalar& epsilon,
      bool lazy_mode,
      int64_t min_row_size_to_use_multithread,
      bool multi_precision,
      bool use_global_beta_pow) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param,
                                                    grad,
                                                    learning_rate,
                                                    moment1,
                                                    moment2,
                                                    beta1_pow,
                                                    beta2_pow,
                                                    master_param,
                                                    skip_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (param.is_dense_tensor() && grad.is_dense_tensor() &&
      learning_rate.is_dense_tensor() && moment1.is_dense_tensor() &&
      moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() &&
      beta2_pow.is_dense_tensor() &&
      (!master_param || master_param->is_dense_tensor()) &&
      (!skip_update || skip_update->is_dense_tensor())) {
    VLOG(6) << "adam_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "adam", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "adam kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_grad = PrepareData(grad, kernel.InputAt(1), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(2), {});
    auto input_moment1 = PrepareData(moment1, kernel.InputAt(3), {});
    auto input_moment2 = PrepareData(moment2, kernel.InputAt(4), {});
    auto input_beta1_pow = PrepareData(beta1_pow, kernel.InputAt(5), {});
    auto input_beta2_pow = PrepareData(beta2_pow, kernel.InputAt(6), {});
    auto input_master_param = PrepareData(master_param, kernel.InputAt(7), {});
    auto input_skip_update = PrepareData(skip_update, kernel.InputAt(8), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<phi::DDim> master_param_record_shapes;
      if (input_master_param) {
        master_param_record_shapes.push_back((*input_master_param).dims());
      }
      std::vector<phi::DDim> skip_update_record_shapes;
      if (input_skip_update) {
        skip_update_record_shapes.push_back((*input_skip_update).dims());
      }
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"moment1", {(*input_moment1).dims()}},
          {"moment2", {(*input_moment2).dims()}},
          {"beta1_pow", {(*input_beta1_pow).dims()}},
          {"beta2_pow", {(*input_beta2_pow).dims()}},
          {"master_param", master_param_record_shapes},
          {"skip_update", skip_update_record_shapes}};
      platform::RecordOpInfoSupplement("adam_", input_shapes);
    }

    std::tuple<Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               paddle::optional<Tensor>&>
        api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "adam_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);
    phi::MetaTensor meta_out_2(kernel_out_2);
    phi::MetaTensor meta_out_3(kernel_out_3);
    phi::MetaTensor meta_out_4(kernel_out_4);
    phi::MetaTensor meta_out_5(kernel_out_5);

    phi::AdamInferMeta(MakeMetaTensor(*input_param),
                       MakeMetaTensor(*input_grad),
                       MakeMetaTensor(*input_learning_rate),
                       MakeMetaTensor(*input_moment1),
                       MakeMetaTensor(*input_moment2),
                       MakeMetaTensor(*input_beta1_pow),
                       MakeMetaTensor(*input_beta2_pow),
                       MakeMetaTensor(input_master_param),
                       MakeMetaTensor(input_skip_update),
                       beta1,
                       beta2,
                       epsilon,
                       lazy_mode,
                       min_row_size_to_use_multithread,
                       multi_precision,
                       use_global_beta_pow,
                       kernel_out_0 ? &meta_out_0 : nullptr,
                       kernel_out_1 ? &meta_out_1 : nullptr,
                       kernel_out_2 ? &meta_out_2 : nullptr,
                       kernel_out_3 ? &meta_out_3 : nullptr,
                       kernel_out_4 ? &meta_out_4 : nullptr,
                       kernel_out_5 ? &meta_out_5 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      const phi::Scalar&,
                                      const phi::Scalar&,
                                      const phi::Scalar&,
                                      bool,
                                      int64_t,
                                      bool,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "adam_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_grad,
                 *input_learning_rate,
                 *input_moment1,
                 *input_moment2,
                 *input_beta1_pow,
                 *input_beta2_pow,
                 input_master_param,
                 input_skip_update,
                 phi::Scalar(beta1),
                 phi::Scalar(beta2),
                 phi::Scalar(epsilon),
                 lazy_mode,
                 min_row_size_to_use_multithread,
                 multi_precision,
                 use_global_beta_pow,
                 kernel_out_0,
                 kernel_out_1,
                 kernel_out_2,
                 kernel_out_3,
                 kernel_out_4,
                 kernel_out_5);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    }
    return api_output;
  }

  if (param.is_dense_tensor() && grad.is_selected_rows() &&
      learning_rate.is_dense_tensor() && moment1.is_dense_tensor() &&
      moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() &&
      beta2_pow.is_dense_tensor() &&
      (!master_param || master_param->is_dense_tensor()) &&
      (!skip_update || skip_update->is_dense_tensor())) {
    VLOG(6) << "adam_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "adam_dense_param_sparse_grad",
            {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "adam_dense_param_sparse_grad kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_grad = TensorToSelectedRows(grad);

    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(2), {});
    auto input_moment1 = PrepareData(moment1, kernel.InputAt(3), {});
    auto input_moment2 = PrepareData(moment2, kernel.InputAt(4), {});
    auto input_beta1_pow = PrepareData(beta1_pow, kernel.InputAt(5), {});
    auto input_beta2_pow = PrepareData(beta2_pow, kernel.InputAt(6), {});
    auto input_master_param = PrepareData(master_param, kernel.InputAt(7), {});
    auto input_skip_update = PrepareData(skip_update, kernel.InputAt(8), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<phi::DDim> master_param_record_shapes;
      if (input_master_param) {
        master_param_record_shapes.push_back((*input_master_param).dims());
      }
      std::vector<phi::DDim> skip_update_record_shapes;
      if (input_skip_update) {
        skip_update_record_shapes.push_back((*input_skip_update).dims());
      }
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"moment1", {(*input_moment1).dims()}},
          {"moment2", {(*input_moment2).dims()}},
          {"beta1_pow", {(*input_beta1_pow).dims()}},
          {"beta2_pow", {(*input_beta2_pow).dims()}},
          {"master_param", master_param_record_shapes},
          {"skip_update", skip_update_record_shapes}};
      platform::RecordOpInfoSupplement("adam_", input_shapes);
    }

    std::tuple<Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               paddle::optional<Tensor>&>
        api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "adam_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);
    phi::MetaTensor meta_out_2(kernel_out_2);
    phi::MetaTensor meta_out_3(kernel_out_3);
    phi::MetaTensor meta_out_4(kernel_out_4);
    phi::MetaTensor meta_out_5(kernel_out_5);

    phi::AdamInferMeta(MakeMetaTensor(*input_param),
                       MakeMetaTensor(*input_grad),
                       MakeMetaTensor(*input_learning_rate),
                       MakeMetaTensor(*input_moment1),
                       MakeMetaTensor(*input_moment2),
                       MakeMetaTensor(*input_beta1_pow),
                       MakeMetaTensor(*input_beta2_pow),
                       MakeMetaTensor(input_master_param),
                       MakeMetaTensor(input_skip_update),
                       beta1,
                       beta2,
                       epsilon,
                       lazy_mode,
                       min_row_size_to_use_multithread,
                       multi_precision,
                       use_global_beta_pow,
                       kernel_out_0 ? &meta_out_0 : nullptr,
                       kernel_out_1 ? &meta_out_1 : nullptr,
                       kernel_out_2 ? &meta_out_2 : nullptr,
                       kernel_out_3 ? &meta_out_3 : nullptr,
                       kernel_out_4 ? &meta_out_4 : nullptr,
                       kernel_out_5 ? &meta_out_5 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::SelectedRows&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      const phi::Scalar&,
                                      const phi::Scalar&,
                                      const phi::Scalar&,
                                      bool,
                                      int64_t,
                                      bool,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "adam_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_grad,
                 *input_learning_rate,
                 *input_moment1,
                 *input_moment2,
                 *input_beta1_pow,
                 *input_beta2_pow,
                 input_master_param,
                 input_skip_update,
                 phi::Scalar(beta1),
                 phi::Scalar(beta2),
                 phi::Scalar(epsilon),
                 lazy_mode,
                 min_row_size_to_use_multithread,
                 multi_precision,
                 use_global_beta_pow,
                 kernel_out_0,
                 kernel_out_1,
                 kernel_out_2,
                 kernel_out_3,
                 kernel_out_4,
                 kernel_out_5);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (adam_) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&> adamax_(
    Tensor& param,
    const Tensor& grad,
    const Tensor& learning_rate,
    Tensor& moment,
    Tensor& inf_norm,
    const Tensor& beta1_pow,
    float beta1,
    float beta2,
    float epsilon) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(
        param, grad, learning_rate, moment, inf_norm, beta1_pow);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "adamax_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "adamax", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "adamax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_param = PrepareData(param, kernel.InputAt(0), {});
  auto input_grad = PrepareData(grad, kernel.InputAt(1), {});
  auto input_learning_rate = PrepareData(learning_rate, kernel.InputAt(2), {});
  auto input_moment = PrepareData(moment, kernel.InputAt(3), {});
  auto input_inf_norm = PrepareData(inf_norm, kernel.InputAt(4), {});
  auto input_beta1_pow = PrepareData(beta1_pow, kernel.InputAt(5), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"param", {(*input_param).dims()}},
        {"grad", {(*input_grad).dims()}},
        {"learning_rate", {(*input_learning_rate).dims()}},
        {"moment", {(*input_moment).dims()}},
        {"inf_norm", {(*input_inf_norm).dims()}},
        {"beta1_pow", {(*input_beta1_pow).dims()}}};
    platform::RecordOpInfoSupplement("adamax_", input_shapes);
  }

  std::tuple<Tensor&, Tensor&, Tensor&> api_output{param, moment, inf_norm};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "adamax_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::AdamaxInferMeta(MakeMetaTensor(*input_param),
                       MakeMetaTensor(*input_grad),
                       MakeMetaTensor(*input_learning_rate),
                       MakeMetaTensor(*input_moment),
                       MakeMetaTensor(*input_inf_norm),
                       MakeMetaTensor(*input_beta1_pow),
                       beta1,
                       beta2,
                       epsilon,
                       kernel_out_0 ? &meta_out_0 : nullptr,
                       kernel_out_1 ? &meta_out_1 : nullptr,
                       kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "adamax_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_param,
               *input_grad,
               *input_learning_rate,
               *input_moment,
               *input_inf_norm,
               *input_beta1_pow,
               beta1,
               beta2,
               epsilon,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&,
                      Tensor&,
                      Tensor&,
                      Tensor&,
                      Tensor&,
                      paddle::optional<Tensor>&>
adamw_(Tensor& param,
       const Tensor& grad,
       const Tensor& learning_rate,
       Tensor& moment1,
       Tensor& moment2,
       Tensor& beta1_pow,
       Tensor& beta2_pow,
       paddle::optional<Tensor>& master_param,
       const paddle::optional<Tensor>& skip_update,
       const Scalar& beta1,
       const Scalar& beta2,
       const Scalar& epsilon,
       float lr_ratio,
       float coeff,
       bool with_decay,
       bool lazy_mode,
       int64_t min_row_size_to_use_multithread,
       bool multi_precision,
       bool use_global_beta_pow) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param,
                                                    grad,
                                                    learning_rate,
                                                    moment1,
                                                    moment2,
                                                    beta1_pow,
                                                    beta2_pow,
                                                    master_param,
                                                    skip_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "adamw_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "adamw", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "adamw kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_param = PrepareData(param, kernel.InputAt(0), {});
  auto input_grad = PrepareData(grad, kernel.InputAt(1), {});
  auto input_learning_rate = PrepareData(learning_rate, kernel.InputAt(2), {});
  auto input_moment1 = PrepareData(moment1, kernel.InputAt(3), {});
  auto input_moment2 = PrepareData(moment2, kernel.InputAt(4), {});
  auto input_beta1_pow = PrepareData(beta1_pow, kernel.InputAt(5), {});
  auto input_beta2_pow = PrepareData(beta2_pow, kernel.InputAt(6), {});
  auto input_master_param = PrepareData(master_param, kernel.InputAt(7), {});
  auto input_skip_update = PrepareData(skip_update, kernel.InputAt(8), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> master_param_record_shapes;
    if (input_master_param) {
      master_param_record_shapes.push_back((*input_master_param).dims());
    }
    std::vector<phi::DDim> skip_update_record_shapes;
    if (input_skip_update) {
      skip_update_record_shapes.push_back((*input_skip_update).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"param", {(*input_param).dims()}},
        {"grad", {(*input_grad).dims()}},
        {"learning_rate", {(*input_learning_rate).dims()}},
        {"moment1", {(*input_moment1).dims()}},
        {"moment2", {(*input_moment2).dims()}},
        {"beta1_pow", {(*input_beta1_pow).dims()}},
        {"beta2_pow", {(*input_beta2_pow).dims()}},
        {"master_param", master_param_record_shapes},
        {"skip_update", skip_update_record_shapes}};
    platform::RecordOpInfoSupplement("adamw_", input_shapes);
  }

  std::tuple<Tensor&,
             Tensor&,
             Tensor&,
             Tensor&,
             Tensor&,
             paddle::optional<Tensor>&>
      api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "adamw_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);
  phi::MetaTensor meta_out_3(kernel_out_3);
  phi::MetaTensor meta_out_4(kernel_out_4);
  phi::MetaTensor meta_out_5(kernel_out_5);

  phi::AdamwInferMeta(MakeMetaTensor(*input_param),
                      MakeMetaTensor(*input_grad),
                      MakeMetaTensor(*input_learning_rate),
                      MakeMetaTensor(*input_moment1),
                      MakeMetaTensor(*input_moment2),
                      MakeMetaTensor(*input_beta1_pow),
                      MakeMetaTensor(*input_beta2_pow),
                      MakeMetaTensor(input_master_param),
                      MakeMetaTensor(input_skip_update),
                      beta1,
                      beta2,
                      epsilon,
                      lr_ratio,
                      coeff,
                      with_decay,
                      lazy_mode,
                      min_row_size_to_use_multithread,
                      multi_precision,
                      use_global_beta_pow,
                      kernel_out_0 ? &meta_out_0 : nullptr,
                      kernel_out_1 ? &meta_out_1 : nullptr,
                      kernel_out_2 ? &meta_out_2 : nullptr,
                      kernel_out_3 ? &meta_out_3 : nullptr,
                      kernel_out_4 ? &meta_out_4 : nullptr,
                      kernel_out_5 ? &meta_out_5 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    float,
                                    float,
                                    bool,
                                    bool,
                                    int64_t,
                                    bool,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "adamw_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_param,
               *input_grad,
               *input_learning_rate,
               *input_moment1,
               *input_moment2,
               *input_beta1_pow,
               *input_beta2_pow,
               input_master_param,
               input_skip_update,
               phi::Scalar(beta1),
               phi::Scalar(beta2),
               phi::Scalar(epsilon),
               lr_ratio,
               coeff,
               with_decay,
               lazy_mode,
               min_row_size_to_use_multithread,
               multi_precision,
               use_global_beta_pow,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2,
               kernel_out_3,
               kernel_out_4,
               kernel_out_5);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
  }
  return api_output;
}

PADDLE_API Tensor add(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "add API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "add kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("add", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "add infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "add compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& add_(Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "add API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "add kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("add", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "add infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "add compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor add_n(const std::vector<Tensor>& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "add_n API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add_n", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "add_n kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x_vec = PrepareData(x, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_x.size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      ddims_vec.emplace_back((*input_x[i]).dims());
    }
    input_shapes.emplace_back("x", ddims_vec);
    platform::RecordOpInfoSupplement("add_n", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "add_n infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::AddNInferMeta(x_metas, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "add_n compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor addmm(const Tensor& input,
                        const Tensor& x,
                        const Tensor& y,
                        float alpha,
                        float beta) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "addmm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "addmm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "addmm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_x = PrepareData(x, kernel.InputAt(1), {});
  auto input_y = PrepareData(y, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}},
        {"x", {(*input_x).dims()}},
        {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("addmm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "addmm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::AddmmInferMeta(MakeMetaTensor(*input_input),
                      MakeMetaTensor(*input_x),
                      MakeMetaTensor(*input_y),
                      alpha,
                      beta,
                      &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "addmm compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_input, *input_x, *input_y, alpha, beta, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor affine_grid(const Tensor& input,
                              const IntArray& outputShape,
                              bool use_cudnn,
                              bool align_corners) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "affine_grid API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "affine_grid",
      {kernel_backend, kernel_layout, kernel_data_type},
      use_cudnn);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "affine_grid kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}};
    platform::RecordOpInfoSupplement("affine_grid", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "affine_grid infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::AffineGridInferMeta(
      MakeMetaTensor(*input_input), outputShape, align_corners, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "affine_grid compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               phi::IntArray(outputShape),
               align_corners,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor all(const Tensor& x,
                      const std::vector<int64_t>& dims,
                      bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "all API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "all", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "all kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("all", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "all infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), dims, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "all compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, dims, keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor allclose(const Tensor& x,
                           const Tensor& y,
                           const Scalar& rtol,
                           const Scalar& atol,
                           bool equal_nan) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "allclose API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "allclose", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "allclose kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("allclose", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "allclose infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::AllValueCompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "allclose compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_y,
               phi::Scalar(rtol),
               phi::Scalar(atol),
               equal_nan,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor amax(const Tensor& x,
                       const std::vector<int64_t>& dims,
                       bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "amax API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "amax", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "amax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("amax", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "amax infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), dims, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "amax compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, dims, keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor amin(const Tensor& x,
                       const std::vector<int64_t>& dims,
                       bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "amin API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "amin", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "amin kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("amin", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "amin infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), dims, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "amin compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, dims, keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor angle(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "angle API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "angle", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "angle kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("angle", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "angle infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "angle compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor any(const Tensor& x,
                      const std::vector<int64_t>& dims,
                      bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "any API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "any", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "any kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("any", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "any infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceInferMeta(MakeMetaTensor(*input_x), dims, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "any compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, dims, keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor arange(const Tensor& start,
                         const Tensor& end,
                         const Tensor& step,
                         DataType dtype,
                         const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(start, end, step);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "arange API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "arange", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "arange kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_start = PrepareData(start, kernel.InputAt(0), {false, true});
  auto input_end = PrepareData(end, kernel.InputAt(1), {false, true});
  auto input_step = PrepareData(step, kernel.InputAt(2), {false, true});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"start", {(*input_start).dims()}},
        {"end", {(*input_end).dims()}},
        {"step", {(*input_step).dims()}}};
    platform::RecordOpInfoSupplement("arange", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "arange infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ArangeInferMeta(MakeMetaTensor(*input_start),
                       MakeMetaTensor(*input_end),
                       MakeMetaTensor(*input_step),
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "arange compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_start, *input_end, *input_step, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor argmax(const Tensor& x,
                         const Scalar& axis,
                         bool keepdims,
                         bool flatten,
                         int dtype) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "argmax API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "arg_max", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "arg_max kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("argmax", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "argmax infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ArgMinMaxInferMeta(
      MakeMetaTensor(*input_x), axis, keepdims, flatten, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    bool,
                                    bool,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "argmax compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::Scalar(axis),
               keepdims,
               flatten,
               dtype,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor argmin(const Tensor& x,
                         const Scalar& axis,
                         bool keepdims,
                         bool flatten,
                         int dtype) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "argmin API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "arg_min", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "arg_min kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("argmin", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "argmin infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ArgMinMaxInferMeta(
      MakeMetaTensor(*input_x), axis, keepdims, flatten, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    bool,
                                    bool,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "argmin compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::Scalar(axis),
               keepdims,
               flatten,
               dtype,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> argsort(const Tensor& x,
                                              int axis,
                                              bool descending) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "argsort API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "argsort", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "argsort kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("argsort", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "argsort infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::ArgsortInferMeta(MakeMetaTensor(*input_x),
                        axis,
                        descending,
                        kernel_out_0 ? &meta_out_0 : nullptr,
                        kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "argsort compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, axis, descending, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor as_complex(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "as_complex API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "as_complex", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "as_complex kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("as_complex", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "as_complex infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::AsComplexInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "as_complex compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor as_real(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "as_real API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "as_real", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "as_real kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("as_real", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "as_real infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::AsRealInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "as_real compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor asin(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "asin API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asin", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "asin kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("asin", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "asin infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "asin compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor asinh(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "asinh API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asinh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "asinh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("asinh", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "asinh infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "asinh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor assign(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "assign kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("assign", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "assign infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "assign compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& assign_out_(const Tensor& x, Tensor& output) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, output);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign_out_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "assign kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("assign_out_", input_shapes);
  }

  Tensor& api_output = output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "assign_out_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "assign_out_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& assign_value_(Tensor& output,
                                 const std::vector<int>& shape,
                                 DataType dtype,
                                 const std::vector<phi::Scalar>& values,
                                 const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackendWithInputOrder(place, output);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(output);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "assign_value_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign_value", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "assign_value kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("assign_value_", input_shapes);
  }

  Tensor& api_output = output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "assign_value_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::AssignValueInferMeta(shape, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<int>&,
                                    DataType,
                                    const std::vector<phi::Scalar>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "assign_value_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, shape, dtype, values, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor atan(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atan API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atan", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "atan kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("atan", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "atan infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "atan compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor atanh(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "atanh API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atanh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "atanh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("atanh", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "atanh infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "atanh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> auc(
    const Tensor& x,
    const Tensor& label,
    const Tensor& stat_pos,
    const Tensor& stat_neg,
    const paddle::optional<Tensor>& ins_tag_weight,
    const std::string& curve,
    int num_thresholds,
    int slide_steps) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, label, stat_pos, stat_neg, ins_tag_weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "auc API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "auc", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "auc kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  auto input_stat_pos = PrepareData(stat_pos, kernel.InputAt(2), {});
  auto input_stat_neg = PrepareData(stat_neg, kernel.InputAt(3), {});
  auto input_ins_tag_weight =
      PrepareData(ins_tag_weight, kernel.InputAt(4), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> ins_tag_weight_record_shapes;
    if (input_ins_tag_weight) {
      ins_tag_weight_record_shapes.push_back((*input_ins_tag_weight).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"label", {(*input_label).dims()}},
        {"stat_pos", {(*input_stat_pos).dims()}},
        {"stat_neg", {(*input_stat_neg).dims()}},
        {"ins_tag_weight", ins_tag_weight_record_shapes}};
    platform::RecordOpInfoSupplement("auc", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "auc infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::AucInferMeta(MakeMetaTensor(*input_x),
                    MakeMetaTensor(*input_label),
                    MakeMetaTensor(*input_stat_pos),
                    MakeMetaTensor(*input_stat_neg),
                    MakeMetaTensor(input_ins_tag_weight),
                    curve,
                    num_thresholds,
                    slide_steps,
                    kernel_out_0 ? &meta_out_0 : nullptr,
                    kernel_out_1 ? &meta_out_1 : nullptr,
                    kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const std::string&,
                                    int,
                                    int,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "auc compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_label,
               *input_stat_pos,
               *input_stat_neg,
               input_ins_tag_weight,
               curve,
               num_thresholds,
               slide_steps,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&>
average_accumulates_(const Tensor& param,
                     Tensor& in_sum_1,
                     Tensor& in_sum_2,
                     Tensor& in_sum_3,
                     Tensor& in_num_accumulates,
                     Tensor& in_old_num_accumulates,
                     Tensor& in_num_updates,
                     float average_window,
                     int64_t max_average_window,
                     int64_t min_average_window) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param,
                                                    in_sum_1,
                                                    in_sum_2,
                                                    in_sum_3,
                                                    in_num_accumulates,
                                                    in_old_num_accumulates,
                                                    in_num_updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "average_accumulates_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "average_accumulates", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "average_accumulates kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_param = PrepareData(param, kernel.InputAt(0), {});
  auto input_in_sum_1 = PrepareData(in_sum_1, kernel.InputAt(1), {});
  auto input_in_sum_2 = PrepareData(in_sum_2, kernel.InputAt(2), {});
  auto input_in_sum_3 = PrepareData(in_sum_3, kernel.InputAt(3), {});
  auto input_in_num_accumulates =
      PrepareData(in_num_accumulates, kernel.InputAt(4), {});
  auto input_in_old_num_accumulates =
      PrepareData(in_old_num_accumulates, kernel.InputAt(5), {});
  auto input_in_num_updates =
      PrepareData(in_num_updates, kernel.InputAt(6), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"param", {(*input_param).dims()}},
        {"in_sum_1", {(*input_in_sum_1).dims()}},
        {"in_sum_2", {(*input_in_sum_2).dims()}},
        {"in_sum_3", {(*input_in_sum_3).dims()}},
        {"in_num_accumulates", {(*input_in_num_accumulates).dims()}},
        {"in_old_num_accumulates", {(*input_in_old_num_accumulates).dims()}},
        {"in_num_updates", {(*input_in_num_updates).dims()}}};
    platform::RecordOpInfoSupplement("average_accumulates_", input_shapes);
  }

  std::tuple<Tensor&, Tensor&, Tensor&, Tensor&, Tensor&, Tensor&> api_output{
      in_sum_1,
      in_sum_2,
      in_sum_3,
      in_num_accumulates,
      in_old_num_accumulates,
      in_num_updates};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "average_accumulates_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);
  phi::MetaTensor meta_out_3(kernel_out_3);
  phi::MetaTensor meta_out_4(kernel_out_4);
  phi::MetaTensor meta_out_5(kernel_out_5);

  phi::AverageAccumulatesInferMeta(
      MakeMetaTensor(*input_param),
      MakeMetaTensor(*input_in_sum_1),
      MakeMetaTensor(*input_in_sum_2),
      MakeMetaTensor(*input_in_sum_3),
      MakeMetaTensor(*input_in_num_accumulates),
      MakeMetaTensor(*input_in_old_num_accumulates),
      MakeMetaTensor(*input_in_num_updates),
      average_window,
      max_average_window,
      min_average_window,
      kernel_out_0 ? &meta_out_0 : nullptr,
      kernel_out_1 ? &meta_out_1 : nullptr,
      kernel_out_2 ? &meta_out_2 : nullptr,
      kernel_out_3 ? &meta_out_3 : nullptr,
      kernel_out_4 ? &meta_out_4 : nullptr,
      kernel_out_5 ? &meta_out_5 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    int64_t,
                                    int64_t,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "average_accumulates_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_param,
               *input_in_sum_1,
               *input_in_sum_2,
               *input_in_sum_3,
               *input_in_num_accumulates,
               *input_in_old_num_accumulates,
               *input_in_num_updates,
               average_window,
               max_average_window,
               min_average_window,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2,
               kernel_out_3,
               kernel_out_4,
               kernel_out_5);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor>
batch_norm(const Tensor& x,
           const Tensor& scale,
           const Tensor& bias,
           const Tensor& mean,
           const Tensor& variance,
           float momentum,
           float epsilon,
           const std::string& data_layout,
           bool is_test,
           bool use_global_stats,
           bool trainable_statistics,
           bool fuse_with_relu) {
  return batch_norm_impl(x,
                         scale,
                         bias,
                         mean,
                         variance,
                         momentum,
                         epsilon,
                         data_layout,
                         is_test,
                         use_global_stats,
                         trainable_statistics,
                         fuse_with_relu);
}
PADDLE_API Tensor bce_loss(const Tensor& input, const Tensor& label) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bce_loss API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bce_loss", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bce_loss kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}, {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("bce_loss", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bce_loss infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::BCELossInferMeta(
      MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bce_loss compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_input, *input_label, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor
bicubic_interp(const Tensor& x,
               const paddle::optional<Tensor>& out_size,
               const paddle::optional<std::vector<Tensor>>& size_tensor,
               const paddle::optional<Tensor>& scale_tensor,
               const std::string& data_layout,
               int out_d,
               int out_h,
               int out_w,
               const std::vector<float>& scale,
               const std::string& interp_method,
               bool align_corners,
               int align_mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bicubic_interp API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bicubic_interp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bicubic_interp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_out_size = PrepareData(out_size, kernel.InputAt(1), {});
  auto input_size_tensor_vec = PrepareData(size_tensor, kernel.InputAt(2), {});
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec) {
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(
        input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> out_size_record_shapes;
    if (input_out_size) {
      out_size_record_shapes.push_back((*input_out_size).dims());
    }
    std::vector<phi::DDim> scale_tensor_record_shapes;
    if (input_scale_tensor) {
      scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"out_size", out_size_record_shapes},
        {"scale_tensor", scale_tensor_record_shapes}};
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    if (input_size_tensor) {
      ddims_vec.reserve(input_size_tensor->size());
      for (size_t i = 0; i < input_size_tensor->size(); ++i) {
        ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
      }
    }
    input_shapes.emplace_back("size_tensor", ddims_vec);
    platform::RecordOpInfoSupplement("bicubic_interp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bicubic_interp infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(
      size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x),
                            MakeMetaTensor(input_out_size),
                            size_tensor_metas,
                            MakeMetaTensor(input_scale_tensor),
                            data_layout,
                            out_d,
                            out_h,
                            out_w,
                            scale,
                            interp_method,
                            align_corners,
                            align_mode,
                            &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature =
      void (*)(const platform::DeviceContext&,
               const phi::DenseTensor&,
               const paddle::optional<phi::DenseTensor>&,
               const paddle::optional<std::vector<const phi::DenseTensor*>>&,
               const paddle::optional<phi::DenseTensor>&,
               const std::string&,
               int,
               int,
               int,
               const std::vector<float>&,
               const std::string&,
               bool,
               int,
               phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bicubic_interp compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_out_size,
               input_size_tensor,
               input_scale_tensor,
               data_layout,
               out_d,
               out_h,
               out_w,
               scale,
               interp_method,
               align_corners,
               align_mode,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor
bilinear_interp(const Tensor& x,
                const paddle::optional<Tensor>& out_size,
                const paddle::optional<std::vector<Tensor>>& size_tensor,
                const paddle::optional<Tensor>& scale_tensor,
                const std::string& data_layout,
                int out_d,
                int out_h,
                int out_w,
                const std::vector<float>& scale,
                const std::string& interp_method,
                bool align_corners,
                int align_mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bilinear_interp API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bilinear_interp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bilinear_interp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_out_size = PrepareData(out_size, kernel.InputAt(1), {});
  auto input_size_tensor_vec = PrepareData(size_tensor, kernel.InputAt(2), {});
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec) {
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(
        input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> out_size_record_shapes;
    if (input_out_size) {
      out_size_record_shapes.push_back((*input_out_size).dims());
    }
    std::vector<phi::DDim> scale_tensor_record_shapes;
    if (input_scale_tensor) {
      scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"out_size", out_size_record_shapes},
        {"scale_tensor", scale_tensor_record_shapes}};
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    if (input_size_tensor) {
      ddims_vec.reserve(input_size_tensor->size());
      for (size_t i = 0; i < input_size_tensor->size(); ++i) {
        ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
      }
    }
    input_shapes.emplace_back("size_tensor", ddims_vec);
    platform::RecordOpInfoSupplement("bilinear_interp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bilinear_interp infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(
      size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x),
                            MakeMetaTensor(input_out_size),
                            size_tensor_metas,
                            MakeMetaTensor(input_scale_tensor),
                            data_layout,
                            out_d,
                            out_h,
                            out_w,
                            scale,
                            interp_method,
                            align_corners,
                            align_mode,
                            &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature =
      void (*)(const platform::DeviceContext&,
               const phi::DenseTensor&,
               const paddle::optional<phi::DenseTensor>&,
               const paddle::optional<std::vector<const phi::DenseTensor*>>&,
               const paddle::optional<phi::DenseTensor>&,
               const std::string&,
               int,
               int,
               int,
               const std::vector<float>&,
               const std::string&,
               bool,
               int,
               phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bilinear_interp compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_out_size,
               input_size_tensor,
               input_scale_tensor,
               data_layout,
               out_d,
               out_h,
               out_w,
               scale,
               interp_method,
               align_corners,
               align_mode,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor
bilinear_tensor_product(const Tensor& x,
                        const Tensor& y,
                        const Tensor& weight,
                        const paddle::optional<Tensor>& bias) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bilinear_tensor_product API kernel key: [" << kernel_backend
          << ", " << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bilinear_tensor_product",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bilinear_tensor_product kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  auto input_weight = PrepareData(weight, kernel.InputAt(2), {});
  auto input_bias = PrepareData(bias, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> bias_record_shapes;
    if (input_bias) {
      bias_record_shapes.push_back((*input_bias).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"y", {(*input_y).dims()}},
        {"weight", {(*input_weight).dims()}},
        {"bias", bias_record_shapes}};
    platform::RecordOpInfoSupplement("bilinear_tensor_product", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bilinear_tensor_product infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::BilinearTensorProductInferMeta(MakeMetaTensor(*input_x),
                                      MakeMetaTensor(*input_y),
                                      MakeMetaTensor(*input_weight),
                                      MakeMetaTensor(input_bias),
                                      &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bilinear_tensor_product compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_y, *input_weight, input_bias, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor bitwise_and(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_and API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_and", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bitwise_and kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("bitwise_and", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bitwise_and infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bitwise_and compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor bitwise_not(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_not API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_not", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bitwise_not kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("bitwise_not", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bitwise_not infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bitwise_not compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor bitwise_or(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_or API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_or", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bitwise_or kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("bitwise_or", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bitwise_or infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bitwise_or compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor bitwise_xor(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bitwise_xor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bitwise_xor", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bitwise_xor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("bitwise_xor", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bitwise_xor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bitwise_xor compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor bmm(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "bmm API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bmm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "bmm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("bmm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "bmm infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::BmmInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "bmm compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor box_coder(const Tensor& prior_box,
                            const paddle::optional<Tensor>& prior_box_var,
                            const Tensor& target_box,
                            const std::string& code_type,
                            bool box_normalized,
                            int axis,
                            const std::vector<float>& variance) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(prior_box, prior_box_var, target_box);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "box_coder API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "box_coder", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "box_coder kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_prior_box = PrepareData(prior_box, kernel.InputAt(0), {});
  auto input_prior_box_var = PrepareData(prior_box_var, kernel.InputAt(1), {});
  auto input_target_box = PrepareData(target_box, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> prior_box_var_record_shapes;
    if (input_prior_box_var) {
      prior_box_var_record_shapes.push_back((*input_prior_box_var).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"prior_box", {(*input_prior_box).dims()}},
        {"prior_box_var", prior_box_var_record_shapes},
        {"target_box", {(*input_target_box).dims()}}};
    platform::RecordOpInfoSupplement("box_coder", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "box_coder infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::BoxCoderInferMeta(MakeMetaTensor(*input_prior_box),
                         MakeMetaTensor(input_prior_box_var),
                         MakeMetaTensor(*input_target_box),
                         code_type,
                         box_normalized,
                         axis,
                         variance,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    bool,
                                    int,
                                    const std::vector<float>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "box_coder compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_prior_box,
               input_prior_box_var,
               *input_target_box,
               code_type,
               box_normalized,
               axis,
               variance,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor brelu(const Tensor& x, float t_min, float t_max) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "brelu API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "brelu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "brelu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("brelu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "brelu infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "brelu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, t_min, t_max, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor cast(const Tensor& x, DataType out_dtype) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cast API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cast", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cast kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("cast", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cast infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CastInferMeta(MakeMetaTensor(*input_x), out_dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cast compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, out_dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor ceil(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "ceil API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "ceil", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "ceil kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("ceil", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "ceil infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "ceil compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& ceil_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "ceil API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "ceil", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "ceil kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("ceil", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "ceil infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "ceil compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor celu(const Tensor& x, float alpha) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "celu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "celu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "celu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("celu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "celu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "celu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, alpha, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> class_center_sample(const Tensor& label,
                                                          int num_classes,
                                                          int num_samples,
                                                          int ring_id,
                                                          int rank,
                                                          int nranks,
                                                          bool fix_seed,
                                                          int seed) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "class_center_sample API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "class_center_sample", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "class_center_sample kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_label = PrepareData(label, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("class_center_sample", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "class_center_sample infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::ClassCenterSampleInferMeta(MakeMetaTensor(*input_label),
                                  num_classes,
                                  num_samples,
                                  ring_id,
                                  rank,
                                  nranks,
                                  fix_seed,
                                  seed,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    int,
                                    int,
                                    int,
                                    bool,
                                    int,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "class_center_sample compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_label,
               num_classes,
               num_samples,
               ring_id,
               rank,
               nranks,
               fix_seed,
               seed,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor clip(const Tensor& x, const Scalar& min, const Scalar& max) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "clip API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "clip", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "clip kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("clip", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "clip infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "clip compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::Scalar(min), phi::Scalar(max), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& clip_(Tensor& x, const Scalar& min, const Scalar& max) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "clip API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "clip", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "clip kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("clip", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "clip infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "clip compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::Scalar(min), phi::Scalar(max), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor clip_by_norm(const Tensor& x, float max_norm) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "clip_by_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "clip_by_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "clip_by_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("clip_by_norm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "clip_by_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ClipByNormInferMeta(MakeMetaTensor(*input_x), max_norm, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "clip_by_norm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, max_norm, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>, Tensor> coalesce_tensor(
    const std::vector<Tensor>& input,
    DataType dtype,
    bool copy_data,
    bool set_constant,
    bool persist_output,
    float constant,
    bool use_align,
    int align_size,
    int size_of_dtype,
    const std::vector<int64_t>& concated_shapes,
    const std::vector<int64_t>& concated_ranks) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "coalesce_tensor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "coalesce_tensor", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "coalesce_tensor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input_vec = PrepareData(input, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_input(input_input_vec->size());
  for (size_t i = 0; i < input_input.size(); ++i) {
    input_input[i] = &input_input_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_input.size());
    for (size_t i = 0; i < input_input.size(); ++i) {
      ddims_vec.emplace_back((*input_input[i]).dims());
    }
    input_shapes.emplace_back("input", ddims_vec);
    platform::RecordOpInfoSupplement("coalesce_tensor", input_shapes);
  }

  std::tuple<std::vector<Tensor>, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(input.size(), &std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "coalesce_tensor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto input_meta_vec = MakeMetaTensor(input_input);
  std::vector<const phi::MetaTensor*> input_metas(input_meta_vec.size());
  for (size_t i = 0; i < input_meta_vec.size(); ++i) {
    input_metas[i] = &input_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(
      kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] =
        kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::CoalesceTensorInferMeta(input_metas,
                               dtype,
                               copy_data,
                               set_constant,
                               persist_output,
                               constant,
                               use_align,
                               align_size,
                               size_of_dtype,
                               concated_shapes,
                               concated_ranks,
                               kernel_out_0_metas,
                               kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    DataType,
                                    bool,
                                    bool,
                                    bool,
                                    float,
                                    bool,
                                    int,
                                    int,
                                    const std::vector<int64_t>&,
                                    const std::vector<int64_t>&,
                                    std::vector<phi::DenseTensor*>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "coalesce_tensor compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               input_input,
               dtype,
               copy_data,
               set_constant,
               persist_output,
               constant,
               use_align,
               align_size,
               size_of_dtype,
               concated_shapes,
               concated_ranks,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor complex(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "complex API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "complex", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "complex kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("complex", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "complex infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ComplexInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "complex compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor concat(const std::vector<Tensor>& x, const Scalar& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "concat API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "concat", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "concat kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x_vec = PrepareData(x, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_x.size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      ddims_vec.emplace_back((*input_x[i]).dims());
    }
    input_shapes.emplace_back("x", ddims_vec);
    platform::RecordOpInfoSupplement("concat", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "concat infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ConcatInferMeta(x_metas, axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "concat compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, input_x, phi::Scalar(axis), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor conj(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conj API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conj", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "conj kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("conj", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "conj infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "conj compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor conv2d(const Tensor& input,
                         const Tensor& filter,
                         const std::vector<int>& strides,
                         const std::vector<int>& paddings,
                         const std::string& padding_algorithm,
                         int groups,
                         const std::vector<int>& dilations,
                         const std::string& data_format,
                         bool use_addto,
                         int workspace_size_MB,
                         bool exhaustive_search) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv2d API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "conv2d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_filter = PrepareData(filter, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}},
        {"filter", {(*input_filter).dims()}}};
    platform::RecordOpInfoSupplement("conv2d", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "conv2d infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ConvInferMeta(MakeMetaTensor(*input_input),
                     MakeMetaTensor(*input_filter),
                     strides,
                     paddings,
                     padding_algorithm,
                     groups,
                     dilations,
                     data_format,
                     use_addto,
                     workspace_size_MB,
                     exhaustive_search,
                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::string&,
                                    int,
                                    const std::vector<int>&,
                                    const std::string&,
                                    bool,
                                    int,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "conv2d compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               *input_filter,
               strides,
               paddings,
               padding_algorithm,
               groups,
               dilations,
               data_format,
               use_addto,
               workspace_size_MB,
               exhaustive_search,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor conv2d_transpose(const Tensor& x,
                                   const Tensor& filter,
                                   const std::vector<int>& strides,
                                   const std::vector<int>& paddings,
                                   const std::vector<int>& output_padding,
                                   const std::vector<int>& output_size,
                                   const std::string& padding_algorithm,
                                   int groups,
                                   const std::vector<int>& dilations,
                                   const std::string& data_format) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv2d_transpose API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d_transpose",
      {kernel_backend, kernel_layout, kernel_data_type},
      true);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "conv2d_transpose kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_filter = PrepareData(filter, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"filter", {(*input_filter).dims()}}};
    platform::RecordOpInfoSupplement("conv2d_transpose", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "conv2d_transpose infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ConvTransposeInferMeta(MakeMetaTensor(*input_x),
                              MakeMetaTensor(*input_filter),
                              strides,
                              paddings,
                              output_padding,
                              output_size,
                              padding_algorithm,
                              groups,
                              dilations,
                              data_format,
                              &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::string&,
                                    int,
                                    const std::vector<int>&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "conv2d_transpose compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_filter,
               strides,
               paddings,
               output_padding,
               output_size,
               padding_algorithm,
               groups,
               dilations,
               data_format,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor conv3d(const Tensor& input,
                         const Tensor& filter,
                         const std::vector<int>& strides,
                         const std::vector<int>& paddings,
                         const std::string& paddding_algorithm,
                         int groups,
                         const std::vector<int>& dilations,
                         const std::string& data_format,
                         bool use_addto,
                         int workspace_size_MB,
                         bool exhaustive_search) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv3d API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv3d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "conv3d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_filter = PrepareData(filter, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}},
        {"filter", {(*input_filter).dims()}}};
    platform::RecordOpInfoSupplement("conv3d", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "conv3d infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ConvInferMeta(MakeMetaTensor(*input_input),
                     MakeMetaTensor(*input_filter),
                     strides,
                     paddings,
                     paddding_algorithm,
                     groups,
                     dilations,
                     data_format,
                     use_addto,
                     workspace_size_MB,
                     exhaustive_search,
                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::string&,
                                    int,
                                    const std::vector<int>&,
                                    const std::string&,
                                    bool,
                                    int,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "conv3d compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               *input_filter,
               strides,
               paddings,
               paddding_algorithm,
               groups,
               dilations,
               data_format,
               use_addto,
               workspace_size_MB,
               exhaustive_search,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor conv3d_transpose(const Tensor& x,
                                   const Tensor& filter,
                                   const std::vector<int>& strides,
                                   const std::vector<int>& paddings,
                                   const std::vector<int>& output_padding,
                                   const std::vector<int>& output_size,
                                   const std::string& padding_algorithm,
                                   int groups,
                                   const std::vector<int>& dilations,
                                   const std::string& data_format) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "conv3d_transpose API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv3d_transpose",
      {kernel_backend, kernel_layout, kernel_data_type},
      true);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "conv3d_transpose kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_filter = PrepareData(filter, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"filter", {(*input_filter).dims()}}};
    platform::RecordOpInfoSupplement("conv3d_transpose", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "conv3d_transpose infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ConvTransposeInferMeta(MakeMetaTensor(*input_x),
                              MakeMetaTensor(*input_filter),
                              strides,
                              paddings,
                              output_padding,
                              output_size,
                              padding_algorithm,
                              groups,
                              dilations,
                              data_format,
                              &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::string&,
                                    int,
                                    const std::vector<int>&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "conv3d_transpose compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_filter,
               strides,
               paddings,
               output_padding,
               output_size,
               padding_algorithm,
               groups,
               dilations,
               data_format,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor copy_to(const Tensor& x, const Place& place, bool blocking) {
  return copy_to_impl(x, place, blocking);
}
PADDLE_API Tensor cos(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cos API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cos", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cos kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("cos", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cos infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cos compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor cosh(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cosh API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cosh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cosh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("cosh", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cosh infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cosh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor crop_tensor(const Tensor& x,
                              const IntArray& shape,
                              const IntArray& offsets) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "crop_tensor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "crop_tensor", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "crop_tensor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("crop_tensor", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "crop_tensor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CropTensorInferMeta(MakeMetaTensor(*input_x), shape, offsets, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "crop_tensor compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::IntArray(shape),
               phi::IntArray(offsets),
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> cross_entropy_with_softmax(
    const Tensor& input,
    const Tensor& label,
    bool soft_label,
    bool use_softmax,
    bool numeric_stable_mode,
    int ignore_index,
    int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cross_entropy_with_softmax API kernel key: [" << kernel_backend
          << ", " << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cross_entropy_with_softmax",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cross_entropy_with_softmax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}, {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("cross_entropy_with_softmax",
                                     input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cross_entropy_with_softmax infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::CrossEntropyWithSoftmaxInferMeta(MakeMetaTensor(*input_input),
                                        MakeMetaTensor(*input_label),
                                        soft_label,
                                        use_softmax,
                                        numeric_stable_mode,
                                        ignore_index,
                                        axis,
                                        kernel_out_0 ? &meta_out_0 : nullptr,
                                        kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    bool,
                                    int,
                                    int,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cross_entropy_with_softmax compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               *input_label,
               soft_label,
               use_softmax,
               numeric_stable_mode,
               ignore_index,
               axis,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor cumprod(const Tensor& x, int dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cumprod API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumprod", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cumprod kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("cumprod", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cumprod infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cumprod compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor cumsum(const Tensor& x,
                         const Scalar& axis,
                         bool flatten,
                         bool exclusive,
                         bool reverse) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "cumsum API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumsum", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "cumsum kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("cumsum", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "cumsum infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CumScalarAxisInferMeta(
      MakeMetaTensor(*input_x), axis, flatten, exclusive, reverse, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    bool,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "cumsum compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::Scalar(axis),
               flatten,
               exclusive,
               reverse,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor decode_jpeg(const Tensor& x, const std::string& mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "decode_jpeg API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "decode_jpeg", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "decode_jpeg kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("decode_jpeg", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "decode_jpeg infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DecodeJpegInferMeta(MakeMetaTensor(*input_x), mode, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "decode_jpeg compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, mode, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor deformable_conv(const Tensor& x,
                                  const Tensor& offset,
                                  const Tensor& filter,
                                  const paddle::optional<Tensor>& mask,
                                  const std::vector<int>& strides,
                                  const std::vector<int>& paddings,
                                  const std::vector<int>& dilations,
                                  int deformable_groups,
                                  int groups,
                                  int im2col_step) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, offset, filter, mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "deformable_conv API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "deformable_conv", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "deformable_conv kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_offset = PrepareData(offset, kernel.InputAt(1), {});
  auto input_filter = PrepareData(filter, kernel.InputAt(2), {});
  auto input_mask = PrepareData(mask, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> mask_record_shapes;
    if (input_mask) {
      mask_record_shapes.push_back((*input_mask).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"offset", {(*input_offset).dims()}},
        {"filter", {(*input_filter).dims()}},
        {"mask", mask_record_shapes}};
    platform::RecordOpInfoSupplement("deformable_conv", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "deformable_conv infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DeformableConvInferMeta(MakeMetaTensor(*input_x),
                               MakeMetaTensor(*input_offset),
                               MakeMetaTensor(*input_filter),
                               MakeMetaTensor(input_mask),
                               strides,
                               paddings,
                               dilations,
                               deformable_groups,
                               groups,
                               im2col_step,
                               &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    int,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "deformable_conv compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_offset,
               *input_filter,
               input_mask,
               strides,
               paddings,
               dilations,
               deformable_groups,
               groups,
               im2col_step,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor depthwise_conv2d(const Tensor& x,
                                   const Tensor& filter,
                                   const std::vector<int>& strides,
                                   const std::vector<int>& paddings,
                                   const std::string& padding_algorithm,
                                   int groups,
                                   const std::vector<int>& dilations,
                                   const std::string& data_format,
                                   bool use_addto,
                                   int workspace_size_MB,
                                   bool exhaustive_search,
                                   bool fuse_relu,
                                   bool use_gpudnn) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "depthwise_conv2d API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depthwise_conv2d",
      {kernel_backend, kernel_layout, kernel_data_type},
      use_gpudnn);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "depthwise_conv2d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_filter = PrepareData(filter, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"filter", {(*input_filter).dims()}}};
    platform::RecordOpInfoSupplement("depthwise_conv2d", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "depthwise_conv2d infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ConvInferMeta(MakeMetaTensor(*input_x),
                     MakeMetaTensor(*input_filter),
                     strides,
                     paddings,
                     padding_algorithm,
                     groups,
                     dilations,
                     data_format,
                     use_addto,
                     workspace_size_MB,
                     exhaustive_search,
                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::string&,
                                    int,
                                    const std::vector<int>&,
                                    const std::string&,
                                    bool,
                                    int,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "depthwise_conv2d compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_filter,
               strides,
               paddings,
               padding_algorithm,
               groups,
               dilations,
               data_format,
               use_addto,
               workspace_size_MB,
               exhaustive_search,
               fuse_relu,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor
depthwise_conv2d_transpose(const Tensor& x,
                           const Tensor& filter,
                           const std::vector<int>& strides,
                           const std::vector<int>& paddings,
                           const std::vector<int>& output_padding,
                           const std::vector<int>& output_size,
                           const std::string& padding_algorithm,
                           int groups,
                           const std::vector<int>& dilations,
                           const std::string& data_format) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "depthwise_conv2d_transpose API kernel key: [" << kernel_backend
          << ", " << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depthwise_conv2d_transpose",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "depthwise_conv2d_transpose kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_filter = PrepareData(filter, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"filter", {(*input_filter).dims()}}};
    platform::RecordOpInfoSupplement("depthwise_conv2d_transpose",
                                     input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "depthwise_conv2d_transpose infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ConvTransposeInferMeta(MakeMetaTensor(*input_x),
                              MakeMetaTensor(*input_filter),
                              strides,
                              paddings,
                              output_padding,
                              output_size,
                              padding_algorithm,
                              groups,
                              dilations,
                              data_format,
                              &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::string&,
                                    int,
                                    const std::vector<int>&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "depthwise_conv2d_transpose compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_filter,
               strides,
               paddings,
               output_padding,
               output_size,
               padding_algorithm,
               groups,
               dilations,
               data_format,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor det(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "det API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "determinant", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "determinant kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("det", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "det infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "det compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor diag_embed(const Tensor& x, int offset, int dim1, int dim2) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "diag_embed API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diag_embed", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "diag_embed kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("diag_embed", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "diag_embed infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DiagEmbedInferMeta(
      MakeMetaTensor(*input_x), offset, dim1, dim2, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "diag_embed compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, offset, dim1, dim2, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>, std::vector<Tensor>, Tensor>
distribute_fpn_proposals(const Tensor& fpn_rois,
                         const paddle::optional<Tensor>& rois_num,
                         int min_level,
                         int max_level,
                         int refer_level,
                         int refer_scale,
                         bool pixel_offset) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(fpn_rois);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(fpn_rois, rois_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "distribute_fpn_proposals API kernel key: [" << kernel_backend
          << ", " << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "distribute_fpn_proposals",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "distribute_fpn_proposals kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_fpn_rois = PrepareData(fpn_rois, kernel.InputAt(0), {});
  auto input_rois_num = PrepareData(rois_num, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> rois_num_record_shapes;
    if (input_rois_num) {
      rois_num_record_shapes.push_back((*input_rois_num).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"fpn_rois", {(*input_fpn_rois).dims()}},
        {"rois_num", rois_num_record_shapes}};
    platform::RecordOpInfoSupplement("distribute_fpn_proposals", input_shapes);
  }

  std::tuple<std::vector<Tensor>, std::vector<Tensor>, Tensor> api_output;
  auto kernel_out_0 =
      SetKernelOutput(max_level - min_level + 1, &std::get<0>(api_output));
  auto kernel_out_1 =
      SetKernelOutput(max_level - min_level + 1, &std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "distribute_fpn_proposals infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(
      kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] =
        kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(
      kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] =
        kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::DistributeFpnProposalsInferMeta(MakeMetaTensor(*input_fpn_rois),
                                       MakeMetaTensor(input_rois_num),
                                       min_level,
                                       max_level,
                                       refer_level,
                                       refer_scale,
                                       pixel_offset,
                                       kernel_out_0_metas,
                                       kernel_out_1_metas,
                                       kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    int,
                                    int,
                                    int,
                                    int,
                                    bool,
                                    std::vector<phi::DenseTensor*>&,
                                    std::vector<phi::DenseTensor*>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "distribute_fpn_proposals compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_fpn_rois,
               input_rois_num,
               min_level,
               max_level,
               refer_level,
               refer_scale,
               pixel_offset,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor divide(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "divide API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "divide", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "divide kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("divide", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "divide infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "divide compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> dropout(
    const Tensor& x,
    const paddle::optional<Tensor>& seed_tensor,
    const Scalar& p,
    bool is_test,
    const std::string& mode,
    int seed,
    bool fix_seed) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, seed_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dropout API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dropout", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "dropout kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_seed_tensor = PrepareData(seed_tensor, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> seed_tensor_record_shapes;
    if (input_seed_tensor) {
      seed_tensor_record_shapes.push_back((*input_seed_tensor).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"seed_tensor", seed_tensor_record_shapes}};
    platform::RecordOpInfoSupplement("dropout", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "dropout infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::DropoutInferMeta(MakeMetaTensor(*input_x),
                        MakeMetaTensor(input_seed_tensor),
                        p,
                        is_test,
                        mode,
                        seed,
                        fix_seed,
                        kernel_out_0 ? &meta_out_0 : nullptr,
                        kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const phi::Scalar&,
                                    bool,
                                    const std::string&,
                                    int,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "dropout compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_seed_tensor,
               phi::Scalar(p),
               is_test,
               mode,
               seed,
               fix_seed,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> edit_distance(
    const Tensor& hyps,
    const Tensor& refs,
    const paddle::optional<Tensor>& hypslength,
    const paddle::optional<Tensor>& refslength,
    bool normalized) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(DataType::FLOAT32);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(hyps, refs, hypslength, refslength);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "edit_distance API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "edit_distance", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "edit_distance kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_hyps = PrepareData(hyps, kernel.InputAt(0), {});
  auto input_refs = PrepareData(refs, kernel.InputAt(1), {});
  auto input_hypslength = PrepareData(hypslength, kernel.InputAt(2), {});
  auto input_refslength = PrepareData(refslength, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> hypslength_record_shapes;
    if (input_hypslength) {
      hypslength_record_shapes.push_back((*input_hypslength).dims());
    }
    std::vector<phi::DDim> refslength_record_shapes;
    if (input_refslength) {
      refslength_record_shapes.push_back((*input_refslength).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"hyps", {(*input_hyps).dims()}},
        {"refs", {(*input_refs).dims()}},
        {"hypslength", hypslength_record_shapes},
        {"refslength", refslength_record_shapes}};
    platform::RecordOpInfoSupplement("edit_distance", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "edit_distance infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::EditDistanceInferMeta(MakeMetaTensor(*input_hyps),
                             MakeMetaTensor(*input_refs),
                             MakeMetaTensor(input_hypslength),
                             MakeMetaTensor(input_refslength),
                             normalized,
                             kernel_out_0 ? &meta_out_0 : nullptr,
                             kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "edit_distance compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_hyps,
               *input_refs,
               input_hypslength,
               input_refslength,
               normalized,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> eigh(const Tensor& x,
                                           const std::string& uplo) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eigh API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "eigh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("eigh", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "eigh infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::EighInferMeta(MakeMetaTensor(*input_x),
                     uplo,
                     kernel_out_0 ? &meta_out_0 : nullptr,
                     kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "eigh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, uplo, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor eigvals(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eigvals API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigvals", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "eigvals kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("eigvals", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "eigvals infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::EigvalsInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "eigvals compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> eigvalsh(const Tensor& x,
                                               const std::string& uplo,
                                               bool is_test) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eigvalsh API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigvalsh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "eigvalsh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("eigvalsh", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "eigvalsh infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::EigvalshInferMeta(MakeMetaTensor(*input_x),
                         uplo,
                         is_test,
                         kernel_out_0 ? &meta_out_0 : nullptr,
                         kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "eigvalsh compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, uplo, is_test, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, std::vector<Tensor>, std::vector<Tensor>> einsum(
    const std::vector<Tensor>& x, const std::string& equation) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "einsum API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "einsum_raw", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "einsum_raw kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x_vec = PrepareData(x, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_x.size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      ddims_vec.emplace_back((*input_x[i]).dims());
    }
    input_shapes.emplace_back("x", ddims_vec);
    platform::RecordOpInfoSupplement("einsum", input_shapes);
  }

  std::tuple<Tensor, std::vector<Tensor>, std::vector<Tensor>> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(x.size(), &std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(x.size(), &std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "einsum infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out_0(kernel_out_0);

  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(
      kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] =
        kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(
      kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] =
        kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  phi::EinsumRawInferMeta(x_metas,
                          equation,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1_metas,
                          kernel_out_2_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    const std::string&,
                                    phi::DenseTensor*,
                                    std::vector<phi::DenseTensor*>&,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "einsum compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, input_x, equation, kernel_out_0, kernel_out_1, kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor elementwise_pow(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "elementwise_pow API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elementwise_pow", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "elementwise_pow kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("elementwise_pow", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "elementwise_pow infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "elementwise_pow compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor elu(const Tensor& x, float alpha) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "elu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "elu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("elu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "elu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "elu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, alpha, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& elu_(Tensor& x, float alpha) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "elu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "elu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("elu", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "elu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "elu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, alpha, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor embedding(const Tensor& x,
                            const Tensor& weight,
                            int64_t padding_idx,
                            bool sparse) {
  return embedding_impl(x, weight, padding_idx, sparse);
}
PADDLE_API Tensor empty(const IntArray& shape,
                        DataType dtype,
                        const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "empty API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "empty", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "empty kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("empty", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "empty infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::IntArray&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "empty compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, phi::IntArray(shape), dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor empty_like(const Tensor& x,
                             DataType dtype,
                             const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackendWithInputOrder(place, x);

  kernel_data_type = ParseDataTypeWithInputOrder(dtype, x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "empty_like API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "empty_like", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "empty_like kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("empty_like", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "empty_like infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CreateLikeInferMeta(MakeMetaTensor(*input_x), dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "empty_like compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor equal(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "equal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "equal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "equal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("equal", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "equal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "equal compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor equal_all(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "equal_all API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "equal_all", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "equal_all kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("equal_all", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "equal_all infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CompareAllInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "equal_all compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor exp(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "exp API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "exp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "exp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("exp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "exp infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "exp compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& exp_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "exp API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "exp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "exp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("exp", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "exp infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "exp compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor expand(const Tensor& x, const IntArray& shape) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "expand API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expand", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "expand kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("expand", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "expand infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ExpandInferMeta(MakeMetaTensor(*input_x), shape, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "expand compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(shape), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor expand_as(const Tensor& x,
                            const paddle::optional<Tensor>& y,
                            const std::vector<int>& target_shape) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "expand_as API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expand_as", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "expand_as kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> y_record_shapes;
    if (input_y) {
      y_record_shapes.push_back((*input_y).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", y_record_shapes}};
    platform::RecordOpInfoSupplement("expand_as", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "expand_as infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ExpandAsInferMeta(MakeMetaTensor(*input_x),
                         MakeMetaTensor(input_y),
                         target_shape,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const std::vector<int>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "expand_as compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, input_y, target_shape, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor expm1(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "expm1 API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expm1", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "expm1 kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("expm1", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "expm1 infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "expm1 compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& exponential_(Tensor& x, float lambda) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "exponential_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "exponential", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "exponential kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("exponential_", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "exponential_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "exponential_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, lambda, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor eye(const Scalar& num_rows,
                      const Scalar& num_columns,
                      DataType dtype,
                      const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "eye API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eye", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "eye kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("eye", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "eye infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::EyeInferMeta(num_rows, num_columns, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "eye compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               phi::Scalar(num_rows),
               phi::Scalar(num_columns),
               dtype,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fill(const Tensor& x, const Scalar& value) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fill kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fill", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fill infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fill compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(value), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& fill_(Tensor& x, const Scalar& value) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fill kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fill", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fill infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fill compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(value), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fill_diagonal(const Tensor& x,
                                float value,
                                int offset,
                                bool wrap) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fill_diagonal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fill_diagonal", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FillDiagonalInferMeta(
      MakeMetaTensor(*input_x), value, offset, wrap, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    int,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, value, offset, wrap, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& fill_diagonal_(Tensor& x,
                                  float value,
                                  int offset,
                                  bool wrap) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fill_diagonal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fill_diagonal", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FillDiagonalInferMeta(
      MakeMetaTensor(*input_x), value, offset, wrap, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    int,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, value, offset, wrap, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fill_diagonal_tensor(
    const Tensor& x, const Tensor& y, int64_t offset, int dim1, int dim2) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal_tensor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal_tensor",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fill_diagonal_tensor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("fill_diagonal_tensor", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal_tensor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FillDiagonalTensorInferMeta(MakeMetaTensor(*input_x),
                                   MakeMetaTensor(*input_y),
                                   offset,
                                   dim1,
                                   dim2,
                                   &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int64_t,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal_tensor compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, offset, dim1, dim2, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& fill_diagonal_tensor_(
    Tensor& x, const Tensor& y, int64_t offset, int dim1, int dim2) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fill_diagonal_tensor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal_tensor",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fill_diagonal_tensor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("fill_diagonal_tensor", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal_tensor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FillDiagonalTensorInferMeta(MakeMetaTensor(*input_x),
                                   MakeMetaTensor(*input_y),
                                   offset,
                                   dim1,
                                   dim2,
                                   &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int64_t,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fill_diagonal_tensor compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, offset, dim1, dim2, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor flatten(const Tensor& x, int start_axis, int stop_axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flatten API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flatten_with_xshape", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "flatten_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("flatten", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  kernel_out_0->ShareBufferWith(*input_x);
  kernel_out_0->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation "
             "and inplace version.";
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "flatten infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::FlattenWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                  start_axis,
                                  stop_axis,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "flatten compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, start_axis, stop_axis, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor& flatten_(Tensor& x, int start_axis, int stop_axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flatten API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flatten_with_xshape", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "flatten_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("flatten", input_shapes);
  }

  std::tuple<Tensor&, Tensor> api_output{x, Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "flatten infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::FlattenWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                  start_axis,
                                  stop_axis,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "flatten compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, start_axis, stop_axis, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor flip(const Tensor& x, const std::vector<int>& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "flip API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flip", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "flip kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("flip", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "flip infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FlipInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "flip compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor floor(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "floor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "floor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("floor", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "floor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "floor compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& floor_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "floor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "floor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("floor", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "floor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "floor compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor floor_divide(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "floor_divide API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor_divide", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "floor_divide kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("floor_divide", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "floor_divide infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "floor_divide compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fmax(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fmax API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fmax", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fmax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("fmax", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fmax infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fmax compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor fmin(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fmin API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fmin", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fmin kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("fmin", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fmin infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fmin compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor frame(const Tensor& x,
                        int frame_length,
                        int hop_length,
                        int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "frame API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "frame", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "frame kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("frame", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "frame infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FrameInferMeta(
      MakeMetaTensor(*input_x), frame_length, hop_length, axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "frame compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, frame_length, hop_length, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor frobenius_norm(const Tensor& x,
                                 const std::vector<int64_t>& axis,
                                 bool keep_dim,
                                 bool reduce_all) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "frobenius_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "frobenius_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "frobenius_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("frobenius_norm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "frobenius_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceInferMetaBase(
      MakeMetaTensor(*input_x), axis, keep_dim, reduce_all, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "frobenius_norm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, keep_dim, reduce_all, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor full(const IntArray& shape,
                       const Scalar& value,
                       DataType dtype,
                       const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "full API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "full kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("full", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "full infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::IntArray&,
                                    const phi::Scalar&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "full compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, phi::IntArray(shape), phi::Scalar(value), dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& full_(Tensor& output,
                         const IntArray& shape,
                         const Scalar& value,
                         DataType dtype,
                         const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(output);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "full_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "full kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("full_", input_shapes);
  }

  Tensor& api_output = output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "full_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::IntArray&,
                                    const phi::Scalar&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "full_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, phi::IntArray(shape), phi::Scalar(value), dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor full_batch_size_like(const Tensor& input,
                                       const std::vector<int>& shape,
                                       DataType dtype,
                                       const Scalar& value,
                                       int input_dim_idx,
                                       int output_dim_idx,
                                       const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "full_batch_size_like API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full_batch_size_like",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "full_batch_size_like kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}};
    platform::RecordOpInfoSupplement("full_batch_size_like", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "full_batch_size_like infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FullBatchSizeLikeInferMeta(MakeMetaTensor(*input_input),
                                  shape,
                                  value,
                                  dtype,
                                  input_dim_idx,
                                  output_dim_idx,
                                  &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const phi::Scalar&,
                                    DataType,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "full_batch_size_like compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               shape,
               phi::Scalar(value),
               dtype,
               input_dim_idx,
               output_dim_idx,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor full_like(const Tensor& x,
                            const Scalar& value,
                            DataType dtype,
                            const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackendWithInputOrder(place, x);

  kernel_data_type = ParseDataTypeWithInputOrder(dtype, x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "full_like API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "full_like", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "full_like kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {true});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("full_like", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "full_like infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CreateLikeInferMeta(MakeMetaTensor(*input_x), dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "full_like compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(value), dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor gather(const Tensor& x,
                         const Tensor& index,
                         const Scalar& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gather API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "gather kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"index", {(*input_index).dims()}}};
    platform::RecordOpInfoSupplement("gather", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "gather infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::GatherInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "gather compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_index, phi::Scalar(axis), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor gather_nd(const Tensor& x, const Tensor& index) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gather_nd API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather_nd", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "gather_nd kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"index", {(*input_index).dims()}}};
    platform::RecordOpInfoSupplement("gather_nd", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "gather_nd infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::GatherNdInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "gather_nd compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_index, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor gather_tree(const Tensor& ids, const Tensor& parents) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(ids, parents);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gather_tree API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather_tree", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "gather_tree kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_ids = PrepareData(ids, kernel.InputAt(0), {});
  auto input_parents = PrepareData(parents, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"ids", {(*input_ids).dims()}}, {"parents", {(*input_parents).dims()}}};
    platform::RecordOpInfoSupplement("gather_tree", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "gather_tree infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::GatherTreeMeta(
      MakeMetaTensor(*input_ids), MakeMetaTensor(*input_parents), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "gather_tree compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_ids, *input_parents, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor gaussian_random(const IntArray& shape,
                                  float mean,
                                  float std,
                                  int seed,
                                  DataType dtype,
                                  const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "gaussian_random API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gaussian_random", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "gaussian_random kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("gaussian_random", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "gaussian_random infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::GaussianRandomInferMeta(shape, mean, std, seed, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::IntArray&,
                                    float,
                                    float,
                                    int,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "gaussian_random compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, phi::IntArray(shape), mean, std, seed, dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor gelu(const Tensor& x, bool approximate) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gelu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gelu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "gelu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("gelu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "gelu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "gelu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, approximate, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> generate_proposals_v2(
    const Tensor& scores,
    const Tensor& bbox_deltas,
    const Tensor& im_shape,
    const Tensor& anchors,
    const Tensor& variances,
    int pre_nms_top_n,
    int post_nms_top_n,
    float nms_thresh,
    float min_size,
    float eta,
    bool pixel_offset) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(
        scores, bbox_deltas, im_shape, anchors, variances);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "generate_proposals_v2 API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "generate_proposals_v2",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "generate_proposals_v2 kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_scores = PrepareData(scores, kernel.InputAt(0), {});
  auto input_bbox_deltas = PrepareData(bbox_deltas, kernel.InputAt(1), {});
  auto input_im_shape = PrepareData(im_shape, kernel.InputAt(2), {});
  auto input_anchors = PrepareData(anchors, kernel.InputAt(3), {});
  auto input_variances = PrepareData(variances, kernel.InputAt(4), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"scores", {(*input_scores).dims()}},
        {"bbox_deltas", {(*input_bbox_deltas).dims()}},
        {"im_shape", {(*input_im_shape).dims()}},
        {"anchors", {(*input_anchors).dims()}},
        {"variances", {(*input_variances).dims()}}};
    platform::RecordOpInfoSupplement("generate_proposals_v2", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "generate_proposals_v2 infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::GenerateProposalsV2InferMeta(MakeMetaTensor(*input_scores),
                                    MakeMetaTensor(*input_bbox_deltas),
                                    MakeMetaTensor(*input_im_shape),
                                    MakeMetaTensor(*input_anchors),
                                    MakeMetaTensor(*input_variances),
                                    pre_nms_top_n,
                                    post_nms_top_n,
                                    nms_thresh,
                                    min_size,
                                    eta,
                                    pixel_offset,
                                    kernel_out_0 ? &meta_out_0 : nullptr,
                                    kernel_out_1 ? &meta_out_1 : nullptr,
                                    kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    float,
                                    float,
                                    float,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "generate_proposals_v2 compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_scores,
               *input_bbox_deltas,
               *input_im_shape,
               *input_anchors,
               *input_variances,
               pre_nms_top_n,
               post_nms_top_n,
               nms_thresh,
               min_size,
               eta,
               pixel_offset,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor graph_send_recv(const Tensor& x,
                                  const Tensor& src_index,
                                  const Tensor& dst_index,
                                  const std::string& reduce_op,
                                  const IntArray& out_size) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, src_index, dst_index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "graph_send_recv API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "graph_send_recv", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "graph_send_recv kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_src_index = PrepareData(src_index, kernel.InputAt(1), {});
  auto input_dst_index = PrepareData(dst_index, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"src_index", {(*input_src_index).dims()}},
        {"dst_index", {(*input_dst_index).dims()}}};
    platform::RecordOpInfoSupplement("graph_send_recv", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "graph_send_recv infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::GraphSendRecvInferMeta(MakeMetaTensor(*input_x),
                              MakeMetaTensor(*input_src_index),
                              MakeMetaTensor(*input_dst_index),
                              reduce_op,
                              out_size,
                              kernel_out_0 ? &meta_out_0 : nullptr,
                              kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "graph_send_recv compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_src_index,
               *input_dst_index,
               reduce_op,
               phi::IntArray(out_size),
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor graph_send_ue_recv(const Tensor& x,
                                     const Tensor& y,
                                     const Tensor& src_index,
                                     const Tensor& dst_index,
                                     const std::string& message_op,
                                     const std::string& reduce_op,
                                     const IntArray& out_size) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, src_index, dst_index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "graph_send_ue_recv API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "graph_send_ue_recv", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "graph_send_ue_recv kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  auto input_src_index = PrepareData(src_index, kernel.InputAt(2), {});
  auto input_dst_index = PrepareData(dst_index, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"y", {(*input_y).dims()}},
        {"src_index", {(*input_src_index).dims()}},
        {"dst_index", {(*input_dst_index).dims()}}};
    platform::RecordOpInfoSupplement("graph_send_ue_recv", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "graph_send_ue_recv infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::GraphSendUERecvInferMeta(MakeMetaTensor(*input_x),
                                MakeMetaTensor(*input_y),
                                MakeMetaTensor(*input_src_index),
                                MakeMetaTensor(*input_dst_index),
                                message_op,
                                reduce_op,
                                out_size,
                                kernel_out_0 ? &meta_out_0 : nullptr,
                                kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    const std::string&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "graph_send_ue_recv compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_y,
               *input_src_index,
               *input_dst_index,
               message_op,
               reduce_op,
               phi::IntArray(out_size),
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor greater_equal(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "greater_equal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "greater_equal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "greater_equal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("greater_equal", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "greater_equal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "greater_equal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor greater_than(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "greater_than API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "greater_than", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "greater_than kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("greater_than", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "greater_than infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "greater_than compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor grid_sample(const Tensor& x,
                              const Tensor& grid,
                              const std::string& mode,
                              const std::string& padding_mode,
                              bool align_corners) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, grid);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "grid_sample API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "grid_sample", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "grid_sample kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_grid = PrepareData(grid, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"grid", {(*input_grid).dims()}}};
    platform::RecordOpInfoSupplement("grid_sample", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "grid_sample infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::GridSampleBaseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_grid), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    const std::string&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "grid_sample compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_grid,
               mode,
               padding_mode,
               align_corners,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor group_norm(const Tensor& x,
                             const paddle::optional<Tensor>& scale,
                             const paddle::optional<Tensor>& bias,
                             float epsilon,
                             int groups,
                             const std::string& data_layout) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "group_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "group_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "group_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_scale = PrepareData(scale, kernel.InputAt(1), {});
  auto input_bias = PrepareData(bias, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> scale_record_shapes;
    if (input_scale) {
      scale_record_shapes.push_back((*input_scale).dims());
    }
    std::vector<phi::DDim> bias_record_shapes;
    if (input_bias) {
      bias_record_shapes.push_back((*input_bias).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"scale", scale_record_shapes},
        {"bias", bias_record_shapes}};
    platform::RecordOpInfoSupplement("group_norm", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "group_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::GroupNormInferMeta(MakeMetaTensor(*input_x),
                          MakeMetaTensor(input_scale),
                          MakeMetaTensor(input_bias),
                          epsilon,
                          groups,
                          data_layout,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr,
                          kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    float,
                                    int,
                                    const std::string&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "group_norm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_scale,
               input_bias,
               epsilon,
               groups,
               data_layout,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor gumbel_softmax(const Tensor& x,
                                 float temperature,
                                 bool hard,
                                 int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "gumbel_softmax API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gumbel_softmax", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "gumbel_softmax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("gumbel_softmax", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "gumbel_softmax infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::GumbelSoftmaxInferMeta(
      MakeMetaTensor(*input_x), temperature, hard, axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    bool,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "gumbel_softmax compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, temperature, hard, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor hard_shrink(const Tensor& x, float threshold) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hard_shrink API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hard_shrink", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "hard_shrink kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("hard_shrink", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "hard_shrink infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "hard_shrink compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, threshold, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor hard_sigmoid(const Tensor& x, float slope, float offset) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hard_sigmoid API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hard_sigmoid", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "hard_sigmoid kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("hard_sigmoid", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "hard_sigmoid infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "hard_sigmoid compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, slope, offset, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor hard_swish(const Tensor& x,
                             float threshold,
                             float scale,
                             float offset) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hard_swish API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hard_swish", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "hard_swish kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("hard_swish", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "hard_swish infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "hard_swish compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, threshold, scale, offset, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> hierarchical_sigmoid(
    const Tensor& x,
    const Tensor& w,
    const Tensor& label,
    const paddle::optional<Tensor>& path,
    const paddle::optional<Tensor>& code,
    const paddle::optional<Tensor>& bias,
    int num_classes,
    bool remote_prefetch,
    int trainer_id,
    const std::vector<int64_t>& height_sections,
    const std::vector<std::string>& epmap,
    const std::vector<std::string>& table_names,
    bool is_sparse) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, w, label, path, code, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "hierarchical_sigmoid API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hierarchical_sigmoid",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "hierarchical_sigmoid kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_w = PrepareData(w, kernel.InputAt(1), {});
  auto input_label = PrepareData(label, kernel.InputAt(2), {});
  auto input_path = PrepareData(path, kernel.InputAt(3), {});
  auto input_code = PrepareData(code, kernel.InputAt(4), {});
  auto input_bias = PrepareData(bias, kernel.InputAt(5), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> path_record_shapes;
    if (input_path) {
      path_record_shapes.push_back((*input_path).dims());
    }
    std::vector<phi::DDim> code_record_shapes;
    if (input_code) {
      code_record_shapes.push_back((*input_code).dims());
    }
    std::vector<phi::DDim> bias_record_shapes;
    if (input_bias) {
      bias_record_shapes.push_back((*input_bias).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"w", {(*input_w).dims()}},
        {"label", {(*input_label).dims()}},
        {"path", path_record_shapes},
        {"code", code_record_shapes},
        {"bias", bias_record_shapes}};
    platform::RecordOpInfoSupplement("hierarchical_sigmoid", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "hierarchical_sigmoid infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::HierarchicalSigmoidInferMeta(MakeMetaTensor(*input_x),
                                    MakeMetaTensor(*input_w),
                                    MakeMetaTensor(*input_label),
                                    MakeMetaTensor(input_path),
                                    MakeMetaTensor(input_code),
                                    MakeMetaTensor(input_bias),
                                    num_classes,
                                    remote_prefetch,
                                    trainer_id,
                                    height_sections,
                                    epmap,
                                    table_names,
                                    is_sparse,
                                    kernel_out_0 ? &meta_out_0 : nullptr,
                                    kernel_out_1 ? &meta_out_1 : nullptr,
                                    kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    int,
                                    bool,
                                    int,
                                    const std::vector<int64_t>&,
                                    const std::vector<std::string>&,
                                    const std::vector<std::string>&,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "hierarchical_sigmoid compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_w,
               *input_label,
               input_path,
               input_code,
               input_bias,
               num_classes,
               remote_prefetch,
               trainer_id,
               height_sections,
               epmap,
               table_names,
               is_sparse,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor histogram(const Tensor& x, int64_t bins, int min, int max) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "histogram API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "histogram", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "histogram kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("histogram", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "histogram infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::HistogramInferMeta(MakeMetaTensor(*input_x), bins, min, max, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int64_t,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "histogram compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, bins, min, max, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> huber_loss(const Tensor& input,
                                                 const Tensor& label,
                                                 float delta) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "huber_loss API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "huber_loss", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "huber_loss kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}, {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("huber_loss", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "huber_loss infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::HuberLossInferMeta(MakeMetaTensor(*input_input),
                          MakeMetaTensor(*input_label),
                          delta,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "huber_loss compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_input, *input_label, delta, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor imag(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "imag API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "imag", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "imag kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("imag", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "imag infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "imag compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor increment(const Tensor& x, float value) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "increment API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "increment", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "increment kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("increment", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "increment infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::IncrementInferMeta(MakeMetaTensor(*input_x), value, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "increment compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, value, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& increment_(Tensor& x, float value) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "increment API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "increment", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "increment kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("increment", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "increment infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::IncrementInferMeta(MakeMetaTensor(*input_x), value, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "increment compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, value, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor index_add(const Tensor& x,
                            const Tensor& index,
                            const Tensor& add_value,
                            int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, add_value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_add API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_add", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "index_add kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  auto input_add_value = PrepareData(add_value, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"index", {(*input_index).dims()}},
        {"add_value", {(*input_add_value).dims()}}};
    platform::RecordOpInfoSupplement("index_add", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "index_add infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::IndexAddInferMeta(MakeMetaTensor(*input_x),
                         MakeMetaTensor(*input_index),
                         MakeMetaTensor(*input_add_value),
                         axis,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "index_add compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_index, *input_add_value, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& index_add_(Tensor& x,
                              const Tensor& index,
                              const Tensor& add_value,
                              int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, add_value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_add API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_add", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "index_add kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  auto input_add_value = PrepareData(add_value, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"index", {(*input_index).dims()}},
        {"add_value", {(*input_add_value).dims()}}};
    platform::RecordOpInfoSupplement("index_add", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "index_add infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::IndexAddInferMeta(MakeMetaTensor(*input_x),
                         MakeMetaTensor(*input_index),
                         MakeMetaTensor(*input_add_value),
                         axis,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "index_add compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_index, *input_add_value, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor index_sample(const Tensor& x, const Tensor& index) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_sample API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_sample", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "index_sample kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"index", {(*input_index).dims()}}};
    platform::RecordOpInfoSupplement("index_sample", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "index_sample infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::IndexSampleInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "index_sample compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_index, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor index_select(const Tensor& x, const Tensor& index, int dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "index_select API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_select", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "index_select kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"index", {(*input_index).dims()}}};
    platform::RecordOpInfoSupplement("index_select", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "index_select infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::IndexSelectInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "index_select compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_index, dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor instance_norm(const Tensor& x,
                                const paddle::optional<Tensor>& scale,
                                const paddle::optional<Tensor>& bias,
                                float epsilon) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "instance_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "instance_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "instance_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_scale = PrepareData(scale, kernel.InputAt(1), {});
  auto input_bias = PrepareData(bias, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> scale_record_shapes;
    if (input_scale) {
      scale_record_shapes.push_back((*input_scale).dims());
    }
    std::vector<phi::DDim> bias_record_shapes;
    if (input_bias) {
      bias_record_shapes.push_back((*input_bias).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"scale", scale_record_shapes},
        {"bias", bias_record_shapes}};
    platform::RecordOpInfoSupplement("instance_norm", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "instance_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::InstanceNormInferMeta(MakeMetaTensor(*input_x),
                             MakeMetaTensor(input_scale),
                             MakeMetaTensor(input_bias),
                             epsilon,
                             kernel_out_0 ? &meta_out_0 : nullptr,
                             kernel_out_1 ? &meta_out_1 : nullptr,
                             kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "instance_norm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_scale,
               input_bias,
               epsilon,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor inverse(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "inverse API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "inverse", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "inverse kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("inverse", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "inverse infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::InverseInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "inverse compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor is_empty(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "is_empty API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "is_empty", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "is_empty kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("is_empty", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "is_empty infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::IsEmptyInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "is_empty compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor isclose(const Tensor& x,
                          const Tensor& y,
                          const Scalar& rtol,
                          const Scalar& atol,
                          bool equal_nan) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "isclose API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "isclose", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "isclose kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("isclose", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "isclose infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ValueCompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "isclose compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_y,
               phi::Scalar(rtol),
               phi::Scalar(atol),
               equal_nan,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor isfinite(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (x.is_dense_tensor()) {
    VLOG(6) << "isfinite API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "isfinite", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "isfinite kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = PrepareData(x, kernel.InputAt(0), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("isfinite", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "isfinite infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "isfinite compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  if (x.is_selected_rows()) {
    VLOG(6) << "isfinite API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "infinite_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "infinite_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = TensorToSelectedRows(x);

    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("isfinite", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "isfinite infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::SelectedRows&,
                                      phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "isfinite compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (isfinite) for input tensors is unimplemented, please "
      "check the type of input tensors."));
}

PADDLE_API Tensor isinf(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (x.is_dense_tensor()) {
    VLOG(6) << "isinf API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "isinf", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "isinf kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = PrepareData(x, kernel.InputAt(0), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("isinf", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "isinf infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "isinf compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  if (x.is_selected_rows()) {
    VLOG(6) << "isinf API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "isinf_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "isinf_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = TensorToSelectedRows(x);

    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("isinf", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "isinf infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::SelectedRows&,
                                      phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "isinf compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (isinf) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API Tensor isnan(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (x.is_dense_tensor()) {
    VLOG(6) << "isnan API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "isnan", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "isnan kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = PrepareData(x, kernel.InputAt(0), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("isnan", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "isnan infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "isnan compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  if (x.is_selected_rows()) {
    VLOG(6) << "isnan API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "isnan_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "isnan_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = TensorToSelectedRows(x);

    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("isnan", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "isnan infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::IsfiniteInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::SelectedRows&,
                                      phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "isnan compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (isnan) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API Tensor kldiv_loss(const Tensor& x,
                             const Tensor& label,
                             const std::string& reduction) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "kldiv_loss API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kldiv_loss", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "kldiv_loss kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("kldiv_loss", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "kldiv_loss infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::KLDivInferMeta(MakeMetaTensor(*input_x),
                      MakeMetaTensor(*input_label),
                      reduction,
                      &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "kldiv_loss compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_label, reduction, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor kron(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "kron API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kron", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "kron kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("kron", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "kron infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::KronInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "kron compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> kthvalue(const Tensor& x,
                                               int k,
                                               int axis,
                                               bool keepdim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "kthvalue API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kthvalue", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "kthvalue kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("kthvalue", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "kthvalue infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::KthvalueInferMeta(MakeMetaTensor(*input_x),
                         k,
                         axis,
                         keepdim,
                         kernel_out_0 ? &meta_out_0 : nullptr,
                         kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "kthvalue compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, k, axis, keepdim, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor label_smooth(const Tensor& label,
                               const paddle::optional<Tensor>& prior_dist,
                               float epsilon) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(label);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(label, prior_dist);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "label_smooth API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "label_smooth", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "label_smooth kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_label = PrepareData(label, kernel.InputAt(0), {});
  auto input_prior_dist = PrepareData(prior_dist, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> prior_dist_record_shapes;
    if (input_prior_dist) {
      prior_dist_record_shapes.push_back((*input_prior_dist).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"label", {(*input_label).dims()}},
        {"prior_dist", prior_dist_record_shapes}};
    platform::RecordOpInfoSupplement("label_smooth", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "label_smooth infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_label), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "label_smooth compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_label, input_prior_dist, epsilon, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&,
                      Tensor&,
                      Tensor&,
                      Tensor&,
                      Tensor&,
                      paddle::optional<Tensor>&>
lamb_(Tensor& param,
      const Tensor& grad,
      const Tensor& learning_rate,
      Tensor& moment1,
      Tensor& moment2,
      Tensor& beta1_pow,
      Tensor& beta2_pow,
      paddle::optional<Tensor>& master_param,
      const paddle::optional<Tensor>& skip_update,
      float weight_decay,
      float beta1,
      float beta2,
      float epsilon,
      bool multi_precision) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param,
                                                    grad,
                                                    learning_rate,
                                                    moment1,
                                                    moment2,
                                                    beta1_pow,
                                                    beta2_pow,
                                                    master_param,
                                                    skip_update);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (param.is_dense_tensor() && grad.is_dense_tensor() &&
      learning_rate.is_dense_tensor() && moment1.is_dense_tensor() &&
      moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() &&
      beta2_pow.is_dense_tensor() &&
      (!master_param || master_param->is_dense_tensor()) &&
      (!skip_update || skip_update->is_dense_tensor())) {
    VLOG(6) << "lamb_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "lamb", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "lamb kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_grad = PrepareData(grad, kernel.InputAt(1), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(2), {});
    auto input_moment1 = PrepareData(moment1, kernel.InputAt(3), {});
    auto input_moment2 = PrepareData(moment2, kernel.InputAt(4), {});
    auto input_beta1_pow = PrepareData(beta1_pow, kernel.InputAt(5), {});
    auto input_beta2_pow = PrepareData(beta2_pow, kernel.InputAt(6), {});
    auto input_master_param = PrepareData(master_param, kernel.InputAt(7), {});
    auto input_skip_update = PrepareData(skip_update, kernel.InputAt(8), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<phi::DDim> master_param_record_shapes;
      if (input_master_param) {
        master_param_record_shapes.push_back((*input_master_param).dims());
      }
      std::vector<phi::DDim> skip_update_record_shapes;
      if (input_skip_update) {
        skip_update_record_shapes.push_back((*input_skip_update).dims());
      }
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"moment1", {(*input_moment1).dims()}},
          {"moment2", {(*input_moment2).dims()}},
          {"beta1_pow", {(*input_beta1_pow).dims()}},
          {"beta2_pow", {(*input_beta2_pow).dims()}},
          {"master_param", master_param_record_shapes},
          {"skip_update", skip_update_record_shapes}};
      platform::RecordOpInfoSupplement("lamb_", input_shapes);
    }

    std::tuple<Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               paddle::optional<Tensor>&>
        api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "lamb_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);
    phi::MetaTensor meta_out_2(kernel_out_2);
    phi::MetaTensor meta_out_3(kernel_out_3);
    phi::MetaTensor meta_out_4(kernel_out_4);
    phi::MetaTensor meta_out_5(kernel_out_5);

    phi::LambInferMeta(MakeMetaTensor(*input_param),
                       MakeMetaTensor(*input_grad),
                       MakeMetaTensor(*input_learning_rate),
                       MakeMetaTensor(*input_moment1),
                       MakeMetaTensor(*input_moment2),
                       MakeMetaTensor(*input_beta1_pow),
                       MakeMetaTensor(*input_beta2_pow),
                       MakeMetaTensor(input_master_param),
                       MakeMetaTensor(input_skip_update),
                       weight_decay,
                       beta1,
                       beta2,
                       epsilon,
                       multi_precision,
                       kernel_out_0 ? &meta_out_0 : nullptr,
                       kernel_out_1 ? &meta_out_1 : nullptr,
                       kernel_out_2 ? &meta_out_2 : nullptr,
                       kernel_out_3 ? &meta_out_3 : nullptr,
                       kernel_out_4 ? &meta_out_4 : nullptr,
                       kernel_out_5 ? &meta_out_5 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      float,
                                      float,
                                      float,
                                      float,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "lamb_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_grad,
                 *input_learning_rate,
                 *input_moment1,
                 *input_moment2,
                 *input_beta1_pow,
                 *input_beta2_pow,
                 input_master_param,
                 input_skip_update,
                 weight_decay,
                 beta1,
                 beta2,
                 epsilon,
                 multi_precision,
                 kernel_out_0,
                 kernel_out_1,
                 kernel_out_2,
                 kernel_out_3,
                 kernel_out_4,
                 kernel_out_5);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    }
    return api_output;
  }

  if (param.is_dense_tensor() && grad.is_selected_rows() &&
      learning_rate.is_dense_tensor() && moment1.is_dense_tensor() &&
      moment2.is_dense_tensor() && beta1_pow.is_dense_tensor() &&
      beta2_pow.is_dense_tensor() &&
      (!master_param || master_param->is_dense_tensor()) &&
      (!skip_update || skip_update->is_dense_tensor())) {
    VLOG(6) << "lamb_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "lamb_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "lamb_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_grad = TensorToSelectedRows(grad);

    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(2), {});
    auto input_moment1 = PrepareData(moment1, kernel.InputAt(3), {});
    auto input_moment2 = PrepareData(moment2, kernel.InputAt(4), {});
    auto input_beta1_pow = PrepareData(beta1_pow, kernel.InputAt(5), {});
    auto input_beta2_pow = PrepareData(beta2_pow, kernel.InputAt(6), {});
    auto input_master_param = PrepareData(master_param, kernel.InputAt(7), {});
    auto input_skip_update = PrepareData(skip_update, kernel.InputAt(8), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<phi::DDim> master_param_record_shapes;
      if (input_master_param) {
        master_param_record_shapes.push_back((*input_master_param).dims());
      }
      std::vector<phi::DDim> skip_update_record_shapes;
      if (input_skip_update) {
        skip_update_record_shapes.push_back((*input_skip_update).dims());
      }
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"moment1", {(*input_moment1).dims()}},
          {"moment2", {(*input_moment2).dims()}},
          {"beta1_pow", {(*input_beta1_pow).dims()}},
          {"beta2_pow", {(*input_beta2_pow).dims()}},
          {"master_param", master_param_record_shapes},
          {"skip_update", skip_update_record_shapes}};
      platform::RecordOpInfoSupplement("lamb_", input_shapes);
    }

    std::tuple<Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               Tensor&,
               paddle::optional<Tensor>&>
        api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
    auto kernel_out_5 = SetKernelOutput(std::get<5>(api_output).get_ptr());
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "lamb_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);
    phi::MetaTensor meta_out_2(kernel_out_2);
    phi::MetaTensor meta_out_3(kernel_out_3);
    phi::MetaTensor meta_out_4(kernel_out_4);
    phi::MetaTensor meta_out_5(kernel_out_5);

    phi::LambInferMeta(MakeMetaTensor(*input_param),
                       MakeMetaTensor(*input_grad),
                       MakeMetaTensor(*input_learning_rate),
                       MakeMetaTensor(*input_moment1),
                       MakeMetaTensor(*input_moment2),
                       MakeMetaTensor(*input_beta1_pow),
                       MakeMetaTensor(*input_beta2_pow),
                       MakeMetaTensor(input_master_param),
                       MakeMetaTensor(input_skip_update),
                       weight_decay,
                       beta1,
                       beta2,
                       epsilon,
                       multi_precision,
                       kernel_out_0 ? &meta_out_0 : nullptr,
                       kernel_out_1 ? &meta_out_1 : nullptr,
                       kernel_out_2 ? &meta_out_2 : nullptr,
                       kernel_out_3 ? &meta_out_3 : nullptr,
                       kernel_out_4 ? &meta_out_4 : nullptr,
                       kernel_out_5 ? &meta_out_5 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::SelectedRows&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      float,
                                      float,
                                      float,
                                      float,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "lamb_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_grad,
                 *input_learning_rate,
                 *input_moment1,
                 *input_moment2,
                 *input_beta1_pow,
                 *input_beta2_pow,
                 input_master_param,
                 input_skip_update,
                 weight_decay,
                 beta1,
                 beta2,
                 epsilon,
                 multi_precision,
                 kernel_out_0,
                 kernel_out_1,
                 kernel_out_2,
                 kernel_out_3,
                 kernel_out_4,
                 kernel_out_5);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
      TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
      TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (lamb_) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> layer_norm(
    const Tensor& x,
    const paddle::optional<Tensor>& scale,
    const paddle::optional<Tensor>& bias,
    float epsilon,
    int begin_norm_axis,
    bool is_test) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "layer_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "layer_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "layer_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_scale = PrepareData(scale, kernel.InputAt(1), {});
  auto input_bias = PrepareData(bias, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> scale_record_shapes;
    if (input_scale) {
      scale_record_shapes.push_back((*input_scale).dims());
    }
    std::vector<phi::DDim> bias_record_shapes;
    if (input_bias) {
      bias_record_shapes.push_back((*input_bias).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"scale", scale_record_shapes},
        {"bias", bias_record_shapes}};
    platform::RecordOpInfoSupplement("layer_norm", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "layer_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::LayerNormInferMeta(MakeMetaTensor(*input_x),
                          MakeMetaTensor(input_scale),
                          MakeMetaTensor(input_bias),
                          epsilon,
                          begin_norm_axis,
                          is_test,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr,
                          kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    float,
                                    int,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "layer_norm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_scale,
               input_bias,
               epsilon,
               begin_norm_axis,
               is_test,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor leaky_relu(const Tensor& x, float alpha) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "leaky_relu API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "leaky_relu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "leaky_relu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("leaky_relu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "leaky_relu infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "leaky_relu compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, alpha, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor lerp(const Tensor& x, const Tensor& y, const Tensor& weight) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lerp API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lerp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "lerp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  auto input_weight = PrepareData(weight, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"y", {(*input_y).dims()}},
        {"weight", {(*input_weight).dims()}}};
    platform::RecordOpInfoSupplement("lerp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "lerp infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::LerpInferMeta(MakeMetaTensor(*input_x),
                     MakeMetaTensor(*input_y),
                     MakeMetaTensor(*input_weight),
                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "lerp compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& lerp_(Tensor& x, const Tensor& y, const Tensor& weight) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lerp API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lerp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "lerp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  auto input_weight = PrepareData(weight, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"y", {(*input_y).dims()}},
        {"weight", {(*input_weight).dims()}}};
    platform::RecordOpInfoSupplement("lerp", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "lerp infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::LerpInferMeta(MakeMetaTensor(*input_x),
                     MakeMetaTensor(*input_y),
                     MakeMetaTensor(*input_weight),
                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "lerp compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor less_equal(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "less_equal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "less_equal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "less_equal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("less_equal", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "less_equal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "less_equal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor less_than(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "less_than API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "less_than", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "less_than kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("less_than", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "less_than infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "less_than compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor
linear_interp(const Tensor& x,
              const paddle::optional<Tensor>& out_size,
              const paddle::optional<std::vector<Tensor>>& size_tensor,
              const paddle::optional<Tensor>& scale_tensor,
              const std::string& data_layout,
              int out_d,
              int out_h,
              int out_w,
              const std::vector<float>& scale,
              const std::string& interp_method,
              bool align_corners,
              int align_mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "linear_interp API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "linear_interp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "linear_interp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_out_size = PrepareData(out_size, kernel.InputAt(1), {});
  auto input_size_tensor_vec = PrepareData(size_tensor, kernel.InputAt(2), {});
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec) {
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(
        input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> out_size_record_shapes;
    if (input_out_size) {
      out_size_record_shapes.push_back((*input_out_size).dims());
    }
    std::vector<phi::DDim> scale_tensor_record_shapes;
    if (input_scale_tensor) {
      scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"out_size", out_size_record_shapes},
        {"scale_tensor", scale_tensor_record_shapes}};
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    if (input_size_tensor) {
      ddims_vec.reserve(input_size_tensor->size());
      for (size_t i = 0; i < input_size_tensor->size(); ++i) {
        ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
      }
    }
    input_shapes.emplace_back("size_tensor", ddims_vec);
    platform::RecordOpInfoSupplement("linear_interp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "linear_interp infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(
      size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x),
                            MakeMetaTensor(input_out_size),
                            size_tensor_metas,
                            MakeMetaTensor(input_scale_tensor),
                            data_layout,
                            out_d,
                            out_h,
                            out_w,
                            scale,
                            interp_method,
                            align_corners,
                            align_mode,
                            &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature =
      void (*)(const platform::DeviceContext&,
               const phi::DenseTensor&,
               const paddle::optional<phi::DenseTensor>&,
               const paddle::optional<std::vector<const phi::DenseTensor*>>&,
               const paddle::optional<phi::DenseTensor>&,
               const std::string&,
               int,
               int,
               int,
               const std::vector<float>&,
               const std::string&,
               bool,
               int,
               phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "linear_interp compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_out_size,
               input_size_tensor,
               input_scale_tensor,
               data_layout,
               out_d,
               out_h,
               out_w,
               scale,
               interp_method,
               align_corners,
               align_mode,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor linspace(const Tensor& start,
                           const Tensor& stop,
                           const Tensor& number,
                           DataType dtype) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(dtype);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(start, stop, number);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "linspace API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "linspace", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "linspace kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_start = PrepareData(start, kernel.InputAt(0), {});
  auto input_stop = PrepareData(stop, kernel.InputAt(1), {});
  auto input_number = PrepareData(number, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"start", {(*input_start).dims()}},
        {"stop", {(*input_stop).dims()}},
        {"number", {(*input_number).dims()}}};
    platform::RecordOpInfoSupplement("linspace", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "linspace infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::LinspaceInferMeta(MakeMetaTensor(*input_start),
                         MakeMetaTensor(*input_stop),
                         MakeMetaTensor(*input_number),
                         dtype,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "linspace compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_start, *input_stop, *input_number, dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor log(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "log kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("log", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "log infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "log compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor log10(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log10 API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log10", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "log10 kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("log10", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "log10 infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "log10 compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor log1p(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log1p API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log1p", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "log1p kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("log1p", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "log1p infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "log1p compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor log2(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log2 API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log2", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "log2 kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("log2", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "log2 infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "log2 compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor log_loss(const Tensor& input,
                           const Tensor& label,
                           float epsilon) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log_loss API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_loss", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "log_loss kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}, {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("log_loss", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "log_loss infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::LogLossInferMeta(MakeMetaTensor(*input_input),
                        MakeMetaTensor(*input_label),
                        epsilon,
                        &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "log_loss compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_input, *input_label, epsilon, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor log_softmax(const Tensor& x, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "log_softmax API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_softmax", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "log_softmax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("log_softmax", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "log_softmax infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMetaCheckAxis(MakeMetaTensor(*input_x), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "log_softmax compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logcumsumexp(
    const Tensor& x, int axis, bool flatten, bool exclusive, bool reverse) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logcumsumexp API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logcumsumexp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logcumsumexp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("logcumsumexp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logcumsumexp infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CumInferMeta(
      MakeMetaTensor(*input_x), axis, flatten, exclusive, reverse, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    bool,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logcumsumexp compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, axis, flatten, exclusive, reverse, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logical_and(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_and API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_and", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logical_and kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("logical_and", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logical_and infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logical_and compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logical_not(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_not API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_not", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logical_not kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("logical_not", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logical_not infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logical_not compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logical_or(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_or API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_or", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logical_or kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("logical_or", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logical_or infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logical_or compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logical_xor(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logical_xor API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logical_xor", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logical_xor kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("logical_xor", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logical_xor infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logical_xor compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logit(const Tensor& x, float eps) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logit API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logit", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logit kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("logit", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logit infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logit compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, eps, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logsigmoid(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logsigmoid API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logsigmoid", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logsigmoid kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("logsigmoid", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logsigmoid infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logsigmoid compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor logsumexp(const Tensor& x,
                            const std::vector<int64_t>& axis,
                            bool keepdim,
                            bool reduce_all) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "logsumexp API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logsumexp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "logsumexp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("logsumexp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "logsumexp infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::LogsumexpInferMeta(
      MakeMetaTensor(*input_x), axis, keepdim, reduce_all, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "logsumexp compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, reduce_all, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> lstsq(
    const Tensor& x,
    const Tensor& y,
    const Scalar& rcond,
    const std::string& driver) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lstsq API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lstsq", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "lstsq kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("lstsq", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "lstsq infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);
  phi::MetaTensor meta_out_3(kernel_out_3);

  phi::LstsqInferMeta(MakeMetaTensor(*input_x),
                      MakeMetaTensor(*input_y),
                      rcond,
                      driver,
                      kernel_out_0 ? &meta_out_0 : nullptr,
                      kernel_out_1 ? &meta_out_1 : nullptr,
                      kernel_out_2 ? &meta_out_2 : nullptr,
                      kernel_out_3 ? &meta_out_3 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    const std::string&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "lstsq compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_y,
               phi::Scalar(rcond),
               driver,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2,
               kernel_out_3);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> lu(const Tensor& x, bool pivot) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "lu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("lu", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "lu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::LUInferMeta(MakeMetaTensor(*input_x),
                   pivot,
                   kernel_out_0 ? &meta_out_0 : nullptr,
                   kernel_out_1 ? &meta_out_1 : nullptr,
                   kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "lu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, pivot, kernel_out_0, kernel_out_1, kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> lu_unpack(const Tensor& x,
                                                        const Tensor& pivots,
                                                        bool unpack_ludata,
                                                        bool unpack_pivots) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, pivots);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "lu_unpack API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lu_unpack", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "lu_unpack kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_pivots = PrepareData(pivots, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"pivots", {(*input_pivots).dims()}}};
    platform::RecordOpInfoSupplement("lu_unpack", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "lu_unpack infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::LUUnpackInferMeta(MakeMetaTensor(*input_x),
                         MakeMetaTensor(*input_pivots),
                         unpack_ludata,
                         unpack_pivots,
                         kernel_out_0 ? &meta_out_0 : nullptr,
                         kernel_out_1 ? &meta_out_1 : nullptr,
                         kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "lu_unpack compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_pivots,
               unpack_ludata,
               unpack_pivots,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> margin_cross_entropy(const Tensor& logits,
                                                           const Tensor& label,
                                                           bool return_softmax,
                                                           int ring_id,
                                                           int rank,
                                                           int nranks,
                                                           float margin1,
                                                           float margin2,
                                                           float margin3,
                                                           float scale) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(logits);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(logits, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "margin_cross_entropy API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "margin_cross_entropy",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "margin_cross_entropy kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_logits = PrepareData(logits, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"logits", {(*input_logits).dims()}},
        {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("margin_cross_entropy", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "margin_cross_entropy infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::MarginCrossEntropyInferMeta(MakeMetaTensor(*input_logits),
                                   MakeMetaTensor(*input_label),
                                   return_softmax,
                                   ring_id,
                                   rank,
                                   nranks,
                                   margin1,
                                   margin2,
                                   margin3,
                                   scale,
                                   kernel_out_0 ? &meta_out_0 : nullptr,
                                   kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    int,
                                    int,
                                    int,
                                    float,
                                    float,
                                    float,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "margin_cross_entropy compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_logits,
               *input_label,
               return_softmax,
               ring_id,
               rank,
               nranks,
               margin1,
               margin2,
               margin3,
               scale,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor masked_select(const Tensor& x, const Tensor& mask) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "masked_select API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "masked_select", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "masked_select kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_mask = PrepareData(mask, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"mask", {(*input_mask).dims()}}};
    platform::RecordOpInfoSupplement("masked_select", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "masked_select infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MaskedSelectInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_mask), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "masked_select compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_mask, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor matmul(const Tensor& x,
                         const Tensor& y,
                         bool transpose_x,
                         bool transpose_y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matmul API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matmul", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "matmul kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("matmul", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "matmul infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MatmulInferMeta(MakeMetaTensor(*input_x),
                       MakeMetaTensor(*input_y),
                       transpose_x,
                       transpose_y,
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "matmul compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_y, transpose_x, transpose_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> matrix_nms(const Tensor& bboxes,
                                                         const Tensor& scores,
                                                         float score_threshold,
                                                         int nms_top_k,
                                                         int keep_top_k,
                                                         float post_threshold,
                                                         bool use_gaussian,
                                                         float gaussian_sigma,
                                                         int background_label,
                                                         bool normalized) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(bboxes, scores);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_nms API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_nms", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "matrix_nms kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_bboxes = PrepareData(bboxes, kernel.InputAt(0), {});
  auto input_scores = PrepareData(scores, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"bboxes", {(*input_bboxes).dims()}},
        {"scores", {(*input_scores).dims()}}};
    platform::RecordOpInfoSupplement("matrix_nms", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "matrix_nms infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::MatrixNMSInferMeta(MakeMetaTensor(*input_bboxes),
                          MakeMetaTensor(*input_scores),
                          score_threshold,
                          nms_top_k,
                          keep_top_k,
                          post_threshold,
                          use_gaussian,
                          gaussian_sigma,
                          background_label,
                          normalized,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr,
                          kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    int,
                                    int,
                                    float,
                                    bool,
                                    float,
                                    int,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "matrix_nms compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_bboxes,
               *input_scores,
               score_threshold,
               nms_top_k,
               keep_top_k,
               post_threshold,
               use_gaussian,
               gaussian_sigma,
               background_label,
               normalized,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor matrix_power(const Tensor& x, int n) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_power API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_power", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "matrix_power kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("matrix_power", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "matrix_power infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "matrix_power compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, n, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor matrix_rank(const Tensor& x,
                              float tol,
                              bool use_default_tol,
                              bool hermitian) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_rank API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_rank", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "matrix_rank kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("matrix_rank", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "matrix_rank infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MatrixRankInferMeta(
      MakeMetaTensor(*input_x), use_default_tol, hermitian, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "matrix_rank compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, tol, use_default_tol, hermitian, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor matrix_rank_tol(const Tensor& x,
                                  const Tensor& atol_tensor,
                                  bool use_default_tol,
                                  bool hermitian) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, atol_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "matrix_rank_tol API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_rank_tol", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "matrix_rank_tol kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_atol_tensor = PrepareData(atol_tensor, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"atol_tensor", {(*input_atol_tensor).dims()}}};
    platform::RecordOpInfoSupplement("matrix_rank_tol", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "matrix_rank_tol infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MatrixRankTolInferMeta(MakeMetaTensor(*input_x),
                              MakeMetaTensor(*input_atol_tensor),
                              use_default_tol,
                              hermitian,
                              &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "matrix_rank_tol compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_atol_tensor,
               use_default_tol,
               hermitian,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor max(const Tensor& x, const IntArray& dims, bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "max API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "max kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("max", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "max infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceIntArrayAxisInferMeta(
      MakeMetaTensor(*input_x), dims, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "max compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(dims), keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> max_pool2d_with_index(
    const Tensor& x,
    const std::vector<int>& kernel_size,
    const std::vector<int>& strides,
    const std::vector<int>& paddings,
    bool global_pooling,
    bool adaptive) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "max_pool2d_with_index API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max_pool2d_with_index",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "max_pool2d_with_index kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("max_pool2d_with_index", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "max_pool2d_with_index infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::MaxPoolWithIndexInferMeta(MakeMetaTensor(*input_x),
                                 kernel_size,
                                 strides,
                                 paddings,
                                 global_pooling,
                                 adaptive,
                                 kernel_out_0 ? &meta_out_0 : nullptr,
                                 kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "max_pool2d_with_index compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               kernel_size,
               strides,
               paddings,
               global_pooling,
               adaptive,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> max_pool3d_with_index(
    const Tensor& x,
    const std::vector<int>& kernel_size,
    const std::vector<int>& strides,
    const std::vector<int>& paddings,
    bool global_pooling,
    bool adaptive) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "max_pool3d_with_index API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max_pool3d_with_index",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "max_pool3d_with_index kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("max_pool3d_with_index", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "max_pool3d_with_index infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::MaxPoolWithIndexInferMeta(MakeMetaTensor(*input_x),
                                 kernel_size,
                                 strides,
                                 paddings,
                                 global_pooling,
                                 adaptive,
                                 kernel_out_0 ? &meta_out_0 : nullptr,
                                 kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "max_pool3d_with_index compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               kernel_size,
               strides,
               paddings,
               global_pooling,
               adaptive,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor maximum(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "maximum API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "maximum", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "maximum kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("maximum", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "maximum infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "maximum compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor maxout(const Tensor& x, int groups, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "maxout API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "maxout", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "maxout kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("maxout", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "maxout infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MaxOutInferMeta(MakeMetaTensor(*input_x), groups, axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "maxout compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, groups, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor mean(const Tensor& x, const IntArray& dims, bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mean API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mean", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "mean kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("mean", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "mean infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceIntArrayAxisInferMeta(
      MakeMetaTensor(*input_x), dims, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "mean compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(dims), keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor mean_all(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mean_all API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mean_all", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "mean_all kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("mean_all", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "mean_all infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MeanAllInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "mean_all compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>&,
                      std::vector<Tensor>&,
                      std::vector<Tensor>&,
                      std::vector<Tensor>&,
                      std::vector<Tensor>&,
                      paddle::optional<std::vector<Tensor>>&>
merged_adam_(std::vector<Tensor>& param,
             const std::vector<Tensor>& grad,
             const std::vector<Tensor>& learning_rate,
             std::vector<Tensor>& moment1,
             std::vector<Tensor>& moment2,
             std::vector<Tensor>& beta1_pow,
             std::vector<Tensor>& beta2_pow,
             paddle::optional<std::vector<Tensor>>& master_param,
             const Scalar& beta1,
             const Scalar& beta2,
             const Scalar& epsilon,
             bool multi_precision,
             bool use_global_beta_pow) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(param,
                                                    grad,
                                                    learning_rate,
                                                    moment1,
                                                    moment2,
                                                    beta1_pow,
                                                    beta2_pow,
                                                    master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "merged_adam_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "merged_adam", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "merged_adam kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  std::vector<const phi::DenseTensor*> input_param =
      TensorToConstDenseTensorPtr(param);
  auto input_grad_vec = PrepareData(grad, kernel.InputAt(1), {});
  std::vector<const phi::DenseTensor*> input_grad(input_grad_vec->size());
  for (size_t i = 0; i < input_grad.size(); ++i) {
    input_grad[i] = &input_grad_vec->at(i);
  }
  auto input_learning_rate_vec =
      PrepareData(learning_rate, kernel.InputAt(2), {});
  std::vector<const phi::DenseTensor*> input_learning_rate(
      input_learning_rate_vec->size());
  for (size_t i = 0; i < input_learning_rate.size(); ++i) {
    input_learning_rate[i] = &input_learning_rate_vec->at(i);
  }
  std::vector<const phi::DenseTensor*> input_moment1 =
      TensorToConstDenseTensorPtr(moment1);
  std::vector<const phi::DenseTensor*> input_moment2 =
      TensorToConstDenseTensorPtr(moment2);
  std::vector<const phi::DenseTensor*> input_beta1_pow =
      TensorToConstDenseTensorPtr(beta1_pow);
  std::vector<const phi::DenseTensor*> input_beta2_pow =
      TensorToConstDenseTensorPtr(beta2_pow);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_master_param =
      TensorToConstDenseTensorPtr(master_param);
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_param.size());
    for (size_t i = 0; i < input_param.size(); ++i) {
      ddims_vec.emplace_back((*input_param[i]).dims());
    }
    input_shapes.emplace_back("param", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_grad.size());
    for (size_t i = 0; i < input_grad.size(); ++i) {
      ddims_vec.emplace_back((*input_grad[i]).dims());
    }
    input_shapes.emplace_back("grad", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_learning_rate.size());
    for (size_t i = 0; i < input_learning_rate.size(); ++i) {
      ddims_vec.emplace_back((*input_learning_rate[i]).dims());
    }
    input_shapes.emplace_back("learning_rate", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_moment1.size());
    for (size_t i = 0; i < input_moment1.size(); ++i) {
      ddims_vec.emplace_back((*input_moment1[i]).dims());
    }
    input_shapes.emplace_back("moment1", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_moment2.size());
    for (size_t i = 0; i < input_moment2.size(); ++i) {
      ddims_vec.emplace_back((*input_moment2[i]).dims());
    }
    input_shapes.emplace_back("moment2", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_beta1_pow.size());
    for (size_t i = 0; i < input_beta1_pow.size(); ++i) {
      ddims_vec.emplace_back((*input_beta1_pow[i]).dims());
    }
    input_shapes.emplace_back("beta1_pow", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_beta2_pow.size());
    for (size_t i = 0; i < input_beta2_pow.size(); ++i) {
      ddims_vec.emplace_back((*input_beta2_pow[i]).dims());
    }
    input_shapes.emplace_back("beta2_pow", ddims_vec);
    ddims_vec.clear();
    if (input_master_param) {
      ddims_vec.reserve(input_master_param->size());
      for (size_t i = 0; i < input_master_param->size(); ++i) {
        ddims_vec.emplace_back((*input_master_param->at(i)).dims());
      }
    }
    input_shapes.emplace_back("master_param", ddims_vec);
    platform::RecordOpInfoSupplement("merged_adam_", input_shapes);
  }

  std::tuple<std::vector<Tensor>&,
             std::vector<Tensor>&,
             std::vector<Tensor>&,
             std::vector<Tensor>&,
             std::vector<Tensor>&,
             paddle::optional<std::vector<Tensor>>&>
      api_output{param, moment1, moment2, beta1_pow, beta2_pow, master_param};
  auto kernel_out_0 =
      SetInplaceVectorKernelOutput(param.size(), &std::get<0>(api_output));
  auto kernel_out_1 =
      SetInplaceVectorKernelOutput(param.size(), &std::get<1>(api_output));
  auto kernel_out_2 =
      SetInplaceVectorKernelOutput(param.size(), &std::get<2>(api_output));
  auto kernel_out_3 =
      SetInplaceVectorKernelOutput(param.size(), &std::get<3>(api_output));
  auto kernel_out_4 =
      SetInplaceVectorKernelOutput(param.size(), &std::get<4>(api_output));
  auto kernel_out_5 = SetInplaceOptionalVectorKernelOutput(
      param.size(), std::get<5>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "merged_adam_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto param_meta_vec = MakeMetaTensor(input_param);
  std::vector<const phi::MetaTensor*> param_metas(param_meta_vec.size());
  for (size_t i = 0; i < param_meta_vec.size(); ++i) {
    param_metas[i] = &param_meta_vec[i];
  }

  auto grad_meta_vec = MakeMetaTensor(input_grad);
  std::vector<const phi::MetaTensor*> grad_metas(grad_meta_vec.size());
  for (size_t i = 0; i < grad_meta_vec.size(); ++i) {
    grad_metas[i] = &grad_meta_vec[i];
  }

  auto learning_rate_meta_vec = MakeMetaTensor(input_learning_rate);
  std::vector<const phi::MetaTensor*> learning_rate_metas(
      learning_rate_meta_vec.size());
  for (size_t i = 0; i < learning_rate_meta_vec.size(); ++i) {
    learning_rate_metas[i] = &learning_rate_meta_vec[i];
  }

  auto moment1_meta_vec = MakeMetaTensor(input_moment1);
  std::vector<const phi::MetaTensor*> moment1_metas(moment1_meta_vec.size());
  for (size_t i = 0; i < moment1_meta_vec.size(); ++i) {
    moment1_metas[i] = &moment1_meta_vec[i];
  }

  auto moment2_meta_vec = MakeMetaTensor(input_moment2);
  std::vector<const phi::MetaTensor*> moment2_metas(moment2_meta_vec.size());
  for (size_t i = 0; i < moment2_meta_vec.size(); ++i) {
    moment2_metas[i] = &moment2_meta_vec[i];
  }

  auto beta1_pow_meta_vec = MakeMetaTensor(input_beta1_pow);
  std::vector<const phi::MetaTensor*> beta1_pow_metas(
      beta1_pow_meta_vec.size());
  for (size_t i = 0; i < beta1_pow_meta_vec.size(); ++i) {
    beta1_pow_metas[i] = &beta1_pow_meta_vec[i];
  }

  auto beta2_pow_meta_vec = MakeMetaTensor(input_beta2_pow);
  std::vector<const phi::MetaTensor*> beta2_pow_metas(
      beta2_pow_meta_vec.size());
  for (size_t i = 0; i < beta2_pow_meta_vec.size(); ++i) {
    beta2_pow_metas[i] = &beta2_pow_meta_vec[i];
  }

  auto master_param_meta_vec = MakeMetaTensor(input_master_param);
  paddle::optional<std::vector<const phi::MetaTensor*>> master_param_metas(
      master_param_meta_vec.size());
  for (size_t i = 0; i < master_param_meta_vec.size(); ++i) {
    master_param_metas->at(i) = &master_param_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(
      kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] =
        kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(
      kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] =
        kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(
      kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] =
        kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  auto kernel_out_3_meta_vec = MakeMetaTensor(kernel_out_3);
  std::vector<phi::MetaTensor*> kernel_out_3_metas(
      kernel_out_3_meta_vec.size());
  for (size_t i = 0; i < kernel_out_3_meta_vec.size(); ++i) {
    kernel_out_3_metas[i] =
        kernel_out_3[i] ? &kernel_out_3_meta_vec[i] : nullptr;
  }
  auto kernel_out_4_meta_vec = MakeMetaTensor(kernel_out_4);
  std::vector<phi::MetaTensor*> kernel_out_4_metas(
      kernel_out_4_meta_vec.size());
  for (size_t i = 0; i < kernel_out_4_meta_vec.size(); ++i) {
    kernel_out_4_metas[i] =
        kernel_out_4[i] ? &kernel_out_4_meta_vec[i] : nullptr;
  }
  auto kernel_out_5_meta_vec = MakeMetaTensor(kernel_out_5);
  std::vector<phi::MetaTensor*> kernel_out_5_metas(
      kernel_out_5_meta_vec.size());
  for (size_t i = 0; i < kernel_out_5_meta_vec.size(); ++i) {
    kernel_out_5_metas[i] =
        kernel_out_5[i] ? &kernel_out_5_meta_vec[i] : nullptr;
  }
  phi::MergedAdamInferMeta(param_metas,
                           grad_metas,
                           learning_rate_metas,
                           moment1_metas,
                           moment2_metas,
                           beta1_pow_metas,
                           beta2_pow_metas,
                           master_param_metas,
                           beta1,
                           beta2,
                           epsilon,
                           multi_precision,
                           use_global_beta_pow,
                           kernel_out_0_metas,
                           kernel_out_1_metas,
                           kernel_out_2_metas,
                           kernel_out_3_metas,
                           kernel_out_4_metas,
                           kernel_out_5_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature =
      void (*)(const platform::DeviceContext&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const paddle::optional<std::vector<const phi::DenseTensor*>>&,
               const phi::Scalar&,
               const phi::Scalar&,
               const phi::Scalar&,
               bool,
               bool,
               std::vector<phi::DenseTensor*>&,
               std::vector<phi::DenseTensor*>&,
               std::vector<phi::DenseTensor*>&,
               std::vector<phi::DenseTensor*>&,
               std::vector<phi::DenseTensor*>&,
               std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "merged_adam_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               input_param,
               input_grad,
               input_learning_rate,
               input_moment1,
               input_moment2,
               input_beta1_pow,
               input_beta2_pow,
               input_master_param,
               phi::Scalar(beta1),
               phi::Scalar(beta2),
               phi::Scalar(epsilon),
               multi_precision,
               use_global_beta_pow,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2,
               kernel_out_3,
               kernel_out_4,
               kernel_out_5);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
  }
  return api_output;
}

PADDLE_API std::tuple<std::vector<Tensor>&,
                      std::vector<Tensor>&,
                      paddle::optional<std::vector<Tensor>>&>
merged_momentum_(std::vector<Tensor>& param,
                 const std::vector<Tensor>& grad,
                 std::vector<Tensor>& velocity,
                 const std::vector<Tensor>& learning_rate,
                 paddle::optional<std::vector<Tensor>>& master_param,
                 float mu,
                 bool use_nesterov,
                 const std::vector<std::string>& regularization_method,
                 const std::vector<float>& regularization_coeff,
                 bool multi_precision,
                 float rescale_grad) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(
        param, grad, velocity, learning_rate, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "merged_momentum_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "merged_momentum", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "merged_momentum kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  std::vector<const phi::DenseTensor*> input_param =
      TensorToConstDenseTensorPtr(param);
  auto input_grad_vec = PrepareData(grad, kernel.InputAt(1), {});
  std::vector<const phi::DenseTensor*> input_grad(input_grad_vec->size());
  for (size_t i = 0; i < input_grad.size(); ++i) {
    input_grad[i] = &input_grad_vec->at(i);
  }
  std::vector<const phi::DenseTensor*> input_velocity =
      TensorToConstDenseTensorPtr(velocity);
  auto input_learning_rate_vec =
      PrepareData(learning_rate, kernel.InputAt(3), {});
  std::vector<const phi::DenseTensor*> input_learning_rate(
      input_learning_rate_vec->size());
  for (size_t i = 0; i < input_learning_rate.size(); ++i) {
    input_learning_rate[i] = &input_learning_rate_vec->at(i);
  }
  paddle::optional<std::vector<const phi::DenseTensor*>> input_master_param =
      TensorToConstDenseTensorPtr(master_param);
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_param.size());
    for (size_t i = 0; i < input_param.size(); ++i) {
      ddims_vec.emplace_back((*input_param[i]).dims());
    }
    input_shapes.emplace_back("param", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_grad.size());
    for (size_t i = 0; i < input_grad.size(); ++i) {
      ddims_vec.emplace_back((*input_grad[i]).dims());
    }
    input_shapes.emplace_back("grad", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_velocity.size());
    for (size_t i = 0; i < input_velocity.size(); ++i) {
      ddims_vec.emplace_back((*input_velocity[i]).dims());
    }
    input_shapes.emplace_back("velocity", ddims_vec);
    ddims_vec.clear();
    ddims_vec.reserve(input_learning_rate.size());
    for (size_t i = 0; i < input_learning_rate.size(); ++i) {
      ddims_vec.emplace_back((*input_learning_rate[i]).dims());
    }
    input_shapes.emplace_back("learning_rate", ddims_vec);
    ddims_vec.clear();
    if (input_master_param) {
      ddims_vec.reserve(input_master_param->size());
      for (size_t i = 0; i < input_master_param->size(); ++i) {
        ddims_vec.emplace_back((*input_master_param->at(i)).dims());
      }
    }
    input_shapes.emplace_back("master_param", ddims_vec);
    platform::RecordOpInfoSupplement("merged_momentum_", input_shapes);
  }

  std::tuple<std::vector<Tensor>&,
             std::vector<Tensor>&,
             paddle::optional<std::vector<Tensor>>&>
      api_output{param, velocity, master_param};
  auto kernel_out_0 =
      SetInplaceVectorKernelOutput(param.size(), &std::get<0>(api_output));
  auto kernel_out_1 =
      SetInplaceVectorKernelOutput(param.size(), &std::get<1>(api_output));
  auto kernel_out_2 = SetInplaceOptionalVectorKernelOutput(
      param.size(), std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "merged_momentum_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto param_meta_vec = MakeMetaTensor(input_param);
  std::vector<const phi::MetaTensor*> param_metas(param_meta_vec.size());
  for (size_t i = 0; i < param_meta_vec.size(); ++i) {
    param_metas[i] = &param_meta_vec[i];
  }

  auto grad_meta_vec = MakeMetaTensor(input_grad);
  std::vector<const phi::MetaTensor*> grad_metas(grad_meta_vec.size());
  for (size_t i = 0; i < grad_meta_vec.size(); ++i) {
    grad_metas[i] = &grad_meta_vec[i];
  }

  auto velocity_meta_vec = MakeMetaTensor(input_velocity);
  std::vector<const phi::MetaTensor*> velocity_metas(velocity_meta_vec.size());
  for (size_t i = 0; i < velocity_meta_vec.size(); ++i) {
    velocity_metas[i] = &velocity_meta_vec[i];
  }

  auto learning_rate_meta_vec = MakeMetaTensor(input_learning_rate);
  std::vector<const phi::MetaTensor*> learning_rate_metas(
      learning_rate_meta_vec.size());
  for (size_t i = 0; i < learning_rate_meta_vec.size(); ++i) {
    learning_rate_metas[i] = &learning_rate_meta_vec[i];
  }

  auto master_param_meta_vec = MakeMetaTensor(input_master_param);
  paddle::optional<std::vector<const phi::MetaTensor*>> master_param_metas(
      master_param_meta_vec.size());
  for (size_t i = 0; i < master_param_meta_vec.size(); ++i) {
    master_param_metas->at(i) = &master_param_meta_vec[i];
  }

  auto kernel_out_0_meta_vec = MakeMetaTensor(kernel_out_0);
  std::vector<phi::MetaTensor*> kernel_out_0_metas(
      kernel_out_0_meta_vec.size());
  for (size_t i = 0; i < kernel_out_0_meta_vec.size(); ++i) {
    kernel_out_0_metas[i] =
        kernel_out_0[i] ? &kernel_out_0_meta_vec[i] : nullptr;
  }
  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(
      kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] =
        kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(
      kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] =
        kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  phi::MergedMomentumInferMeta(param_metas,
                               grad_metas,
                               velocity_metas,
                               learning_rate_metas,
                               master_param_metas,
                               mu,
                               use_nesterov,
                               regularization_method,
                               regularization_coeff,
                               multi_precision,
                               rescale_grad,
                               kernel_out_0_metas,
                               kernel_out_1_metas,
                               kernel_out_2_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature =
      void (*)(const platform::DeviceContext&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const std::vector<const phi::DenseTensor*>&,
               const paddle::optional<std::vector<const phi::DenseTensor*>>&,
               float,
               bool,
               const std::vector<std::string>&,
               const std::vector<float>&,
               bool,
               float,
               std::vector<phi::DenseTensor*>&,
               std::vector<phi::DenseTensor*>&,
               std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "merged_momentum_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               input_param,
               input_grad,
               input_velocity,
               input_learning_rate,
               input_master_param,
               mu,
               use_nesterov,
               regularization_method,
               regularization_coeff,
               multi_precision,
               rescale_grad,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API std::vector<Tensor> meshgrid(const std::vector<Tensor>& inputs) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(inputs);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "meshgrid API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "meshgrid", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "meshgrid kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_inputs_vec = PrepareData(inputs, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_inputs(input_inputs_vec->size());
  for (size_t i = 0; i < input_inputs.size(); ++i) {
    input_inputs[i] = &input_inputs_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_inputs.size());
    for (size_t i = 0; i < input_inputs.size(); ++i) {
      ddims_vec.emplace_back((*input_inputs[i]).dims());
    }
    input_shapes.emplace_back("inputs", ddims_vec);
    platform::RecordOpInfoSupplement("meshgrid", input_shapes);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(inputs.size(), &api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "meshgrid infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto inputs_meta_vec = MakeMetaTensor(input_inputs);
  std::vector<const phi::MetaTensor*> inputs_metas(inputs_meta_vec.size());
  for (size_t i = 0; i < inputs_meta_vec.size(); ++i) {
    inputs_metas[i] = &inputs_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::MeshgridInferMeta(inputs_metas, kernel_out_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "meshgrid compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, input_inputs, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor min(const Tensor& x, const IntArray& dims, bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "min API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "min", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "min kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("min", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "min infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceIntArrayAxisInferMeta(
      MakeMetaTensor(*input_x), dims, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "min compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(dims), keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor minimum(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "minimum API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "minimum", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "minimum kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("minimum", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "minimum infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "minimum compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor mish(const Tensor& x, float lambda) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mish API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mish", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "mish kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("mish", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "mish infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "mish compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, lambda, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> mode(const Tensor& x,
                                           int axis,
                                           bool keepdim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "mode API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mode", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "mode kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("mode", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "mode infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::ModeInferMeta(MakeMetaTensor(*input_x),
                     axis,
                     keepdim,
                     kernel_out_0 ? &meta_out_0 : nullptr,
                     kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "mode compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, keepdim, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> momentum_(
    Tensor& param,
    const Tensor& grad,
    Tensor& velocity,
    const Tensor& learning_rate,
    paddle::optional<Tensor>& master_param,
    float mu,
    bool use_nesterov,
    const std::string& regularization_method,
    float regularization_coeff,
    bool multi_precision,
    float rescale_grad) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(
        param, grad, velocity, learning_rate, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "momentum_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "momentum", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "momentum kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_param = PrepareData(param, kernel.InputAt(0), {});
  auto input_grad = PrepareData(grad, kernel.InputAt(1), {});
  auto input_velocity = PrepareData(velocity, kernel.InputAt(2), {});
  auto input_learning_rate = PrepareData(learning_rate, kernel.InputAt(3), {});
  auto input_master_param = PrepareData(master_param, kernel.InputAt(4), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> master_param_record_shapes;
    if (input_master_param) {
      master_param_record_shapes.push_back((*input_master_param).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"param", {(*input_param).dims()}},
        {"grad", {(*input_grad).dims()}},
        {"velocity", {(*input_velocity).dims()}},
        {"learning_rate", {(*input_learning_rate).dims()}},
        {"master_param", master_param_record_shapes}};
    platform::RecordOpInfoSupplement("momentum_", input_shapes);
  }

  std::tuple<Tensor&, Tensor&, paddle::optional<Tensor>&> api_output{
      param, velocity, master_param};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(std::get<2>(api_output).get_ptr());
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "momentum_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::MomentumInferMeta(MakeMetaTensor(*input_param),
                         MakeMetaTensor(*input_grad),
                         MakeMetaTensor(*input_velocity),
                         MakeMetaTensor(*input_learning_rate),
                         MakeMetaTensor(input_master_param),
                         mu,
                         use_nesterov,
                         regularization_method,
                         regularization_coeff,
                         multi_precision,
                         rescale_grad,
                         kernel_out_0 ? &meta_out_0 : nullptr,
                         kernel_out_1 ? &meta_out_1 : nullptr,
                         kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    float,
                                    bool,
                                    const std::string&,
                                    float,
                                    bool,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "momentum_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_param,
               *input_grad,
               *input_velocity,
               *input_learning_rate,
               input_master_param,
               mu,
               use_nesterov,
               regularization_method,
               regularization_coeff,
               multi_precision,
               rescale_grad,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor multi_dot(const std::vector<Tensor>& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multi_dot API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multi_dot", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "multi_dot kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x_vec = PrepareData(x, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_x.size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      ddims_vec.emplace_back((*input_x[i]).dims());
    }
    input_shapes.emplace_back("x", ddims_vec);
    platform::RecordOpInfoSupplement("multi_dot", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "multi_dot infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MultiDotInferMeta(x_metas, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "multi_dot compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> multiclass_nms3(
    const Tensor& bboxes,
    const Tensor& scores,
    const paddle::optional<Tensor>& rois_num,
    float score_threshold,
    int nms_top_k,
    int keep_top_k,
    float nms_threshold,
    bool normalized,
    float nms_eta,
    int background_label) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(bboxes, scores, rois_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multiclass_nms3 API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiclass_nms3", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "multiclass_nms3 kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_bboxes = PrepareData(bboxes, kernel.InputAt(0), {});
  auto input_scores = PrepareData(scores, kernel.InputAt(1), {});
  auto input_rois_num = PrepareData(rois_num, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> rois_num_record_shapes;
    if (input_rois_num) {
      rois_num_record_shapes.push_back((*input_rois_num).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"bboxes", {(*input_bboxes).dims()}},
        {"scores", {(*input_scores).dims()}},
        {"rois_num", rois_num_record_shapes}};
    platform::RecordOpInfoSupplement("multiclass_nms3", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "multiclass_nms3 infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::MultiClassNMSInferMeta(MakeMetaTensor(*input_bboxes),
                              MakeMetaTensor(*input_scores),
                              MakeMetaTensor(input_rois_num),
                              score_threshold,
                              nms_top_k,
                              keep_top_k,
                              nms_threshold,
                              normalized,
                              nms_eta,
                              background_label,
                              kernel_out_0 ? &meta_out_0 : nullptr,
                              kernel_out_1 ? &meta_out_1 : nullptr,
                              kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    float,
                                    int,
                                    int,
                                    float,
                                    bool,
                                    float,
                                    int,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "multiclass_nms3 compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_bboxes,
               *input_scores,
               input_rois_num,
               score_threshold,
               nms_top_k,
               keep_top_k,
               nms_threshold,
               normalized,
               nms_eta,
               background_label,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor multinomial(const Tensor& x,
                              const Scalar& num_samples,
                              bool replacement) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multinomial API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multinomial", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "multinomial kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("multinomial", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "multinomial infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MultinomialInferMeta(
      MakeMetaTensor(*input_x), num_samples, replacement, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "multinomial compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::Scalar(num_samples), replacement, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor multiplex(const std::vector<Tensor>& ins, const Tensor& ids) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(ins);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(ins, ids);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "multiplex API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiplex", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "multiplex kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_ins_vec = PrepareData(ins, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_ins(input_ins_vec->size());
  for (size_t i = 0; i < input_ins.size(); ++i) {
    input_ins[i] = &input_ins_vec->at(i);
  }
  auto input_ids = PrepareData(ids, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"ids", {(*input_ids).dims()}}};
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_ins.size());
    for (size_t i = 0; i < input_ins.size(); ++i) {
      ddims_vec.emplace_back((*input_ins[i]).dims());
    }
    input_shapes.emplace_back("ins", ddims_vec);
    platform::RecordOpInfoSupplement("multiplex", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "multiplex infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto ins_meta_vec = MakeMetaTensor(input_ins);
  std::vector<const phi::MetaTensor*> ins_metas(ins_meta_vec.size());
  for (size_t i = 0; i < ins_meta_vec.size(); ++i) {
    ins_metas[i] = &ins_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::MultiplexInferMeta(ins_metas, MakeMetaTensor(*input_ids), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "multiplex compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, input_ins, *input_ids, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor multiply(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (x.is_dense_tensor() && y.is_dense_tensor()) {
    VLOG(6) << "multiply API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "multiply", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "multiply kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = PrepareData(x, kernel.InputAt(0), {});
    auto input_y = PrepareData(y, kernel.InputAt(1), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
      platform::RecordOpInfoSupplement("multiply", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "multiply infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::ElementwiseInferMeta(
        MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "multiply compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  if (x.is_selected_rows() && y.is_dense_tensor()) {
    VLOG(6) << "multiply API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "multiply_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "multiply_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = TensorToSelectedRows(x);

    auto input_y = PrepareData(y, kernel.InputAt(1), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
      platform::RecordOpInfoSupplement("multiply", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "multiply infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::ElementwiseInferMeta(
        MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::SelectedRows&,
                                      const phi::DenseTensor&,
                                      phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "multiply compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (multiply) for input tensors is unimplemented, please "
      "check the type of input tensors."));
}

PADDLE_API Tensor
nearest_interp(const Tensor& x,
               const paddle::optional<Tensor>& out_size,
               const paddle::optional<std::vector<Tensor>>& size_tensor,
               const paddle::optional<Tensor>& scale_tensor,
               const std::string& data_layout,
               int out_d,
               int out_h,
               int out_w,
               const std::vector<float>& scale,
               const std::string& interp_method,
               bool align_corners,
               int align_mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nearest_interp API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nearest_interp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "nearest_interp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_out_size = PrepareData(out_size, kernel.InputAt(1), {});
  auto input_size_tensor_vec = PrepareData(size_tensor, kernel.InputAt(2), {});
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec) {
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(
        input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> out_size_record_shapes;
    if (input_out_size) {
      out_size_record_shapes.push_back((*input_out_size).dims());
    }
    std::vector<phi::DDim> scale_tensor_record_shapes;
    if (input_scale_tensor) {
      scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"out_size", out_size_record_shapes},
        {"scale_tensor", scale_tensor_record_shapes}};
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    if (input_size_tensor) {
      ddims_vec.reserve(input_size_tensor->size());
      for (size_t i = 0; i < input_size_tensor->size(); ++i) {
        ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
      }
    }
    input_shapes.emplace_back("size_tensor", ddims_vec);
    platform::RecordOpInfoSupplement("nearest_interp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "nearest_interp infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(
      size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x),
                            MakeMetaTensor(input_out_size),
                            size_tensor_metas,
                            MakeMetaTensor(input_scale_tensor),
                            data_layout,
                            out_d,
                            out_h,
                            out_w,
                            scale,
                            interp_method,
                            align_corners,
                            align_mode,
                            &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature =
      void (*)(const platform::DeviceContext&,
               const phi::DenseTensor&,
               const paddle::optional<phi::DenseTensor>&,
               const paddle::optional<std::vector<const phi::DenseTensor*>>&,
               const paddle::optional<phi::DenseTensor>&,
               const std::string&,
               int,
               int,
               int,
               const std::vector<float>&,
               const std::string&,
               bool,
               int,
               phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "nearest_interp compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_out_size,
               input_size_tensor,
               input_scale_tensor,
               data_layout,
               out_d,
               out_h,
               out_w,
               scale,
               interp_method,
               align_corners,
               align_mode,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> nll_loss(
    const Tensor& input,
    const Tensor& label,
    const paddle::optional<Tensor>& weight,
    int64_t ignore_index,
    const std::string& reduction) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, label, weight);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nll_loss API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nll_loss", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "nll_loss kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  auto input_weight = PrepareData(weight, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> weight_record_shapes;
    if (input_weight) {
      weight_record_shapes.push_back((*input_weight).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}},
        {"label", {(*input_label).dims()}},
        {"weight", weight_record_shapes}};
    platform::RecordOpInfoSupplement("nll_loss", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "nll_loss infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::NllLossRawInferMeta(MakeMetaTensor(*input_input),
                           MakeMetaTensor(*input_label),
                           MakeMetaTensor(input_weight),
                           ignore_index,
                           reduction,
                           kernel_out_0 ? &meta_out_0 : nullptr,
                           kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    int64_t,
                                    const std::string&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "nll_loss compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               *input_label,
               input_weight,
               ignore_index,
               reduction,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor nms(const Tensor& x, float threshold) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "nms API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nms", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "nms kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("nms", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "nms infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::NMSInferMeta(MakeMetaTensor(*input_x), threshold, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "nms compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, threshold, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> norm(const Tensor& x,
                                           int axis,
                                           float epsilon,
                                           bool is_test) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "norm API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("norm", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "norm infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::NormInferMeta(MakeMetaTensor(*input_x),
                     axis,
                     epsilon,
                     is_test,
                     kernel_out_0 ? &meta_out_0 : nullptr,
                     kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    float,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "norm compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, axis, epsilon, is_test, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor not_equal(const Tensor& x, const Tensor& y, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "not_equal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "not_equal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "not_equal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("not_equal", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "not_equal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::CompareInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "not_equal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor one_hot(const Tensor& x, const Scalar& num_classes) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "one_hot API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "one_hot", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "one_hot kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("one_hot", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "one_hot infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::OneHotInferMeta(MakeMetaTensor(*input_x), num_classes, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "one_hot compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(num_classes), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor ones(const IntArray& shape,
                       DataType dtype,
                       const Place& place) {
  return full(shape, 1, dtype, place);
}
PADDLE_API Tensor ones_like(const Tensor& x,
                            DataType dtype,
                            const Place& place) {
  return full_like(x, 1, dtype, place);
}
PADDLE_API Tensor p_norm(const Tensor& x,
                         float porder,
                         int axis,
                         float epsilon,
                         bool keepdim,
                         bool asvector) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "p_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "p_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "p_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("p_norm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "p_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::PNormInferMeta(MakeMetaTensor(*input_x),
                      porder,
                      axis,
                      epsilon,
                      keepdim,
                      asvector,
                      &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    int,
                                    float,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "p_norm compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, porder, axis, epsilon, keepdim, asvector, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor pad(const Tensor& x,
                      const std::vector<int>& paddings,
                      const Scalar& pad_value) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pad API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "pad kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("pad", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "pad infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::PadInferMeta(MakeMetaTensor(*input_x), paddings, pad_value, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "pad compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, paddings, phi::Scalar(pad_value), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor pad3d(const Tensor& x,
                        const IntArray& paddings,
                        const std::string& mode,
                        float pad_value,
                        const std::string& data_format) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pad3d API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad3d", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "pad3d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("pad3d", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "pad3d infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::Pad3dInferMeta(MakeMetaTensor(*input_x),
                      paddings,
                      mode,
                      pad_value,
                      data_format,
                      &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    const std::string&,
                                    float,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "pad3d compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::IntArray(paddings),
               mode,
               pad_value,
               data_format,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor pixel_shuffle(const Tensor& x,
                                int upscale_factor,
                                const std::string& data_format) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pixel_shuffle API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pixel_shuffle", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "pixel_shuffle kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("pixel_shuffle", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "pixel_shuffle infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::PixelShuffleInferMeta(
      MakeMetaTensor(*input_x), upscale_factor, data_format, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "pixel_shuffle compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, upscale_factor, data_format, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor pool2d(const Tensor& x,
                         const IntArray& kernel_size,
                         const std::vector<int>& strides,
                         const std::vector<int>& paddings,
                         bool ceil_mode,
                         bool exclusive,
                         const std::string& data_format,
                         const std::string& pooling_type,
                         bool global_pooling,
                         bool adaptive,
                         const std::string& padding_algorithm) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pool2d API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool2d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "pool2d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("pool2d", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "pool2d infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::Pool2DInferMeta(MakeMetaTensor(*input_x),
                       kernel_size,
                       strides,
                       paddings,
                       ceil_mode,
                       exclusive,
                       data_format,
                       pooling_type,
                       global_pooling,
                       adaptive,
                       padding_algorithm,
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    bool,
                                    bool,
                                    const std::string&,
                                    const std::string&,
                                    bool,
                                    bool,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "pool2d compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::IntArray(kernel_size),
               strides,
               paddings,
               ceil_mode,
               exclusive,
               data_format,
               pooling_type,
               global_pooling,
               adaptive,
               padding_algorithm,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor pool2d_gpudnn_unused(const Tensor& x,
                                       const IntArray& kernel_size,
                                       const std::vector<int>& strides,
                                       const std::vector<int>& paddings,
                                       bool ceil_mode,
                                       bool exclusive,
                                       const std::string& data_format,
                                       const std::string& pooling_type,
                                       bool global_pooling,
                                       bool adaptive,
                                       const std::string& padding_algorithm) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pool2d_gpudnn_unused API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool2d", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "pool2d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("pool2d_gpudnn_unused", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "pool2d_gpudnn_unused infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::Pool2DInferMeta(MakeMetaTensor(*input_x),
                       kernel_size,
                       strides,
                       paddings,
                       ceil_mode,
                       exclusive,
                       data_format,
                       pooling_type,
                       global_pooling,
                       adaptive,
                       padding_algorithm,
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    bool,
                                    bool,
                                    const std::string&,
                                    const std::string&,
                                    bool,
                                    bool,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "pool2d_gpudnn_unused compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::IntArray(kernel_size),
               strides,
               paddings,
               ceil_mode,
               exclusive,
               data_format,
               pooling_type,
               global_pooling,
               adaptive,
               padding_algorithm,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor pool3d(const Tensor& x,
                         const std::vector<int>& kernel_size,
                         const std::vector<int>& strides,
                         const std::vector<int>& paddings,
                         bool ceil_mode,
                         bool exclusive,
                         const std::string& data_format,
                         const std::string& pooling_type,
                         bool global_pooling,
                         bool adaptive,
                         const std::string& padding_algorithm,
                         bool use_gpudnn) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pool3d API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool3d", {kernel_backend, kernel_layout, kernel_data_type}, use_gpudnn);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "pool3d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("pool3d", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "pool3d infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::PoolInferMeta(MakeMetaTensor(*input_x),
                     kernel_size,
                     strides,
                     paddings,
                     ceil_mode,
                     exclusive,
                     data_format,
                     pooling_type,
                     global_pooling,
                     adaptive,
                     padding_algorithm,
                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    bool,
                                    bool,
                                    const std::string&,
                                    const std::string&,
                                    bool,
                                    bool,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "pool3d compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               kernel_size,
               strides,
               paddings,
               ceil_mode,
               exclusive,
               data_format,
               pooling_type,
               global_pooling,
               adaptive,
               padding_algorithm,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor pow(const Tensor& x, const Scalar& s) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "pow API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pow", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "pow kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("pow", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "pow infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "pow compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::Scalar(s), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor prelu(const Tensor& x,
                        const Tensor& alpha,
                        const std::string& data_format,
                        const std::string& mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, alpha);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "prelu API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prelu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "prelu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_alpha = PrepareData(alpha, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"alpha", {(*input_alpha).dims()}}};
    platform::RecordOpInfoSupplement("prelu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "prelu infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::PReluInferMeta(MakeMetaTensor(*input_x),
                      MakeMetaTensor(*input_alpha),
                      data_format,
                      mode,
                      &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "prelu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_alpha, data_format, mode, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> prior_box(
    const Tensor& input,
    const Tensor& image,
    const std::vector<float>& min_sizes,
    const std::vector<float>& aspect_ratios,
    const std::vector<float>& variances,
    const std::vector<float>& max_sizes,
    bool flip,
    bool clip,
    float step_w,
    float step_h,
    float offset,
    bool min_max_aspect_ratios_order) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, image);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "prior_box API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prior_box", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "prior_box kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_image = PrepareData(image, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}, {"image", {(*input_image).dims()}}};
    platform::RecordOpInfoSupplement("prior_box", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "prior_box infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::PriorBoxInferMeta(MakeMetaTensor(*input_input),
                         MakeMetaTensor(*input_image),
                         min_sizes,
                         aspect_ratios,
                         variances,
                         max_sizes,
                         flip,
                         clip,
                         step_w,
                         step_h,
                         offset,
                         min_max_aspect_ratios_order,
                         kernel_out_0 ? &meta_out_0 : nullptr,
                         kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<float>&,
                                    const std::vector<float>&,
                                    const std::vector<float>&,
                                    const std::vector<float>&,
                                    bool,
                                    bool,
                                    float,
                                    float,
                                    float,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "prior_box compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               *input_image,
               min_sizes,
               aspect_ratios,
               variances,
               max_sizes,
               flip,
               clip,
               step_w,
               step_h,
               offset,
               min_max_aspect_ratios_order,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor psroi_pool(const Tensor& x,
                             const Tensor& boxes,
                             const paddle::optional<Tensor>& boxes_num,
                             int pooled_height,
                             int pooled_width,
                             int output_channels,
                             float spatial_scale) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "psroi_pool API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "psroi_pool", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "psroi_pool kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_boxes = PrepareData(boxes, kernel.InputAt(1), {});
  auto input_boxes_num = PrepareData(boxes_num, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> boxes_num_record_shapes;
    if (input_boxes_num) {
      boxes_num_record_shapes.push_back((*input_boxes_num).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"boxes", {(*input_boxes).dims()}},
        {"boxes_num", boxes_num_record_shapes}};
    platform::RecordOpInfoSupplement("psroi_pool", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "psroi_pool infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::PsroiPoolInferMeta(MakeMetaTensor(*input_x),
                          MakeMetaTensor(*input_boxes),
                          MakeMetaTensor(input_boxes_num),
                          pooled_height,
                          pooled_width,
                          output_channels,
                          spatial_scale,
                          &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    int,
                                    int,
                                    int,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "psroi_pool compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_boxes,
               input_boxes_num,
               pooled_height,
               pooled_width,
               output_channels,
               spatial_scale,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor put_along_axis(const Tensor& x,
                                 const Tensor& index,
                                 const Tensor& value,
                                 int axis,
                                 const std::string& reduce) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "put_along_axis API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "put_along_axis", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "put_along_axis kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  auto input_value = PrepareData(value, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"index", {(*input_index).dims()}},
        {"value", {(*input_value).dims()}}};
    platform::RecordOpInfoSupplement("put_along_axis", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "put_along_axis infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "put_along_axis compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_index, *input_value, axis, reduce, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& put_along_axis_(Tensor& x,
                                   const Tensor& index,
                                   const Tensor& value,
                                   int axis,
                                   const std::string& reduce) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "put_along_axis API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "put_along_axis", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "put_along_axis kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  auto input_value = PrepareData(value, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"index", {(*input_index).dims()}},
        {"value", {(*input_value).dims()}}};
    platform::RecordOpInfoSupplement("put_along_axis", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "put_along_axis infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "put_along_axis compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_index, *input_value, axis, reduce, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> qr(const Tensor& x,
                                         const std::string& mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "qr API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "qr", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "qr kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("qr", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "qr infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::QrInferMeta(MakeMetaTensor(*input_x),
                   mode,
                   kernel_out_0 ? &meta_out_0 : nullptr,
                   kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "qr compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, mode, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor randint(int low,
                          int high,
                          const IntArray& shape,
                          DataType dtype,
                          const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "randint API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "randint", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "randint kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("randint", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "randint infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RandintInferMeta(low, high, shape, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    int,
                                    int,
                                    const phi::IntArray&,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "randint compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, low, high, phi::IntArray(shape), dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor randperm(int n, DataType dtype, const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "randperm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "randperm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "randperm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("randperm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "randperm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RandpermInferMeta(n, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(
      const platform::DeviceContext&, int, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "randperm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, n, dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor real(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "real API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "real", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "real kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("real", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "real infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RealAndImagInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "real compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor reciprocal(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reciprocal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reciprocal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "reciprocal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("reciprocal", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "reciprocal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "reciprocal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& reciprocal_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reciprocal API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reciprocal", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "reciprocal kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("reciprocal", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "reciprocal infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "reciprocal compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor reduce_prod(const Tensor& x,
                              const IntArray& dims,
                              bool keep_dim,
                              bool reduce_all) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reduce_prod API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prod_raw", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "prod_raw kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("reduce_prod", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "reduce_prod infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReduceIntArrayAxisInferMetaBase(
      MakeMetaTensor(*input_x), dims, keep_dim, reduce_all, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "reduce_prod compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::IntArray(dims),
               keep_dim,
               reduce_all,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor relu(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "relu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "relu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("relu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "relu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "relu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& relu_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "relu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "relu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("relu", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "relu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "relu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor relu6(const Tensor& x, float threshold) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "relu6 API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu6", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "relu6 kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("relu6", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "relu6 infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "relu6 compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, threshold, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor remainder(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "remainder API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "remainder", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "remainder kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("remainder", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "remainder infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "remainder compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& remainder_(Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "remainder API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "remainder", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "remainder kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("remainder", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "remainder infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "remainder compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor renorm(const Tensor& x, float p, int axis, float max_norm) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "renorm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "renorm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "renorm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("renorm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "renorm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    int,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "renorm compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, p, axis, max_norm, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor repeat_interleave(const Tensor& x, int repeats, int dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "repeat_interleave API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "repeat_interleave", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "repeat_interleave kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("repeat_interleave", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "repeat_interleave infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RepeatInterleaveInferMeta(
      MakeMetaTensor(*input_x), repeats, dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "repeat_interleave compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, repeats, dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor repeat_interleave_with_tensor_index(const Tensor& x,
                                                      const Tensor& repeats,
                                                      int dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, repeats);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "repeat_interleave_with_tensor_index API kernel key: ["
          << kernel_backend << ", " << kernel_layout << ", " << kernel_data_type
          << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "repeat_interleave_with_tensor_index",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "repeat_interleave_with_tensor_index kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_repeats = PrepareData(repeats, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"repeats", {(*input_repeats).dims()}}};
    platform::RecordOpInfoSupplement("repeat_interleave_with_tensor_index",
                                     input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "repeat_interleave_with_tensor_index infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RepeatInterleaveWithTensorIndexInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_repeats), dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "repeat_interleave_with_tensor_index compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_repeats, dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor reshape(const Tensor& x, const IntArray& shape) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reshape API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reshape_with_xshape", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "reshape_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("reshape", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  kernel_out_0->ShareBufferWith(*input_x);
  kernel_out_0->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation "
             "and inplace version.";
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "reshape infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::ReshapeWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                  shape,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "reshape compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::IntArray(shape), kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor& reshape_(Tensor& x, const IntArray& shape) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reshape API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reshape_with_xshape", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "reshape_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("reshape", input_shapes);
  }

  std::tuple<Tensor&, Tensor> api_output{x, Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "reshape infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::ReshapeWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                  shape,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "reshape compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::IntArray(shape), kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor reverse(const Tensor& x, const IntArray& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reverse API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reverse", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "reverse kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("reverse", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "reverse infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ReverseInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "reverse compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(axis), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::vector<Tensor> reverse_array(const std::vector<Tensor>& x,
                                             const IntArray& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "reverse_array API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reverse_array", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "reverse_array kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x_vec = PrepareData(x, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_x.size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      ddims_vec.emplace_back((*input_x[i]).dims());
    }
    input_shapes.emplace_back("x", ddims_vec);
    platform::RecordOpInfoSupplement("reverse_array", input_shapes);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(x.size(), &api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "reverse_array infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::ReverseArrayInferMeta(x_metas, axis, kernel_out_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    const phi::IntArray&,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "reverse_array compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, input_x, phi::IntArray(axis), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> rmsprop_(
    Tensor& param,
    Tensor& mean_square,
    const Tensor& grad,
    Tensor& moment,
    const Tensor& learning_rate,
    Tensor& mean_grad,
    float epsilon,
    float decay,
    float momentum,
    bool centered) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(
        param, mean_square, grad, moment, learning_rate, mean_grad);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (param.is_dense_tensor() && mean_square.is_dense_tensor() &&
      grad.is_dense_tensor() && moment.is_dense_tensor() &&
      learning_rate.is_dense_tensor() && mean_grad.is_dense_tensor()) {
    VLOG(6) << "rmsprop_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "rmsprop", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "rmsprop kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_mean_square = PrepareData(mean_square, kernel.InputAt(1), {});
    auto input_grad = PrepareData(grad, kernel.InputAt(2), {});
    auto input_moment = PrepareData(moment, kernel.InputAt(3), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(4), {});
    auto input_mean_grad = PrepareData(mean_grad, kernel.InputAt(5), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"mean_square", {(*input_mean_square).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"moment", {(*input_moment).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"mean_grad", {(*input_mean_grad).dims()}}};
      platform::RecordOpInfoSupplement("rmsprop_", input_shapes);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> api_output{
        param, moment, mean_square, mean_grad};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "rmsprop_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);
    phi::MetaTensor meta_out_2(kernel_out_2);
    phi::MetaTensor meta_out_3(kernel_out_3);

    phi::RmspropInferMeta(MakeMetaTensor(*input_param),
                          MakeMetaTensor(*input_mean_square),
                          MakeMetaTensor(*input_grad),
                          MakeMetaTensor(*input_moment),
                          MakeMetaTensor(*input_learning_rate),
                          MakeMetaTensor(*input_mean_grad),
                          epsilon,
                          decay,
                          momentum,
                          centered,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr,
                          kernel_out_2 ? &meta_out_2 : nullptr,
                          kernel_out_3 ? &meta_out_3 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      float,
                                      float,
                                      float,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "rmsprop_ compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_mean_square,
                 *input_grad,
                 *input_moment,
                 *input_learning_rate,
                 *input_mean_grad,
                 epsilon,
                 decay,
                 momentum,
                 centered,
                 kernel_out_0,
                 kernel_out_1,
                 kernel_out_2,
                 kernel_out_3);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    }
    return api_output;
  }

  if (param.is_dense_tensor() && mean_square.is_dense_tensor() &&
      grad.is_selected_rows() && moment.is_dense_tensor() &&
      learning_rate.is_dense_tensor() && mean_grad.is_dense_tensor()) {
    VLOG(6) << "rmsprop_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "rmsprop_dense_param_sparse_grad",
            {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "rmsprop_dense_param_sparse_grad kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_mean_square = PrepareData(mean_square, kernel.InputAt(1), {});
    auto input_grad = TensorToSelectedRows(grad);

    auto input_moment = PrepareData(moment, kernel.InputAt(3), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(4), {});
    auto input_mean_grad = PrepareData(mean_grad, kernel.InputAt(5), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"mean_square", {(*input_mean_square).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"moment", {(*input_moment).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"mean_grad", {(*input_mean_grad).dims()}}};
      platform::RecordOpInfoSupplement("rmsprop_", input_shapes);
    }

    std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> api_output{
        param, moment, mean_square, mean_grad};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
    auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
    auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "rmsprop_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);
    phi::MetaTensor meta_out_2(kernel_out_2);
    phi::MetaTensor meta_out_3(kernel_out_3);

    phi::RmspropInferMeta(MakeMetaTensor(*input_param),
                          MakeMetaTensor(*input_mean_square),
                          MakeMetaTensor(*input_grad),
                          MakeMetaTensor(*input_moment),
                          MakeMetaTensor(*input_learning_rate),
                          MakeMetaTensor(*input_mean_grad),
                          epsilon,
                          decay,
                          momentum,
                          centered,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr,
                          kernel_out_2 ? &meta_out_2 : nullptr,
                          kernel_out_3 ? &meta_out_3 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::SelectedRows&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      float,
                                      float,
                                      float,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "rmsprop_ compute",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_mean_square,
                 *input_grad,
                 *input_moment,
                 *input_learning_rate,
                 *input_mean_grad,
                 epsilon,
                 decay,
                 momentum,
                 centered,
                 kernel_out_0,
                 kernel_out_1,
                 kernel_out_2,
                 kernel_out_3);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
      TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
      TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (rmsprop_) for input tensors is unimplemented, please "
      "check the type of input tensors."));
}

PADDLE_API Tensor roi_align(const Tensor& x,
                            const Tensor& boxes,
                            const paddle::optional<Tensor>& boxes_num,
                            int pooled_height,
                            int pooled_width,
                            float spatial_scale,
                            int sampling_ratio,
                            bool aligned) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "roi_align API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roi_align", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "roi_align kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_boxes = PrepareData(boxes, kernel.InputAt(1), {});
  auto input_boxes_num = PrepareData(boxes_num, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> boxes_num_record_shapes;
    if (input_boxes_num) {
      boxes_num_record_shapes.push_back((*input_boxes_num).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"boxes", {(*input_boxes).dims()}},
        {"boxes_num", boxes_num_record_shapes}};
    platform::RecordOpInfoSupplement("roi_align", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "roi_align infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RoiAlignInferMeta(MakeMetaTensor(*input_x),
                         MakeMetaTensor(*input_boxes),
                         MakeMetaTensor(input_boxes_num),
                         pooled_height,
                         pooled_width,
                         spatial_scale,
                         sampling_ratio,
                         aligned,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    int,
                                    int,
                                    float,
                                    int,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "roi_align compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_boxes,
               input_boxes_num,
               pooled_height,
               pooled_width,
               spatial_scale,
               sampling_ratio,
               aligned,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor roi_pool(const Tensor& x,
                           const Tensor& boxes,
                           const paddle::optional<Tensor>& boxes_num,
                           int pooled_height,
                           int pooled_width,
                           float spatial_scale) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "roi_pool API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roi_pool", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "roi_pool kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_boxes = PrepareData(boxes, kernel.InputAt(1), {});
  auto input_boxes_num = PrepareData(boxes_num, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> boxes_num_record_shapes;
    if (input_boxes_num) {
      boxes_num_record_shapes.push_back((*input_boxes_num).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"boxes", {(*input_boxes).dims()}},
        {"boxes_num", boxes_num_record_shapes}};
    platform::RecordOpInfoSupplement("roi_pool", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "roi_pool infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::RoiPoolInferMeta(MakeMetaTensor(*input_x),
                        MakeMetaTensor(*input_boxes),
                        MakeMetaTensor(input_boxes_num),
                        pooled_height,
                        pooled_width,
                        spatial_scale,
                        kernel_out_0 ? &meta_out_0 : nullptr,
                        kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    int,
                                    int,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "roi_pool compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_boxes,
               input_boxes_num,
               pooled_height,
               pooled_width,
               spatial_scale,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor roll(const Tensor& x,
                       const IntArray& shifts,
                       const std::vector<int64_t>& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "roll API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roll", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "roll kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("roll", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "roll infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::RollInferMeta(MakeMetaTensor(*input_x), shifts, axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    const std::vector<int64_t>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "roll compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(shifts), axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor round(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "round API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "round", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "round kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("round", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "round infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "round compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& round_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "round API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "round", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "round kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("round", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "round infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "round compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor rsqrt(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rsqrt API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rsqrt", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "rsqrt kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("rsqrt", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "rsqrt infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "rsqrt compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& rsqrt_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "rsqrt API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rsqrt", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "rsqrt kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("rsqrt", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "rsqrt infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "rsqrt compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor scale(const Tensor& x,
                        const Scalar& scale,
                        float bias,
                        bool bias_after_scale) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (x.is_dense_tensor()) {
    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "scale", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "scale kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = PrepareData(x, kernel.InputAt(0), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("scale", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "scale infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::Scalar&,
                                      float,
                                      bool,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "scale compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_x,
                 phi::Scalar(scale),
                 bias,
                 bias_after_scale,
                 kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  if (x.is_selected_rows()) {
    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "scale_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "scale_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = TensorToSelectedRows(x);

    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("scale", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "scale infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::SelectedRows&,
                                      const phi::Scalar&,
                                      float,
                                      bool,
                                      phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "scale compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_x,
                 phi::Scalar(scale),
                 bias,
                 bias_after_scale,
                 kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (scale) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API Tensor& scale_(Tensor& x,
                          const Scalar& scale,
                          float bias,
                          bool bias_after_scale) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (x.is_dense_tensor()) {
    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "scale", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "scale kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = PrepareData(x, kernel.InputAt(0), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("scale", input_shapes);
    }

    Tensor& api_output = x;
    auto kernel_out = SetKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "scale infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::Scalar&,
                                      float,
                                      bool,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "scale compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_x,
                 phi::Scalar(scale),
                 bias,
                 bias_after_scale,
                 kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  if (x.is_selected_rows()) {
    VLOG(6) << "scale API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "scale_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "scale_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_x = TensorToSelectedRows(x);

    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"x", {(*input_x).dims()}}};
      platform::RecordOpInfoSupplement("scale", input_shapes);
    }

    Tensor& api_output = x;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "scale infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::SelectedRows&,
                                      const phi::Scalar&,
                                      float,
                                      bool,
                                      phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "scale compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_x,
                 phi::Scalar(scale),
                 bias,
                 bias_after_scale,
                 kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (scale) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API Tensor scatter(const Tensor& x,
                          const Tensor& index,
                          const Tensor& updates,
                          bool overwrite) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "scatter API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "scatter kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  auto input_updates = PrepareData(updates, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"index", {(*input_index).dims()}},
        {"updates", {(*input_updates).dims()}}};
    platform::RecordOpInfoSupplement("scatter", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "scatter infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ScatterInferMeta(MakeMetaTensor(*input_x),
                        MakeMetaTensor(*input_index),
                        MakeMetaTensor(*input_updates),
                        overwrite,
                        &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "scatter compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_index, *input_updates, overwrite, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& scatter_(Tensor& x,
                            const Tensor& index,
                            const Tensor& updates,
                            bool overwrite) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "scatter API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "scatter kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  auto input_updates = PrepareData(updates, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"index", {(*input_index).dims()}},
        {"updates", {(*input_updates).dims()}}};
    platform::RecordOpInfoSupplement("scatter", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "scatter infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ScatterInferMeta(MakeMetaTensor(*input_x),
                        MakeMetaTensor(*input_index),
                        MakeMetaTensor(*input_updates),
                        overwrite,
                        &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "scatter compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_index, *input_updates, overwrite, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor scatter_nd_add(const Tensor& x,
                                 const Tensor& index,
                                 const Tensor& updates) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, updates);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "scatter_nd_add API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter_nd_add", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "scatter_nd_add kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  auto input_updates = PrepareData(updates, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"index", {(*input_index).dims()}},
        {"updates", {(*input_updates).dims()}}};
    platform::RecordOpInfoSupplement("scatter_nd_add", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "scatter_nd_add infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ScatterNdAddInferMeta(MakeMetaTensor(*input_x),
                             MakeMetaTensor(*input_index),
                             MakeMetaTensor(*input_updates),
                             &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "scatter_nd_add compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_updates, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor searchsorted(const Tensor& sorted_sequence,
                               const Tensor& value,
                               bool out_int32,
                               bool right) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(sorted_sequence);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(sorted_sequence, value);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "searchsorted API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "searchsorted", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "searchsorted kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_sorted_sequence =
      PrepareData(sorted_sequence, kernel.InputAt(0), {});
  auto input_value = PrepareData(value, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"sorted_sequence", {(*input_sorted_sequence).dims()}},
        {"value", {(*input_value).dims()}}};
    platform::RecordOpInfoSupplement("searchsorted", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "searchsorted infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SearchsortedInferMeta(MakeMetaTensor(*input_sorted_sequence),
                             MakeMetaTensor(*input_value),
                             out_int32,
                             right,
                             &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "searchsorted compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_sorted_sequence,
               *input_value,
               out_int32,
               right,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> segment_pool(
    const Tensor& x, const Tensor& segment_ids, const std::string& pooltype) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, segment_ids);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "segment_pool API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "segment_pool", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "segment_pool kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_segment_ids = PrepareData(segment_ids, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"segment_ids", {(*input_segment_ids).dims()}}};
    platform::RecordOpInfoSupplement("segment_pool", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "segment_pool infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::SegmentPoolInferMeta(MakeMetaTensor(*input_x),
                            MakeMetaTensor(*input_segment_ids),
                            pooltype,
                            kernel_out_0 ? &meta_out_0 : nullptr,
                            kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::string&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "segment_pool compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_segment_ids,
               pooltype,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor selu(const Tensor& x, float scale, float alpha) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "selu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "selu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "selu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("selu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "selu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "selu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, scale, alpha, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor&, paddle::optional<Tensor>&> sgd_(
    Tensor& param,
    const Tensor& learning_rate,
    const Tensor& grad,
    paddle::optional<Tensor>& master_param,
    bool multi_precision) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(param);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(param, learning_rate, grad, master_param);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (param.is_dense_tensor() && learning_rate.is_dense_tensor() &&
      grad.is_dense_tensor() &&
      (!master_param || master_param->is_dense_tensor())) {
    VLOG(6) << "sgd_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "sgd", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "sgd kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(1), {false, true});
    auto input_grad = PrepareData(grad, kernel.InputAt(2), {});
    auto input_master_param = PrepareData(master_param, kernel.InputAt(3), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<phi::DDim> master_param_record_shapes;
      if (input_master_param) {
        master_param_record_shapes.push_back((*input_master_param).dims());
      }
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"master_param", master_param_record_shapes}};
      platform::RecordOpInfoSupplement("sgd_", input_shapes);
    }

    std::tuple<Tensor&, paddle::optional<Tensor>&> api_output{param,
                                                              master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(std::get<1>(api_output).get_ptr());
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "sgd_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);

    phi::SgdInferMeta(MakeMetaTensor(*input_param),
                      MakeMetaTensor(*input_learning_rate),
                      MakeMetaTensor(*input_grad),
                      MakeMetaTensor(input_master_param),
                      multi_precision,
                      kernel_out_0 ? &meta_out_0 : nullptr,
                      kernel_out_1 ? &meta_out_1 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "sgd_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_learning_rate,
                 *input_grad,
                 input_master_param,
                 multi_precision,
                 kernel_out_0,
                 kernel_out_1);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    }
    return api_output;
  }

  if (param.is_dense_tensor() && learning_rate.is_dense_tensor() &&
      grad.is_selected_rows() &&
      (!master_param || master_param->is_dense_tensor())) {
    VLOG(6) << "sgd_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "sgd_dense_param_sparse_grad",
            {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "sgd_dense_param_sparse_grad kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = PrepareData(param, kernel.InputAt(0), {});
    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(1), {false, true});
    auto input_grad = TensorToSelectedRows(grad);

    auto input_master_param = PrepareData(master_param, kernel.InputAt(3), {});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<phi::DDim> master_param_record_shapes;
      if (input_master_param) {
        master_param_record_shapes.push_back((*input_master_param).dims());
      }
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"master_param", master_param_record_shapes}};
      platform::RecordOpInfoSupplement("sgd_", input_shapes);
    }

    std::tuple<Tensor&, paddle::optional<Tensor>&> api_output{param,
                                                              master_param};
    auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 = SetKernelOutput(std::get<1>(api_output).get_ptr());
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "sgd_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);

    phi::SgdInferMeta(MakeMetaTensor(*input_param),
                      MakeMetaTensor(*input_learning_rate),
                      MakeMetaTensor(*input_grad),
                      MakeMetaTensor(input_master_param),
                      multi_precision,
                      kernel_out_0 ? &meta_out_0 : nullptr,
                      kernel_out_1 ? &meta_out_1 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      const phi::DenseTensor&,
                                      const phi::SelectedRows&,
                                      const paddle::optional<phi::DenseTensor>&,
                                      bool,
                                      phi::DenseTensor*,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "sgd_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_learning_rate,
                 *input_grad,
                 input_master_param,
                 multi_precision,
                 kernel_out_0,
                 kernel_out_1);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    }
    return api_output;
  }

  if (param.is_selected_rows() && learning_rate.is_dense_tensor() &&
      grad.is_selected_rows() &&
      (!master_param || master_param->is_selected_rows())) {
    VLOG(6) << "sgd_ API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "sgd_sparse_param_sparse_grad",
            {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "sgd_sparse_param_sparse_grad kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_param = TensorToSelectedRows(param);

    auto input_learning_rate =
        PrepareData(learning_rate, kernel.InputAt(1), {false, true});
    auto input_grad = TensorToSelectedRows(grad);

    auto input_master_param = TensorToSelectedRows(master_param);

    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<phi::DDim> master_param_record_shapes;
      if (input_master_param) {
        master_param_record_shapes.push_back((*input_master_param).dims());
      }
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"param", {(*input_param).dims()}},
          {"learning_rate", {(*input_learning_rate).dims()}},
          {"grad", {(*input_grad).dims()}},
          {"master_param", master_param_record_shapes}};
      platform::RecordOpInfoSupplement("sgd_", input_shapes);
    }

    std::tuple<Tensor&, paddle::optional<Tensor>&> api_output{param,
                                                              master_param};
    auto kernel_out_0 = SetSelectedRowsKernelOutput(&std::get<0>(api_output));
    auto kernel_out_1 =
        SetSelectedRowsKernelOutput(std::get<1>(api_output).get_ptr());
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "sgd_ infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out_0(kernel_out_0);
    phi::MetaTensor meta_out_1(kernel_out_1);

    phi::SgdInferMeta(MakeMetaTensor(*input_param),
                      MakeMetaTensor(*input_learning_rate),
                      MakeMetaTensor(*input_grad),
                      MakeMetaTensor(input_master_param),
                      multi_precision,
                      kernel_out_0 ? &meta_out_0 : nullptr,
                      kernel_out_1 ? &meta_out_1 : nullptr);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature =
        void (*)(const platform::DeviceContext&,
                 const phi::SelectedRows&,
                 const phi::DenseTensor&,
                 const phi::SelectedRows&,
                 const paddle::optional<phi::SelectedRows>&,
                 bool,
                 phi::SelectedRows*,
                 phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "sgd_ compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx,
                 *input_param,
                 *input_learning_rate,
                 *input_grad,
                 input_master_param,
                 multi_precision,
                 kernel_out_0,
                 kernel_out_1);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
      TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (sgd_) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API Tensor shape(const Tensor& input) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  if (input.is_dense_tensor()) {
    VLOG(6) << "shape API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "shape", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "shape kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_input = PrepareData(input, kernel.InputAt(0), {true});
    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"input", {(*input_input).dims()}}};
      platform::RecordOpInfoSupplement("shape", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "shape infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::ShapeInferMeta(MakeMetaTensor(*input_input), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::DenseTensor&,
                                      phi::DenseTensor*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "shape compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx, *input_input, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  if (input.is_selected_rows()) {
    VLOG(6) << "shape API kernel key: [" << kernel_backend << ", "
            << kernel_layout << ", " << kernel_data_type << "]";
    auto kernel_result =
        phi::KernelFactory::Instance().SelectKernelOrThrowError(
            "shape_sr", {kernel_backend, kernel_layout, kernel_data_type});
    const auto& kernel = kernel_result.kernel;
    VLOG(6) << "shape_sr kernel: " << kernel;
    auto* dev_ctx = GetDeviceContextByBackend(
        kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

    auto input_input = TensorToSelectedRows(input);

    if (platform::RecordOpInfoSupplement::IsEnabled()) {
      std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
          {"input", {(*input_input).dims()}}};
      platform::RecordOpInfoSupplement("shape", input_shapes);
    }

    Tensor api_output;
    auto kernel_out = SetSelectedRowsKernelOutput(&api_output);
    paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      infer_shape_record_event = new paddle::platform::RecordEvent(
          "shape infer_meta",
          paddle::platform::TracerEventType::OperatorInner,
          1);
    }
    phi::MetaTensor meta_out(kernel_out);

    phi::ShapeInferMeta(MakeMetaTensor(*input_input), &meta_out);

    if (infer_shape_record_event != nullptr) {
      delete infer_shape_record_event;
    }
    using kernel_signature = void (*)(const platform::DeviceContext&,
                                      const phi::SelectedRows&,
                                      phi::SelectedRows*);
    auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
    paddle::platform::RecordEvent* kernel_record_event = nullptr;
    if (paddle::platform::RecordEvent::IsEnabled()) {
      kernel_record_event = new paddle::platform::RecordEvent(
          "shape compute", paddle::platform::TracerEventType::OperatorInner, 1);
    }
    (*kernel_fn)(*dev_ctx, *input_input, kernel_out);
    if (kernel_record_event != nullptr) {
      delete kernel_record_event;
    }
    if (kernel_result.has_fallback_cpu) {
      TransDataBackend(kernel_out, kernel_backend, kernel_out);
    }
    return api_output;
  }

  PADDLE_THROW(phi::errors::Unimplemented(
      "The kernel of (shape) for input tensors is unimplemented, please check "
      "the type of input tensors."));
}

PADDLE_API Tensor shard_index(const Tensor& in,
                              int index_num,
                              int nshards,
                              int shard_id,
                              int ignore_value) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(in);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "shard_index API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "shard_index", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "shard_index kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_in = PrepareData(in, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"in", {(*input_in).dims()}}};
    platform::RecordOpInfoSupplement("shard_index", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "shard_index infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ShardIndexInferMeta(MakeMetaTensor(*input_in),
                           index_num,
                           nshards,
                           shard_id,
                           ignore_value,
                           &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "shard_index compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_in,
               index_num,
               nshards,
               shard_id,
               ignore_value,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor sigmoid(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sigmoid API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sigmoid kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("sigmoid", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sigmoid infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sigmoid compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor sigmoid_cross_entropy_with_logits(const Tensor& x,
                                                    const Tensor& label,
                                                    bool normalize,
                                                    int ignore_index) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, label);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sigmoid_cross_entropy_with_logits API kernel key: ["
          << kernel_backend << ", " << kernel_layout << ", " << kernel_data_type
          << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid_cross_entropy_with_logits",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sigmoid_cross_entropy_with_logits kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"label", {(*input_label).dims()}}};
    platform::RecordOpInfoSupplement("sigmoid_cross_entropy_with_logits",
                                     input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sigmoid_cross_entropy_with_logits infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SigmoidCrossEntropyWithLogitsInferMeta(MakeMetaTensor(*input_x),
                                              MakeMetaTensor(*input_label),
                                              normalize,
                                              ignore_index,
                                              &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sigmoid_cross_entropy_with_logits compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, *input_label, normalize, ignore_index, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor sign(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sign API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sign", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sign kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("sign", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sign infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sign compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor silu(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "silu API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "silu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "silu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("silu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "silu infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "silu compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor sin(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sin API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sin", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sin kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("sin", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sin infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sin compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor sinh(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sinh API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sinh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sinh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("sinh", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sinh infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sinh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor size(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "size API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "size", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "size kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {true});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("size", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "size infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SizeInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "size compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor slice(const Tensor& input,
                        const std::vector<int64_t>& axes,
                        const IntArray& starts,
                        const IntArray& ends,
                        const std::vector<int64_t>& infer_flags,
                        const std::vector<int64_t>& decrease_axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "slice API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "slice", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "slice kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}};
    platform::RecordOpInfoSupplement("slice", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "slice infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SliceRawInferMeta(MakeMetaTensor(*input_input),
                         axes,
                         starts,
                         ends,
                         infer_flags,
                         decrease_axis,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int64_t>&,
                                    const phi::IntArray&,
                                    const phi::IntArray&,
                                    const std::vector<int64_t>&,
                                    const std::vector<int64_t>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "slice compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               axes,
               phi::IntArray(starts),
               phi::IntArray(ends),
               infer_flags,
               decrease_axis,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor slogdet(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "slogdet API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "slogdeterminant", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "slogdeterminant kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("slogdet", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "slogdet infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "slogdet compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor soft_shrink(const Tensor& x, float lambda) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "soft_shrink API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "soft_shrink", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "soft_shrink kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("soft_shrink", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "soft_shrink infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "soft_shrink compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, lambda, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor softmax(const Tensor& x, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softmax API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "softmax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("softmax", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "softmax infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SoftmaxInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "softmax compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& softmax_(Tensor& x, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softmax API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softmax", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "softmax kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("softmax", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "softmax infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SoftmaxInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "softmax compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor softplus(const Tensor& x, float beta, float threshold) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softplus API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softplus", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "softplus kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("softplus", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "softplus infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "softplus compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, beta, threshold, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor softsign(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "softsign API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softsign", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "softsign kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("softsign", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "softsign infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "softsign compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor spectral_norm(const Tensor& weight,
                                const Tensor& u,
                                const Tensor& v,
                                int dim,
                                int power_iters,
                                float eps) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(weight);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(weight, u, v);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "spectral_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "spectral_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "spectral_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_weight = PrepareData(weight, kernel.InputAt(0), {});
  auto input_u = PrepareData(u, kernel.InputAt(1), {});
  auto input_v = PrepareData(v, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"weight", {(*input_weight).dims()}},
        {"u", {(*input_u).dims()}},
        {"v", {(*input_v).dims()}}};
    platform::RecordOpInfoSupplement("spectral_norm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "spectral_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SpectralNormInferMeta(MakeMetaTensor(*input_weight),
                             MakeMetaTensor(*input_u),
                             MakeMetaTensor(*input_v),
                             dim,
                             power_iters,
                             eps,
                             &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "spectral_norm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_weight,
               *input_u,
               *input_v,
               dim,
               power_iters,
               eps,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::vector<Tensor> split(const Tensor& x,
                                     const IntArray& sections,
                                     const Scalar& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "split API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "split", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "split kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("split", input_shapes);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(sections.size(), &api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "split infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::SplitInferMeta(
      MakeMetaTensor(*input_x), sections, axis, kernel_out_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    const phi::Scalar&,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "split compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::IntArray(sections),
               phi::Scalar(axis),
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::vector<Tensor> split_with_num(const Tensor& x,
                                              int num,
                                              const Scalar& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "split_with_num API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "split_with_num", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "split_with_num kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("split_with_num", input_shapes);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(num, &api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "split_with_num infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::SplitWithNumInferMeta(
      MakeMetaTensor(*input_x), num, axis, kernel_out_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    const phi::Scalar&,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "split_with_num compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, num, phi::Scalar(axis), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor sqrt(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sqrt API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sqrt", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sqrt kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("sqrt", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sqrt infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sqrt compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& sqrt_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sqrt API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sqrt", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sqrt kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("sqrt", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sqrt infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sqrt compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor square(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "square API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "square", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "square kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("square", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "square infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "square compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor squared_l2_norm(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "squared_l2_norm API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squared_l2_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "squared_l2_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("squared_l2_norm", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "squared_l2_norm infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SquaredL2NormInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "squared_l2_norm compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor squeeze(const Tensor& x, const IntArray& axes) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "squeeze API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squeeze_with_xshape", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "squeeze_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("squeeze", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  kernel_out_0->ShareBufferWith(*input_x);
  kernel_out_0->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation "
             "and inplace version.";
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "squeeze infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::SqueezeWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                  axes,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "squeeze compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::IntArray(axes), kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor& squeeze_(Tensor& x, const IntArray& axes) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "squeeze API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squeeze_with_xshape", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "squeeze_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("squeeze", input_shapes);
  }

  std::tuple<Tensor&, Tensor> api_output{x, Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "squeeze infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::SqueezeWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                  axes,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "squeeze compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::IntArray(axes), kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor stack(const std::vector<Tensor>& x, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "stack API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "stack", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "stack kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x_vec = PrepareData(x, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_x.size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      ddims_vec.emplace_back((*input_x[i]).dims());
    }
    input_shapes.emplace_back("x", ddims_vec);
    platform::RecordOpInfoSupplement("stack", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "stack infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::StackInferMeta(x_metas, axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "stack compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, input_x, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor strided_slice(const Tensor& x,
                                const std::vector<int>& axes,
                                const IntArray& starts,
                                const IntArray& ends,
                                const IntArray& strides) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "strided_slice API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "strided_slice", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "strided_slice kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("strided_slice", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "strided_slice infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::StridedSliceInferMeta(
      MakeMetaTensor(*input_x), axes, starts, ends, strides, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const phi::IntArray&,
                                    const phi::IntArray&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "strided_slice compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               axes,
               phi::IntArray(starts),
               phi::IntArray(ends),
               phi::IntArray(strides),
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor subtract(const Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "subtract API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "subtract", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "subtract kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("subtract", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "subtract infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "subtract compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& subtract_(Tensor& x, const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "subtract API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "subtract", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "subtract kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("subtract", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "subtract infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::ElementwiseInferMeta(
      MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "subtract compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor sum(const Tensor& x,
                      const IntArray& dims,
                      DataType out_dtype,
                      bool keep_dim) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sum API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sum", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sum kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("sum", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sum infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::SumInferMeta(
      MakeMetaTensor(*input_x), dims, out_dtype, keep_dim, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    DataType,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sum compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::IntArray(dims), out_dtype, keep_dim, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> svd(const Tensor& x,
                                                  bool full_metrices) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "svd API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "svd", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "svd kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("svd", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "svd infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::SvdInferMeta(MakeMetaTensor(*input_x),
                    full_metrices,
                    kernel_out_0 ? &meta_out_0 : nullptr,
                    kernel_out_1 ? &meta_out_1 : nullptr,
                    kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "svd compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               full_metrices,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor swish(const Tensor& x, float beta) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "swish API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "swish", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "swish kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("swish", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "swish infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "swish compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, beta, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor&, Tensor&, Tensor, Tensor, Tensor>
sync_batch_norm_(const Tensor& x,
                 const Tensor& scale,
                 const Tensor& bias,
                 Tensor& mean,
                 Tensor& variance,
                 float momentum,
                 float epsilon,
                 const std::string& data_layout,
                 bool is_test,
                 bool use_global_stats,
                 bool trainable_statistics,
                 bool fuse_with_relu) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, scale, bias, mean, variance);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "sync_batch_norm_ API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sync_batch_norm", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "sync_batch_norm kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_scale = PrepareData(scale, kernel.InputAt(1), {});
  auto input_bias = PrepareData(bias, kernel.InputAt(2), {});
  auto input_mean = PrepareData(mean, kernel.InputAt(3), {});
  auto input_variance = PrepareData(variance, kernel.InputAt(4), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"scale", {(*input_scale).dims()}},
        {"bias", {(*input_bias).dims()}},
        {"mean", {(*input_mean).dims()}},
        {"variance", {(*input_variance).dims()}}};
    platform::RecordOpInfoSupplement("sync_batch_norm_", input_shapes);
  }

  std::tuple<Tensor, Tensor&, Tensor&, Tensor, Tensor, Tensor> api_output{
      Tensor(), mean, variance, Tensor(), Tensor(), Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  auto kernel_out_4 = SetKernelOutput(&std::get<4>(api_output));
  auto kernel_out_5 = SetKernelOutput(&std::get<5>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "sync_batch_norm_ infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);
  phi::MetaTensor meta_out_3(kernel_out_3);
  phi::MetaTensor meta_out_4(kernel_out_4);
  phi::MetaTensor meta_out_5(kernel_out_5);

  phi::BatchNormInferMeta(MakeMetaTensor(*input_x),
                          MakeMetaTensor(*input_scale),
                          MakeMetaTensor(*input_bias),
                          MakeMetaTensor(*input_mean),
                          MakeMetaTensor(*input_variance),
                          momentum,
                          epsilon,
                          data_layout,
                          is_test,
                          use_global_stats,
                          trainable_statistics,
                          fuse_with_relu,
                          kernel_out_0 ? &meta_out_0 : nullptr,
                          kernel_out_1 ? &meta_out_1 : nullptr,
                          kernel_out_2 ? &meta_out_2 : nullptr,
                          kernel_out_3 ? &meta_out_3 : nullptr,
                          kernel_out_4 ? &meta_out_4 : nullptr,
                          kernel_out_5 ? &meta_out_5 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    const std::string&,
                                    bool,
                                    bool,
                                    bool,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "sync_batch_norm_ compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_scale,
               *input_bias,
               *input_mean,
               *input_variance,
               momentum,
               epsilon,
               data_layout,
               is_test,
               use_global_stats,
               trainable_statistics,
               fuse_with_relu,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2,
               kernel_out_3,
               kernel_out_4,
               kernel_out_5);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);
    TransDataBackend(kernel_out_5, kernel_backend, kernel_out_5);
  }
  return api_output;
}

PADDLE_API Tensor take_along_axis(const Tensor& x,
                                  const Tensor& index,
                                  int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, index);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "take_along_axis API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "take_along_axis", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "take_along_axis kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_index = PrepareData(index, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"index", {(*input_index).dims()}}};
    platform::RecordOpInfoSupplement("take_along_axis", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "take_along_axis infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_index), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "take_along_axis compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, *input_index, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor tan(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tan API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tan", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "tan kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("tan", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "tan infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "tan compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor tanh(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tanh API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "tanh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("tanh", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "tanh infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "tanh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& tanh_(Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tanh API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "tanh kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("tanh", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "tanh infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "tanh compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor tanh_shrink(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tanh_shrink API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh_shrink", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "tanh_shrink kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("tanh_shrink", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "tanh_shrink infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "tanh_shrink compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor temporal_shift(const Tensor& x,
                                 int seg_num,
                                 float shift_ratio,
                                 const std::string& data_format_str) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "temporal_shift API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "temporal_shift", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "temporal_shift kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("temporal_shift", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "temporal_shift infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TemporalShiftInferMeta(MakeMetaTensor(*input_x),
                              seg_num,
                              shift_ratio,
                              data_format_str,
                              &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    float,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "temporal_shift compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, seg_num, shift_ratio, data_format_str, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor thresholded_relu(const Tensor& x, float threshold) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "thresholded_relu API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "thresholded_relu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "thresholded_relu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("thresholded_relu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "thresholded_relu infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "thresholded_relu compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, threshold, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor tile(const Tensor& x, const IntArray& repeat_times) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tile API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tile", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "tile kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("tile", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "tile infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TileInferMeta(MakeMetaTensor(*input_x), repeat_times, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "tile compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, phi::IntArray(repeat_times), kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> top_k(
    const Tensor& x, const Scalar& k, int axis, bool largest, bool sorted) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "top_k API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "top_k", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "top_k kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("top_k", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "top_k infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::TopKInferMeta(MakeMetaTensor(*input_x),
                     k,
                     axis,
                     largest,
                     sorted,
                     kernel_out_0 ? &meta_out_0 : nullptr,
                     kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::Scalar&,
                                    int,
                                    bool,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "top_k compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               phi::Scalar(k),
               axis,
               largest,
               sorted,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor transpose(const Tensor& x, const std::vector<int>& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "transpose API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "transpose", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "transpose kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("transpose", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "transpose infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TransposeInferMeta(MakeMetaTensor(*input_x), axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "transpose compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor triangular_solve(const Tensor& x,
                                   const Tensor& y,
                                   bool upper,
                                   bool transpose,
                                   bool unitriangular) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "triangular_solve API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triangular_solve", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "triangular_solve kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_y = PrepareData(y, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("triangular_solve", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "triangular_solve infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TriangularSolveInferMeta(MakeMetaTensor(*input_x),
                                MakeMetaTensor(*input_y),
                                upper,
                                transpose,
                                unitriangular,
                                &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "triangular_solve compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_y,
               upper,
               transpose,
               unitriangular,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor tril_indices(
    int rows, int cols, int offset, DataType dtype, const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "tril_indices API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tril_indices", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "tril_indices kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("tril_indices", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "tril_indices infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TrilIndicesInferMeta(rows, cols, offset, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    int,
                                    int,
                                    int,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "tril_indices compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, rows, cols, offset, dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor tril_triu(const Tensor& x, int diagonal, bool lower) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "tril_triu API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tril_triu", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "tril_triu kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("tril_triu", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "tril_triu infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TrilTriuInferMeta(MakeMetaTensor(*input_x), diagonal, lower, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    bool,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "tril_triu compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, diagonal, lower, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor
trilinear_interp(const Tensor& x,
                 const paddle::optional<Tensor>& out_size,
                 const paddle::optional<std::vector<Tensor>>& size_tensor,
                 const paddle::optional<Tensor>& scale_tensor,
                 const std::string& data_layout,
                 int out_d,
                 int out_h,
                 int out_w,
                 const std::vector<float>& scale,
                 const std::string& interp_method,
                 bool align_corners,
                 int align_mode) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "trilinear_interp API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trilinear_interp", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "trilinear_interp kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_out_size = PrepareData(out_size, kernel.InputAt(1), {});
  auto input_size_tensor_vec = PrepareData(size_tensor, kernel.InputAt(2), {});
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec) {
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(
        input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> out_size_record_shapes;
    if (input_out_size) {
      out_size_record_shapes.push_back((*input_out_size).dims());
    }
    std::vector<phi::DDim> scale_tensor_record_shapes;
    if (input_scale_tensor) {
      scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"out_size", out_size_record_shapes},
        {"scale_tensor", scale_tensor_record_shapes}};
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    if (input_size_tensor) {
      ddims_vec.reserve(input_size_tensor->size());
      for (size_t i = 0; i < input_size_tensor->size(); ++i) {
        ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
      }
    }
    input_shapes.emplace_back("size_tensor", ddims_vec);
    platform::RecordOpInfoSupplement("trilinear_interp", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "trilinear_interp infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto size_tensor_meta_vec = MakeMetaTensor(input_size_tensor);
  paddle::optional<std::vector<const phi::MetaTensor*>> size_tensor_metas(
      size_tensor_meta_vec.size());
  for (size_t i = 0; i < size_tensor_meta_vec.size(); ++i) {
    size_tensor_metas->at(i) = &size_tensor_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::InterpolateInferMeta(MakeMetaTensor(*input_x),
                            MakeMetaTensor(input_out_size),
                            size_tensor_metas,
                            MakeMetaTensor(input_scale_tensor),
                            data_layout,
                            out_d,
                            out_h,
                            out_w,
                            scale,
                            interp_method,
                            align_corners,
                            align_mode,
                            &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature =
      void (*)(const platform::DeviceContext&,
               const phi::DenseTensor&,
               const paddle::optional<phi::DenseTensor>&,
               const paddle::optional<std::vector<const phi::DenseTensor*>>&,
               const paddle::optional<phi::DenseTensor>&,
               const std::string&,
               int,
               int,
               int,
               const std::vector<float>&,
               const std::string&,
               bool,
               int,
               phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "trilinear_interp compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               input_out_size,
               input_size_tensor,
               input_scale_tensor,
               data_layout,
               out_d,
               out_h,
               out_w,
               scale,
               interp_method,
               align_corners,
               align_mode,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor
triu_indices(int row, int col, int offset, DataType dtype, const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "triu_indices API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triu_indices", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "triu_indices kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("triu_indices", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "triu_indices infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TriuIndicesInferMeta(row, col, offset, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    int,
                                    int,
                                    int,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "triu_indices compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, row, col, offset, dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor truncated_gaussian_random(const std::vector<int>& shape,
                                            float mean,
                                            float std,
                                            int seed,
                                            DataType dtype,
                                            const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "truncated_gaussian_random API kernel key: [" << kernel_backend
          << ", " << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "truncated_gaussian_random",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "truncated_gaussian_random kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("truncated_gaussian_random", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "truncated_gaussian_random infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::TruncatedGaussianRandomInferMeta(
      shape, mean, std, seed, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<int>&,
                                    float,
                                    float,
                                    int,
                                    DataType,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "truncated_gaussian_random compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, shape, mean, std, seed, dtype, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::vector<Tensor> unbind(const Tensor& input, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unbind API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unbind", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unbind kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}}};
    platform::RecordOpInfoSupplement("unbind", input_shapes);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(
      axis < 0 ? input.dims()[input.dims().size() + axis] : input.dims()[axis],
      &api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unbind infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::UnbindInferMeta(MakeMetaTensor(*input_input), axis, kernel_out_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unbind compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_input, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor unfold(const Tensor& x,
                         const std::vector<int>& kernel_sizes,
                         const std::vector<int>& strides,
                         const std::vector<int>& paddings,
                         const std::vector<int>& dilations) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unfold API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unfold", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unfold kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("unfold", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unfold infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnfoldInferMeta(MakeMetaTensor(*input_x),
                       kernel_sizes,
                       strides,
                       paddings,
                       dilations,
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unfold compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               kernel_sizes,
               strides,
               paddings,
               dilations,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor uniform_random(const IntArray& shape,
                                 DataType dtype,
                                 const Scalar& min,
                                 const Scalar& max,
                                 int seed,
                                 const Place& place) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_backend = ParseBackend(place);

  kernel_data_type = ParseDataType(dtype);

  VLOG(6) << "uniform_random API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "uniform_random", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "uniform_random kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    platform::RecordOpInfoSupplement("uniform_random", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "uniform_random infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UniformRandomInferMeta(shape, dtype, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::IntArray&,
                                    DataType,
                                    const phi::Scalar&,
                                    const phi::Scalar&,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "uniform_random compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               phi::IntArray(shape),
               dtype,
               phi::Scalar(min),
               phi::Scalar(max),
               seed,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor, Tensor> unique(
    const Tensor& x,
    bool return_index,
    bool return_inverse,
    bool return_counts,
    const std::vector<int>& axis,
    DataType dtype) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unique API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unique", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unique kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("unique", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  auto kernel_out_3 = SetKernelOutput(&std::get<3>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unique infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);
  phi::MetaTensor meta_out_3(kernel_out_3);

  phi::UniqueInferMeta(MakeMetaTensor(*input_x),
                       return_index,
                       return_inverse,
                       return_counts,
                       axis,
                       dtype,
                       kernel_out_0 ? &meta_out_0 : nullptr,
                       kernel_out_1 ? &meta_out_1 : nullptr,
                       kernel_out_2 ? &meta_out_2 : nullptr,
                       kernel_out_3 ? &meta_out_3 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    bool,
                                    const std::vector<int>&,
                                    DataType,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unique compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               return_index,
               return_inverse,
               return_counts,
               axis,
               dtype,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2,
               kernel_out_3);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> unique_consecutive(
    const Tensor& x,
    bool return_inverse,
    bool return_counts,
    const std::vector<int>& axis,
    int dtype) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unique_consecutive API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unique_consecutive", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unique_consecutive kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("unique_consecutive", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unique_consecutive infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::UniqueConsecutiveInferMeta(MakeMetaTensor(*input_x),
                                  return_inverse,
                                  return_counts,
                                  axis,
                                  dtype,
                                  kernel_out_0 ? &meta_out_0 : nullptr,
                                  kernel_out_1 ? &meta_out_1 : nullptr,
                                  kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    bool,
                                    bool,
                                    const std::vector<int>&,
                                    int,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unique_consecutive compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               return_inverse,
               return_counts,
               axis,
               dtype,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor unsqueeze(const Tensor& x, const IntArray& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unsqueeze API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unsqueeze_with_xshape",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unsqueeze_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("unsqueeze", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  kernel_out_0->ShareBufferWith(*input_x);
  kernel_out_0->ShareInplaceVersionCounterWith(*input_x);
  VLOG(3) << "Perform View between Output and Input Tensor, share allocation "
             "and inplace version.";
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unsqueeze infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::UnsqueezeWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                    axis,
                                    kernel_out_0 ? &meta_out_0 : nullptr,
                                    kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unsqueeze compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::IntArray(axis), kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor& unsqueeze_(Tensor& x, const IntArray& axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unsqueeze API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unsqueeze_with_xshape",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unsqueeze_with_xshape kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("unsqueeze", input_shapes);
  }

  std::tuple<Tensor&, Tensor> api_output{x, Tensor()};
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unsqueeze infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::UnsqueezeWithXShapeInferMeta(MakeMetaTensor(*input_x),
                                    axis,
                                    kernel_out_0 ? &meta_out_0 : nullptr,
                                    kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::IntArray&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unsqueeze compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(
      *dev_ctx, *input_x, phi::IntArray(axis), kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API std::vector<Tensor> unstack(const Tensor& x, int axis, int num) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unstack API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unstack", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unstack kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("unstack", input_shapes);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(num, &api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unstack infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::UnStackInferMeta(MakeMetaTensor(*input_x), axis, num, kernel_out_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unstack compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, axis, num, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> viterbi_decode(const Tensor& input,
                                                     const Tensor& transition,
                                                     const Tensor& length,
                                                     bool include_bos_eos_tag) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(input);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(input, transition, length);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "viterbi_decode API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "viterbi_decode", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "viterbi_decode kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_input = PrepareData(input, kernel.InputAt(0), {});
  auto input_transition = PrepareData(transition, kernel.InputAt(1), {});
  auto input_length = PrepareData(length, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"input", {(*input_input).dims()}},
        {"transition", {(*input_transition).dims()}},
        {"length", {(*input_length).dims()}}};
    platform::RecordOpInfoSupplement("viterbi_decode", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "viterbi_decode infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::ViterbiDecodeInferMeta(MakeMetaTensor(*input_input),
                              MakeMetaTensor(*input_transition),
                              MakeMetaTensor(*input_length),
                              include_bos_eos_tag,
                              kernel_out_0 ? &meta_out_0 : nullptr,
                              kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "viterbi_decode compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_input,
               *input_transition,
               *input_length,
               include_bos_eos_tag,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor warpctc(const Tensor& logits,
                          const Tensor& label,
                          const paddle::optional<Tensor>& logits_length,
                          const paddle::optional<Tensor>& labels_length,
                          int blank,
                          bool norm_by_times) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(logits);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(logits, label, logits_length, labels_length);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "warpctc API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "warpctc", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "warpctc kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_logits = PrepareData(logits, kernel.InputAt(0), {});
  auto input_label = PrepareData(label, kernel.InputAt(1), {});
  auto input_logits_length = PrepareData(logits_length, kernel.InputAt(2), {});
  auto input_labels_length = PrepareData(labels_length, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> logits_length_record_shapes;
    if (input_logits_length) {
      logits_length_record_shapes.push_back((*input_logits_length).dims());
    }
    std::vector<phi::DDim> labels_length_record_shapes;
    if (input_labels_length) {
      labels_length_record_shapes.push_back((*input_labels_length).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"logits", {(*input_logits).dims()}},
        {"label", {(*input_label).dims()}},
        {"logits_length", logits_length_record_shapes},
        {"labels_length", labels_length_record_shapes}};
    platform::RecordOpInfoSupplement("warpctc", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "warpctc infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::WarpctcInferMeta(MakeMetaTensor(*input_logits),
                        MakeMetaTensor(*input_label),
                        MakeMetaTensor(input_logits_length),
                        MakeMetaTensor(input_labels_length),
                        blank,
                        norm_by_times,
                        kernel_out_0 ? &meta_out_0 : nullptr,
                        kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    int,
                                    bool,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "warpctc compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_logits,
               *input_label,
               input_logits_length,
               input_labels_length,
               blank,
               norm_by_times,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return std::get<0>(api_output);
}

PADDLE_API Tensor where(const Tensor& condition,
                        const Tensor& x,
                        const Tensor& y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(condition, x, y);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "where API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "where", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "where kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_condition = PrepareData(condition, kernel.InputAt(0), {});
  auto input_x = PrepareData(x, kernel.InputAt(1), {});
  auto input_y = PrepareData(y, kernel.InputAt(2), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"condition", {(*input_condition).dims()}},
        {"x", {(*input_x).dims()}},
        {"y", {(*input_y).dims()}}};
    platform::RecordOpInfoSupplement("where", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "where infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::WhereInferMeta(MakeMetaTensor(*input_condition),
                      MakeMetaTensor(*input_x),
                      MakeMetaTensor(*input_y),
                      &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "where compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_condition, *input_x, *input_y, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor where_index(const Tensor& condition) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(condition);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "where_index API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "where_index", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "where_index kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_condition = PrepareData(condition, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"condition", {(*input_condition).dims()}}};
    platform::RecordOpInfoSupplement("where_index", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "where_index infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::WhereIndexInferMeta(MakeMetaTensor(*input_condition), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "where_index compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_condition, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> yolo_box(const Tensor& x,
                                               const Tensor& img_size,
                                               const std::vector<int>& anchors,
                                               int class_num,
                                               float conf_thresh,
                                               int downsample_ratio,
                                               bool clip_bbox,
                                               float scale_x_y,
                                               bool iou_aware,
                                               float iou_aware_factor) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, img_size);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "yolo_box API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "yolo_box", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "yolo_box kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_img_size = PrepareData(img_size, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"img_size", {(*input_img_size).dims()}}};
    platform::RecordOpInfoSupplement("yolo_box", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "yolo_box infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::YoloBoxInferMeta(MakeMetaTensor(*input_x),
                        MakeMetaTensor(*input_img_size),
                        anchors,
                        class_num,
                        conf_thresh,
                        downsample_ratio,
                        clip_bbox,
                        scale_x_y,
                        iou_aware,
                        iou_aware_factor,
                        kernel_out_0 ? &meta_out_0 : nullptr,
                        kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    int,
                                    float,
                                    int,
                                    bool,
                                    float,
                                    bool,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "yolo_box compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_img_size,
               anchors,
               class_num,
               conf_thresh,
               downsample_ratio,
               clip_bbox,
               scale_x_y,
               iou_aware,
               iou_aware_factor,
               kernel_out_0,
               kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor, Tensor> yolov3_loss(
    const Tensor& x,
    const Tensor& gt_box,
    const Tensor& gt_label,
    const paddle::optional<Tensor>& gt_score,
    const std::vector<int>& anchors,
    const std::vector<int>& anchor_mask,
    int class_num,
    float ignore_thresh,
    int downsample_ratio,
    bool use_label_smooth,
    float scale_x_y) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set =
        ParseKernelKeyByInputArgs(x, gt_box, gt_label, gt_score);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "yolov3_loss API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "yolov3_loss", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "yolov3_loss kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_gt_box = PrepareData(gt_box, kernel.InputAt(1), {});
  auto input_gt_label = PrepareData(gt_label, kernel.InputAt(2), {});
  auto input_gt_score = PrepareData(gt_score, kernel.InputAt(3), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<phi::DDim> gt_score_record_shapes;
    if (input_gt_score) {
      gt_score_record_shapes.push_back((*input_gt_score).dims());
    }
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}},
        {"gt_box", {(*input_gt_box).dims()}},
        {"gt_label", {(*input_gt_label).dims()}},
        {"gt_score", gt_score_record_shapes}};
    platform::RecordOpInfoSupplement("yolov3_loss", input_shapes);
  }

  std::tuple<Tensor, Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  auto kernel_out_2 = SetKernelOutput(&std::get<2>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "yolov3_loss infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);
  phi::MetaTensor meta_out_2(kernel_out_2);

  phi::Yolov3LossInferMeta(MakeMetaTensor(*input_x),
                           MakeMetaTensor(*input_gt_box),
                           MakeMetaTensor(*input_gt_label),
                           MakeMetaTensor(input_gt_score),
                           anchors,
                           anchor_mask,
                           class_num,
                           ignore_thresh,
                           downsample_ratio,
                           use_label_smooth,
                           scale_x_y,
                           kernel_out_0 ? &meta_out_0 : nullptr,
                           kernel_out_1 ? &meta_out_1 : nullptr,
                           kernel_out_2 ? &meta_out_2 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const paddle::optional<phi::DenseTensor>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    int,
                                    float,
                                    int,
                                    bool,
                                    float,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "yolov3_loss compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_gt_box,
               *input_gt_label,
               input_gt_score,
               anchors,
               anchor_mask,
               class_num,
               ignore_thresh,
               downsample_ratio,
               use_label_smooth,
               scale_x_y,
               kernel_out_0,
               kernel_out_1,
               kernel_out_2);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
  }
  return api_output;
}

PADDLE_API Tensor zeros(const IntArray& shape,
                        DataType dtype,
                        const Place& place) {
  return full(shape, 0, dtype, place);
}
PADDLE_API Tensor zeros_like(const Tensor& x,
                             DataType dtype,
                             const Place& place) {
  return full_like(x, 0, dtype, place);
}
PADDLE_API std::vector<Tensor> broadcast_tensors(const std::vector<Tensor>& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "broadcast_tensors API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "broadcast_tensors", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "broadcast_tensors kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x_vec = PrepareData(x, kernel.InputAt(0), {});
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
    std::vector<phi::DDim> ddims_vec;
    ddims_vec.clear();
    ddims_vec.reserve(input_x.size());
    for (size_t i = 0; i < input_x.size(); ++i) {
      ddims_vec.emplace_back((*input_x[i]).dims());
    }
    input_shapes.emplace_back("x", ddims_vec);
    platform::RecordOpInfoSupplement("broadcast_tensors", input_shapes);
  }

  std::vector<Tensor> api_output;
  auto kernel_out = SetKernelOutput(x.size(), &api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "broadcast_tensors infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::BroadcastTensorsInferMeta(x_metas, kernel_out_metas);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const std::vector<const phi::DenseTensor*>&,
                                    std::vector<phi::DenseTensor*>&);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "broadcast_tensors compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, input_x, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor dirichlet(const Tensor& alpha) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(alpha);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "dirichlet API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dirichlet", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "dirichlet kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_alpha = PrepareData(alpha, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"alpha", {(*input_alpha).dims()}}};
    platform::RecordOpInfoSupplement("dirichlet", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "dirichlet infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::DirichletInferMeta(MakeMetaTensor(*input_alpha), &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "dirichlet compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_alpha, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API std::tuple<Tensor, Tensor> eig(const Tensor& x) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "eig API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eig", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "eig kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("eig", input_shapes);
  }

  std::tuple<Tensor, Tensor> api_output;
  auto kernel_out_0 = SetKernelOutput(&std::get<0>(api_output));
  auto kernel_out_1 = SetKernelOutput(&std::get<1>(api_output));
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "eig infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0);
  phi::MetaTensor meta_out_1(kernel_out_1);

  phi::EigInferMeta(MakeMetaTensor(*input_x),
                    kernel_out_0 ? &meta_out_0 : nullptr,
                    kernel_out_1 ? &meta_out_1 : nullptr);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    phi::DenseTensor*,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "eig compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, kernel_out_0, kernel_out_1);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
  }
  return api_output;
}

PADDLE_API Tensor fold(const Tensor& x,
                       const std::vector<int>& output_sizes,
                       const std::vector<int>& kernel_sizes,
                       const std::vector<int>& strides,
                       const std::vector<int>& paddings,
                       const std::vector<int>& dilations) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "fold API kernel key: [" << kernel_backend << ", " << kernel_layout
          << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fold", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "fold kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("fold", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "fold infer_meta", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::FoldInferMeta(MakeMetaTensor(*input_x),
                     output_sizes,
                     kernel_sizes,
                     strides,
                     paddings,
                     dilations,
                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "fold compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               output_sizes,
               kernel_sizes,
               strides,
               paddings,
               dilations,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor overlap_add(const Tensor& x, int hop_length, int axis) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "overlap_add API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "overlap_add", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "overlap_add kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("overlap_add", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "overlap_add infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::OverlapAddInferMeta(
      MakeMetaTensor(*input_x), hop_length, axis, &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    int,
                                    int,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "overlap_add compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx, *input_x, hop_length, axis, kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor uniform_random_inplace(const Tensor& x,
                                         float min,
                                         float max,
                                         int seed,
                                         int diag_num,
                                         int diag_step,
                                         float diag_val) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "uniform_random_inplace API kernel key: [" << kernel_backend
          << ", " << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "uniform_random_inplace",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "uniform_random_inplace kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("uniform_random_inplace", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "uniform_random_inplace infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UniformRandomInplaceInferMeta(MakeMetaTensor(*input_x),
                                     min,
                                     max,
                                     seed,
                                     diag_num,
                                     diag_step,
                                     diag_val,
                                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    int,
                                    int,
                                    int,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "uniform_random_inplace compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               min,
               max,
               seed,
               diag_num,
               diag_step,
               diag_val,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor& uniform_random_inplace_(Tensor& x,
                                           float min,
                                           float max,
                                           int seed,
                                           int diag_num,
                                           int diag_step,
                                           float diag_val) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "uniform_random_inplace API kernel key: [" << kernel_backend
          << ", " << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "uniform_random_inplace",
      {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "uniform_random_inplace kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}};
    platform::RecordOpInfoSupplement("uniform_random_inplace", input_shapes);
  }

  Tensor& api_output = x;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "uniform_random_inplace infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UniformRandomInplaceInferMeta(MakeMetaTensor(*input_x),
                                     min,
                                     max,
                                     seed,
                                     diag_num,
                                     diag_step,
                                     diag_val,
                                     &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    float,
                                    float,
                                    int,
                                    int,
                                    int,
                                    float,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "uniform_random_inplace compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               min,
               max,
               seed,
               diag_num,
               diag_step,
               diag_val,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor unpool(const Tensor& x,
                         const Tensor& indices,
                         const std::vector<int>& ksize,
                         const std::vector<int>& strides,
                         const std::vector<int>& padding,
                         const IntArray& output_size,
                         const std::string& data_format) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unpool API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unpool", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unpool kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_indices = PrepareData(indices, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"indices", {(*input_indices).dims()}}};
    platform::RecordOpInfoSupplement("unpool", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unpool infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::UnpoolInferMeta(MakeMetaTensor(*input_x),
                       MakeMetaTensor(*input_indices),
                       ksize,
                       strides,
                       padding,
                       output_size,
                       data_format,
                       &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const phi::IntArray&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unpool compute", paddle::platform::TracerEventType::OperatorInner, 1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_indices,
               ksize,
               strides,
               padding,
               phi::IntArray(output_size),
               data_format,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

PADDLE_API Tensor unpool3d(const Tensor& x,
                           const Tensor& indices,
                           const std::vector<int>& ksize,
                           const std::vector<int>& strides,
                           const std::vector<int>& padding,
                           const std::vector<int>& output_size,
                           const std::string& data_format) {
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  kernel_data_type = ParseDataType(x);

  if (kernel_backend == Backend::UNDEFINED ||
      kernel_layout == DataLayout::UNDEFINED ||
      kernel_data_type == DataType::UNDEFINED) {
    auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices);
    auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
    if (kernel_backend == Backend::UNDEFINED) {
      kernel_backend = kernel_key.backend();
    }
    if (kernel_layout == DataLayout::UNDEFINED) {
      kernel_layout = kernel_key.layout();
    }
    if (kernel_data_type == DataType::UNDEFINED) {
      kernel_data_type = kernel_key.dtype();
    }
  }

  VLOG(6) << "unpool3d API kernel key: [" << kernel_backend << ", "
          << kernel_layout << ", " << kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unpool3d", {kernel_backend, kernel_layout, kernel_data_type});
  const auto& kernel = kernel_result.kernel;
  VLOG(6) << "unpool3d kernel: " << kernel;
  auto* dev_ctx = GetDeviceContextByBackend(
      kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

  auto input_x = PrepareData(x, kernel.InputAt(0), {});
  auto input_indices = PrepareData(indices, kernel.InputAt(1), {});
  if (platform::RecordOpInfoSupplement::IsEnabled()) {
    std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
        {"x", {(*input_x).dims()}}, {"indices", {(*input_indices).dims()}}};
    platform::RecordOpInfoSupplement("unpool3d", input_shapes);
  }

  Tensor api_output;
  auto kernel_out = SetKernelOutput(&api_output);
  paddle::platform::RecordEvent* infer_shape_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    infer_shape_record_event = new paddle::platform::RecordEvent(
        "unpool3d infer_meta",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  phi::MetaTensor meta_out(kernel_out);

  phi::Unpool3dInferMeta(MakeMetaTensor(*input_x),
                         MakeMetaTensor(*input_indices),
                         ksize,
                         strides,
                         padding,
                         output_size,
                         data_format,
                         &meta_out);

  if (infer_shape_record_event != nullptr) {
    delete infer_shape_record_event;
  }
  using kernel_signature = void (*)(const platform::DeviceContext&,
                                    const phi::DenseTensor&,
                                    const phi::DenseTensor&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::vector<int>&,
                                    const std::string&,
                                    phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  paddle::platform::RecordEvent* kernel_record_event = nullptr;
  if (paddle::platform::RecordEvent::IsEnabled()) {
    kernel_record_event = new paddle::platform::RecordEvent(
        "unpool3d compute",
        paddle::platform::TracerEventType::OperatorInner,
        1);
  }
  (*kernel_fn)(*dev_ctx,
               *input_x,
               *input_indices,
               ksize,
               strides,
               padding,
               output_size,
               data_format,
               kernel_out);
  if (kernel_record_event != nullptr) {
    delete kernel_record_event;
  }
  if (kernel_result.has_fallback_cpu) {
    TransDataBackend(kernel_out, kernel_backend, kernel_out);
  }
  return api_output;
}

}  // namespace experimental
}  // namespace paddle
